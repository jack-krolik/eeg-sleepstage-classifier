{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Sleepdetector' from 'sleepdetector' (/Users/jackkrolik/Library/CloudStorage/OneDrive-Personal/Work/UCSF/Code/eeg-sleepstage-classifier/pytorch/sleepdetector.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msleepdetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sleepdetector\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load Data File\u001b[39;00m\n\u001b[1;32m      9\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/data.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Sleepdetector' from 'sleepdetector' (/Users/jackkrolik/Library/CloudStorage/OneDrive-Personal/Work/UCSF/Code/eeg-sleepstage-classifier/pytorch/sleepdetector.py)"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from sleepdetector import Sleepdetector\n",
    "\n",
    "# Load Data File\n",
    "filepath = '../data/data.mat'\n",
    "mat_file = sio.loadmat(filepath)\n",
    "n_examples = np.shape(mat_file['sig1'])[0]\n",
    "x = np.zeros((4, n_examples, 3000, 1))\n",
    "x[0] = mat_file['sig1']\n",
    "x[1] = mat_file['sig2']\n",
    "x[2] = mat_file['sig3']\n",
    "x[3] = mat_file['sig4']\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "x = torch.from_numpy(x).float()\n",
    "\n",
    "# Load CRNNeeg\n",
    "weights_cnn_path = 'cnn_weights.pth'\n",
    "weights_lstm_path = 'lstm_weights.pth'\n",
    "CRNNeeg = Sleepdetector(cnn_path=weights_cnn_path, lstm_path=weights_lstm_path)\n",
    "\n",
    "# Predict Sleep Stages\n",
    "y_hat = CRNNeeg.predict(x)\n",
    "\n",
    "if isinstance(y_hat, int) and y_hat == -1:\n",
    "    print(\"Error in prediction. Check input dimensions.\")\n",
    "else:\n",
    "    # Load actual labels\n",
    "    labels_file = '../data/labels.mat'\n",
    "    y_true = sio.loadmat(labels_file)['labels'].flatten() - 1  # Subtract 1 to match 0-4 encoding\n",
    "\n",
    "    # Ensure y_hat and y_true have the same length\n",
    "    min_length = min(len(y_hat), len(y_true))\n",
    "    y_hat = y_hat[:min_length]\n",
    "    y_true = y_true[:min_length]\n",
    "\n",
    "    # Compute Cohen's Kappa and Accuracy\n",
    "    ck = cohen_kappa_score(y_true, y_hat)\n",
    "    accuracy = accuracy_score(y_true, y_hat)\n",
    "\n",
    "    print(f\"Cohen's Kappa = {ck:.3f}\")\n",
    "    print(f\"Accuracy = {100*accuracy:.4f}%\")\n",
    "\n",
    "# Optional: Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_true, label='True')\n",
    "plt.plot(y_hat, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted Sleep Stages')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sleep Stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SleepDetectorCNN:\n\tUnexpected key(s) in state_dict: \"conv_blocks.0.0.1.weight\", \"conv_blocks.0.0.1.bias\", \"conv_blocks.0.0.1.running_mean\", \"conv_blocks.0.0.1.running_var\", \"conv_blocks.0.1.1.weight\", \"conv_blocks.0.1.1.bias\", \"conv_blocks.0.1.1.running_mean\", \"conv_blocks.0.1.1.running_var\", \"conv_blocks.0.2.weight\", \"conv_blocks.0.2.bias\", \"conv_blocks.0.2.1.weight\", \"conv_blocks.0.2.1.bias\", \"conv_blocks.0.2.1.running_mean\", \"conv_blocks.0.2.1.running_var\", \"conv_blocks.1.0.1.weight\", \"conv_blocks.1.0.1.bias\", \"conv_blocks.1.0.1.running_mean\", \"conv_blocks.1.0.1.running_var\", \"conv_blocks.1.1.1.weight\", \"conv_blocks.1.1.1.bias\", \"conv_blocks.1.1.1.running_mean\", \"conv_blocks.1.1.1.running_var\", \"conv_blocks.1.2.weight\", \"conv_blocks.1.2.bias\", \"conv_blocks.1.2.1.weight\", \"conv_blocks.1.2.1.bias\", \"conv_blocks.1.2.1.running_mean\", \"conv_blocks.1.2.1.running_var\", \"conv_blocks.2.0.1.weight\", \"conv_blocks.2.0.1.bias\", \"conv_blocks.2.0.1.running_mean\", \"conv_blocks.2.0.1.running_var\", \"conv_blocks.2.1.1.weight\", \"conv_blocks.2.1.1.bias\", \"conv_blocks.2.1.1.running_mean\", \"conv_blocks.2.1.1.running_var\", \"conv_blocks.2.2.weight\", \"conv_blocks.2.2.bias\", \"conv_blocks.2.2.1.weight\", \"conv_blocks.2.2.1.bias\", \"conv_blocks.2.2.1.running_mean\", \"conv_blocks.2.2.1.running_var\", \"conv_blocks.3.0.1.weight\", \"conv_blocks.3.0.1.bias\", \"conv_blocks.3.0.1.running_mean\", \"conv_blocks.3.0.1.running_var\", \"conv_blocks.3.1.1.weight\", \"conv_blocks.3.1.1.bias\", \"conv_blocks.3.1.1.running_mean\", \"conv_blocks.3.1.1.running_var\", \"conv_blocks.3.2.weight\", \"conv_blocks.3.2.bias\", \"conv_blocks.3.2.1.weight\", \"conv_blocks.3.2.1.bias\", \"conv_blocks.3.2.1.running_mean\", \"conv_blocks.3.2.1.running_var\". \n\tsize mismatch for conv_blocks.0.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.0.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.0.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.1.weight: copying a param with shape torch.Size([8, 1, 50]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m SleepDetectorLSTM()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Convert weights\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_cnn_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_keras_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m convert_lstm_weights(lstm_keras_weights, lstm_model)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Save PyTorch weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mconvert_cnn_weights\u001b[0;34m(keras_weights, pytorch_model)\u001b[0m\n\u001b[1;32m     32\u001b[0m     state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(dense_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel:0\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     33\u001b[0m     state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(dense_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias:0\u001b[39m\u001b[38;5;124m'\u001b[39m][:])\n\u001b[0;32m---> 35\u001b[0m \u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pytorch_model\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SleepDetectorCNN:\n\tUnexpected key(s) in state_dict: \"conv_blocks.0.0.1.weight\", \"conv_blocks.0.0.1.bias\", \"conv_blocks.0.0.1.running_mean\", \"conv_blocks.0.0.1.running_var\", \"conv_blocks.0.1.1.weight\", \"conv_blocks.0.1.1.bias\", \"conv_blocks.0.1.1.running_mean\", \"conv_blocks.0.1.1.running_var\", \"conv_blocks.0.2.weight\", \"conv_blocks.0.2.bias\", \"conv_blocks.0.2.1.weight\", \"conv_blocks.0.2.1.bias\", \"conv_blocks.0.2.1.running_mean\", \"conv_blocks.0.2.1.running_var\", \"conv_blocks.1.0.1.weight\", \"conv_blocks.1.0.1.bias\", \"conv_blocks.1.0.1.running_mean\", \"conv_blocks.1.0.1.running_var\", \"conv_blocks.1.1.1.weight\", \"conv_blocks.1.1.1.bias\", \"conv_blocks.1.1.1.running_mean\", \"conv_blocks.1.1.1.running_var\", \"conv_blocks.1.2.weight\", \"conv_blocks.1.2.bias\", \"conv_blocks.1.2.1.weight\", \"conv_blocks.1.2.1.bias\", \"conv_blocks.1.2.1.running_mean\", \"conv_blocks.1.2.1.running_var\", \"conv_blocks.2.0.1.weight\", \"conv_blocks.2.0.1.bias\", \"conv_blocks.2.0.1.running_mean\", \"conv_blocks.2.0.1.running_var\", \"conv_blocks.2.1.1.weight\", \"conv_blocks.2.1.1.bias\", \"conv_blocks.2.1.1.running_mean\", \"conv_blocks.2.1.1.running_var\", \"conv_blocks.2.2.weight\", \"conv_blocks.2.2.bias\", \"conv_blocks.2.2.1.weight\", \"conv_blocks.2.2.1.bias\", \"conv_blocks.2.2.1.running_mean\", \"conv_blocks.2.2.1.running_var\", \"conv_blocks.3.0.1.weight\", \"conv_blocks.3.0.1.bias\", \"conv_blocks.3.0.1.running_mean\", \"conv_blocks.3.0.1.running_var\", \"conv_blocks.3.1.1.weight\", \"conv_blocks.3.1.1.bias\", \"conv_blocks.3.1.1.running_mean\", \"conv_blocks.3.1.1.running_var\", \"conv_blocks.3.2.weight\", \"conv_blocks.3.2.bias\", \"conv_blocks.3.2.1.weight\", \"conv_blocks.3.2.1.bias\", \"conv_blocks.3.2.1.running_mean\", \"conv_blocks.3.2.1.running_var\". \n\tsize mismatch for conv_blocks.0.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.0.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.0.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.1.weight: copying a param with shape torch.Size([8, 1, 50]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8])."
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from sleepdetector import SleepDetectorCNN, SleepDetectorLSTM\n",
    "\n",
    "def load_keras_weights(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        return {key: np.array(f[key]) for key in f.keys()}\n",
    "\n",
    "def convert_cnn_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "    \n",
    "    # Drill down into the 'model_weights' group to access each layer\n",
    "    with h5py.File('cnn_weights.hdf5', 'r') as f:\n",
    "        model_weights_group = f['model_weights']\n",
    "        \n",
    "        # Convert convolutional layers\n",
    "        for i in range(4):  # 4 input signals\n",
    "            for j in range(3):  # 3 conv layers per signal\n",
    "                conv_group = model_weights_group[f'conv1d_{j*4+i+1}/conv1d_{j*4+i+1}']\n",
    "                bn_group = model_weights_group[f'batch_normalization_{j*4+i+1}/batch_normalization_{j*4+i+1}']\n",
    "                \n",
    "                state_dict[f'conv_blocks.{i}.{j}.weight'] = torch.from_numpy(conv_group['kernel:0'][:].transpose(2, 1, 0))\n",
    "                state_dict[f'conv_blocks.{i}.{j}.bias'] = torch.from_numpy(conv_group['bias:0'][:])\n",
    "                state_dict[f'conv_blocks.{i}.{j}.1.weight'] = torch.from_numpy(bn_group['gamma:0'][:])\n",
    "                state_dict[f'conv_blocks.{i}.{j}.1.bias'] = torch.from_numpy(bn_group['beta:0'][:])\n",
    "                state_dict[f'conv_blocks.{i}.{j}.1.running_mean'] = torch.from_numpy(bn_group['moving_mean:0'][:])\n",
    "                state_dict[f'conv_blocks.{i}.{j}.1.running_var'] = torch.from_numpy(bn_group['moving_variance:0'][:])\n",
    "        \n",
    "        # Convert final dense layer\n",
    "        dense_group = model_weights_group['dense_1/dense_1']\n",
    "        state_dict['fc.weight'] = torch.from_numpy(dense_group['kernel:0'][:].T)\n",
    "        state_dict['fc.bias'] = torch.from_numpy(dense_group['bias:0'][:])\n",
    "    \n",
    "    pytorch_model.load_state_dict(state_dict)\n",
    "    return pytorch_model\n",
    "\n",
    "def convert_lstm_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "    \n",
    "    # Similar approach for LSTM weights\n",
    "    with h5py.File('lstm_weights.h5', 'r') as f:\n",
    "        for i in range(4):  # 4 LSTM layers\n",
    "            forward_group = f[f'bidirectional_{i+1}/bidirectional_{i+1}/forward_lstm_{i+1}']\n",
    "            backward_group = f[f'bidirectional_{i+1}/bidirectional_{i+1}/backward_lstm_{i+1}']\n",
    "            \n",
    "            state_dict[f'lstm_layers.{i}.weight_ih_l0'] = torch.from_numpy(forward_group['kernel:0'][:].T)\n",
    "            state_dict[f'lstm_layers.{i}.weight_hh_l0'] = torch.from_numpy(forward_group['recurrent_kernel:0'][:].T)\n",
    "            state_dict[f'lstm_layers.{i}.bias_ih_l0'] = torch.from_numpy(forward_group['bias:0'][:256])\n",
    "            state_dict[f'lstm_layers.{i}.bias_hh_l0'] = torch.from_numpy(forward_group['bias:0'][256:])\n",
    "            \n",
    "            state_dict[f'lstm_layers.{i}.weight_ih_l0_reverse'] = torch.from_numpy(backward_group['kernel:0'][:].T)\n",
    "            state_dict[f'lstm_layers.{i}.weight_hh_l0_reverse'] = torch.from_numpy(backward_group['recurrent_kernel:0'][:].T)\n",
    "            state_dict[f'lstm_layers.{i}.bias_ih_l0_reverse'] = torch.from_numpy(backward_group['bias:0'][:256])\n",
    "            state_dict[f'lstm_layers.{i}.bias_hh_l0_reverse'] = torch.from_numpy(backward_group['bias:0'][256:])\n",
    "        \n",
    "        # Convert final dense layer\n",
    "        dense_group = f['time_distributed_1/time_distributed_1']\n",
    "        state_dict['fc.weight'] = torch.from_numpy(dense_group['kernel:0'][:].T)\n",
    "        state_dict['fc.bias'] = torch.from_numpy(dense_group['bias:0'][:])\n",
    "    \n",
    "    pytorch_model.load_state_dict(state_dict)\n",
    "    return pytorch_model\n",
    "\n",
    "# Load Keras weights\n",
    "cnn_keras_weights = load_keras_weights('cnn_weights.hdf5')\n",
    "lstm_keras_weights = load_keras_weights('lstm_weights.h5')\n",
    "\n",
    "# Initialize PyTorch models\n",
    "cnn_model = SleepDetectorCNN()\n",
    "lstm_model = SleepDetectorLSTM()\n",
    "\n",
    "# Convert weights\n",
    "cnn_model = convert_cnn_weights(cnn_keras_weights, cnn_model)\n",
    "lstm_model = convert_lstm_weights(lstm_keras_weights, lstm_model)\n",
    "\n",
    "# Save PyTorch weights\n",
    "torch.save(cnn_model.state_dict(), 'cnn_weights.pth')\n",
    "torch.save(lstm_model.state_dict(), 'lstm_weights.pth')\n",
    "\n",
    "print(\"Weight conversion completed. PyTorch weights saved as 'cnn_weights.pth' and 'lstm_weights.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SleepDetectorCNN:\n\tsize mismatch for conv_blocks.0.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.0.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.0.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.1.weight: copying a param with shape torch.Size([8, 1, 50]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m SleepDetectorLSTM()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Convert weights\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_cnn_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_keras_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m convert_lstm_weights(lstm_keras_weights, lstm_model)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Save PyTorch weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 100\u001b[0m, in \u001b[0;36mconvert_cnn_weights\u001b[0;34m(keras_weights, pytorch_model)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias:0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in dense_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Load state dict with strict=False to ignore unexpected/missing keys\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pytorch_model\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SleepDetectorCNN:\n\tsize mismatch for conv_blocks.0.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.0.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.1.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.0.weight: copying a param with shape torch.Size([32, 16, 8]) from checkpoint, the shape in current model is torch.Size([8, 1, 50]).\n\tsize mismatch for conv_blocks.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.2.1.weight: copying a param with shape torch.Size([8, 1, 50]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.weight: copying a param with shape torch.Size([16, 8, 8]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv_blocks.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8])."
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from sleepdetector import SleepDetectorCNN, SleepDetectorLSTM\n",
    "\n",
    "def load_keras_weights_cnn(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        model_weights = {}\n",
    "        for layer in f['model_weights'].keys():\n",
    "            if layer.startswith('conv1d') or layer.startswith('batch_normalization'):\n",
    "                nested_group = f['model_weights'][layer][layer]\n",
    "                layer_weights = {key: np.array(nested_group[key]) for key in nested_group.keys()}\n",
    "                model_weights[layer] = layer_weights\n",
    "            elif layer == 'dense_1':\n",
    "                dense_group = f['model_weights']['dense_1']['dense_1']\n",
    "                model_weights['dense_1'] = {key: np.array(dense_group[key]) for key in dense_group.keys()}\n",
    "        return model_weights\n",
    "\n",
    "def load_keras_weights_lstm(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        model_weights = {}\n",
    "        for top_key in f.keys():\n",
    "            if isinstance(f[top_key], h5py.Group):\n",
    "                if top_key.startswith('bidirectional') or top_key == 'time_distributed_1':\n",
    "                    model_weights[top_key] = {}\n",
    "                    for sub_key in f[top_key][top_key].keys():\n",
    "                        layer_group = f[top_key][top_key][sub_key]\n",
    "                        if isinstance(layer_group, h5py.Group):\n",
    "                            layer_weights = {key: np.array(layer_group[key]) for key in layer_group.keys()}\n",
    "                            model_weights[top_key][sub_key] = layer_weights\n",
    "                        else:\n",
    "                            # For datasets directly under the group, just add them\n",
    "                            model_weights[top_key][sub_key] = np.array(layer_group)\n",
    "            else:\n",
    "                # If there's no nested group, just add the dataset\n",
    "                model_weights[top_key] = np.array(f[top_key])\n",
    "        return model_weights\n",
    "\n",
    "def convert_cnn_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "\n",
    "    # Convert convolutional layers\n",
    "    for i in range(4):  # 4 input signals\n",
    "        for j in range(3):  # 3 conv layers per signal\n",
    "            layer_num = j*4 + i + 1\n",
    "            conv_key = f'conv1d_{layer_num}'\n",
    "            bn_key = f'batch_normalization_{layer_num}'\n",
    "\n",
    "            if conv_key in keras_weights and bn_key in keras_weights:\n",
    "                conv_weights = keras_weights[conv_key]\n",
    "                bn_weights = keras_weights[bn_key]\n",
    "\n",
    "                # Adjust keys for PyTorch model\n",
    "                if 'kernel:0' in conv_weights:\n",
    "                    state_dict[f'conv_blocks.{i}.{j}.weight'] = torch.from_numpy(conv_weights['kernel:0'].transpose(2, 1, 0))\n",
    "                else:\n",
    "                    print(f\"Warning: 'kernel:0' not found in {conv_key}\")\n",
    "\n",
    "                if 'bias:0' in conv_weights:\n",
    "                    state_dict[f'conv_blocks.{i}.{j}.bias'] = torch.from_numpy(conv_weights['bias:0'])\n",
    "                else:\n",
    "                    print(f\"Warning: 'bias:0' not found in {conv_key}\")\n",
    "\n",
    "                if f'conv_blocks.{i}.{j}.1.weight' in state_dict:\n",
    "                    if 'gamma:0' in bn_weights:\n",
    "                        state_dict[f'conv_blocks.{i}.{j}.1.weight'] = torch.from_numpy(bn_weights['gamma:0'])\n",
    "                    else:\n",
    "                        print(f\"Warning: 'gamma:0' not found in {bn_key}\")\n",
    "\n",
    "                if f'conv_blocks.{i}.{j}.1.bias' in state_dict:\n",
    "                    if 'beta:0' in bn_weights:\n",
    "                        state_dict[f'conv_blocks.{i}.{j}.1.bias'] = torch.from_numpy(bn_weights['beta:0'])\n",
    "                    else:\n",
    "                        print(f\"Warning: 'beta:0' not found in {bn_key}\")\n",
    "\n",
    "                if f'conv_blocks.{i}.{j}.1.running_mean' in state_dict:\n",
    "                    if 'moving_mean:0' in bn_weights:\n",
    "                        state_dict[f'conv_blocks.{i}.{j}.1.running_mean'] = torch.from_numpy(bn_weights['moving_mean:0'])\n",
    "                    else:\n",
    "                        print(f\"Warning: 'moving_mean:0' not found in {bn_key}\")\n",
    "\n",
    "                if f'conv_blocks.{i}.{j}.1.running_var' in state_dict:\n",
    "                    if 'moving_variance:0' in bn_weights:\n",
    "                        state_dict[f'conv_blocks.{i}.{j}.1.running_var'] = torch.from_numpy(bn_weights['moving_variance:0'])\n",
    "                    else:\n",
    "                        print(f\"Warning: 'moving_variance:0' not found in {bn_key}\")\n",
    "\n",
    "    # Convert final dense layer\n",
    "    dense_weights = keras_weights['dense_1']\n",
    "    if 'kernel:0' in dense_weights:\n",
    "        state_dict['fc.weight'] = torch.from_numpy(dense_weights['kernel:0'].T)\n",
    "    else:\n",
    "        print(f\"Warning: 'kernel:0' not found in dense_1\")\n",
    "    if 'bias:0' in dense_weights:\n",
    "        state_dict['fc.bias'] = torch.from_numpy(dense_weights['bias:0'])\n",
    "    else:\n",
    "        print(f\"Warning: 'bias:0' not found in dense_1\")\n",
    "\n",
    "    # Load state dict with strict=False to ignore unexpected/missing keys\n",
    "    pytorch_model.load_state_dict(state_dict, strict=False)\n",
    "    return pytorch_model\n",
    "\n",
    "def convert_lstm_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "\n",
    "    # Convert LSTM layers\n",
    "    for i in range(4):  # 4 LSTM layers\n",
    "        forward_lstm_key = f'bidirectional_{i+1}'\n",
    "        backward_lstm_key = f'bidirectional_{i+1}'\n",
    "\n",
    "        forward_weights = keras_weights[forward_lstm_key][f'forward_lstm_{i+1}']\n",
    "        backward_weights = keras_weights[backward_lstm_key][f'backward_lstm_{i+1}']\n",
    "\n",
    "        # Forward LSTM\n",
    "        state_dict[f'lstm_layers.{i}.weight_ih_l0'] = torch.from_numpy(forward_weights['kernel:0'].T)\n",
    "        state_dict[f'lstm_layers.{i}.weight_hh_l0'] = torch.from_numpy(forward_weights['recurrent_kernel:0'].T)\n",
    "        state_dict[f'lstm_layers.{i}.bias_ih_l0'] = torch.from_numpy(forward_weights['bias:0'][:256])\n",
    "        state_dict[f'lstm_layers.{i}.bias_hh_l0'] = torch.from_numpy(forward_weights['bias:0'][256:])\n",
    "\n",
    "        # Backward LSTM\n",
    "        state_dict[f'lstm_layers.{i}.weight_ih_l0_reverse'] = torch.from_numpy(backward_weights['kernel:0'].T)\n",
    "        state_dict[f'lstm_layers.{i}.weight_hh_l0_reverse'] = torch.from_numpy(backward_weights['recurrent_kernel:0'].T)\n",
    "        state_dict[f'lstm_layers.{i}.bias_ih_l0_reverse'] = torch.from_numpy(backward_weights['bias:0'][:256])\n",
    "        state_dict[f'lstm_layers.{i}.bias_hh_l0_reverse'] = torch.from_numpy(backward_weights['bias:0'][256:])\n",
    "\n",
    "    # Convert final dense layer\n",
    "    dense_weights = keras_weights['time_distributed_1']['time_distributed_1']\n",
    "    state_dict['fc.weight'] = torch.from_numpy(dense_weights['kernel:0'].T)\n",
    "    state_dict['fc.bias'] = torch.from_numpy(dense_weights['bias:0'])\n",
    "\n",
    "    # Load state dict with strict=False to ignore unexpected/missing keys\n",
    "    pytorch_model.load_state_dict(state_dict, strict=False)\n",
    "    return pytorch_model\n",
    "\n",
    "# Load Keras weights\n",
    "cnn_keras_weights = load_keras_weights_cnn('cnn_weights.hdf5')\n",
    "lstm_keras_weights = load_keras_weights_lstm('lstm_weights.h5')\n",
    "\n",
    "# Initialize PyTorch models\n",
    "cnn_model = SleepDetectorCNN()\n",
    "lstm_model = SleepDetectorLSTM()\n",
    "\n",
    "# Convert weights\n",
    "cnn_model = convert_cnn_weights(cnn_keras_weights, cnn_model)\n",
    "lstm_model = convert_lstm_weights(lstm_keras_weights, lstm_model)\n",
    "\n",
    "# Save PyTorch weights\n",
    "torch.save(cnn_model.state_dict(), 'cnn_weights.pth')\n",
    "torch.save(lstm_model.state_dict(), 'lstm_weights.pth')\n",
    "\n",
    "print(\"Weight conversion completed. PyTorch weights saved as 'cnn_weights.pth' and 'lstm_weights.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SleepDetectorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SleepDetectorCNN, self).__init__()\n",
    "\n",
    "        # 4 input signals\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(4):\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=8, kernel_size=50),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(8),\n",
    "                nn.MaxPool1d(kernel_size=8),\n",
    "\n",
    "                nn.Conv1d(in_channels=8, out_channels=16, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.MaxPool1d(kernel_size=8),\n",
    "\n",
    "                nn.Conv1d(in_channels=16, out_channels=32, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.MaxPool1d(kernel_size=8),\n",
    "            )\n",
    "            self.conv_blocks.append(conv_block)\n",
    "\n",
    "        # Flatten and fully connected layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(4 * 32 * (640 // (8*8*8)), 5)  # Assuming final pooled size is 640/(8*8*8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(4):\n",
    "            out = self.conv_blocks[i](x[i])\n",
    "            outputs.append(out)\n",
    "        \n",
    "        x = torch.cat(outputs, dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SleepDetectorLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SleepDetectorLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size=640, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True),\n",
    "            nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True),\n",
    "            nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True),\n",
    "            nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True),\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(128, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "        \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Load the Keras CNN weights\n",
    "def load_keras_weights_cnn(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        model_weights = {}\n",
    "        for layer_name in ['conv1d_1', 'conv1d_2', 'conv1d_3', 'conv1d_4', 'conv1d_5',\n",
    "                           'conv1d_6', 'conv1d_7', 'conv1d_8', 'conv1d_9', 'conv1d_10',\n",
    "                           'conv1d_11', 'conv1d_12', 'dense_1']:\n",
    "            try:\n",
    "                layer_group = f[f'{layer_name}/{layer_name}']\n",
    "                layer_weights = {key: np.array(layer_group[key]) for key in layer_group.keys()}\n",
    "                model_weights[layer_name] = layer_weights\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Missing {layer_name}/{layer_name} in Keras weights.\")\n",
    "        for bn_name in ['batch_normalization_1', 'batch_normalization_2', 'batch_normalization_3',\n",
    "                        'batch_normalization_4', 'batch_normalization_5', 'batch_normalization_6',\n",
    "                        'batch_normalization_7', 'batch_normalization_8', 'batch_normalization_9',\n",
    "                        'batch_normalization_10', 'batch_normalization_11', 'batch_normalization_12']:\n",
    "            try:\n",
    "                bn_group = f[f'{bn_name}/{bn_name}']\n",
    "                bn_weights = {key: np.array(bn_group[key]) for key in bn_group.keys()}\n",
    "                model_weights[bn_name] = bn_weights\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Missing {bn_name}/{bn_name} in Keras weights.\")\n",
    "        return model_weights\n",
    "\n",
    "# Load the Keras LSTM weights\n",
    "def load_keras_weights_lstm(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        model_weights = {}\n",
    "        for layer_name in ['bidirectional_1', 'bidirectional_2', 'bidirectional_3', 'bidirectional_4']:\n",
    "            model_weights[layer_name] = {}\n",
    "            for sub_layer_name in ['forward_lstm_1', 'backward_lstm_1', 'forward_lstm_2', 'backward_lstm_2',\n",
    "                                   'forward_lstm_3', 'backward_lstm_3', 'forward_lstm_4', 'backward_lstm_4']:\n",
    "                try:\n",
    "                    layer_group = f[f'{layer_name}/{layer_name}/{sub_layer_name}']\n",
    "                    layer_weights = {key: np.array(layer_group[key]) for key in layer_group.keys()}\n",
    "                    model_weights[layer_name][sub_layer_name] = layer_weights\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Missing {layer_name}/{layer_name}/{sub_layer_name} in Keras weights.\")\n",
    "        try:\n",
    "            td_group = f['time_distributed_1/time_distributed_1']\n",
    "            td_weights = {key: np.array(td_group[key]) for key in td_group.keys()}\n",
    "            model_weights['time_distributed_1'] = td_weights\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Missing time_distributed_1/time_distributed_1 in Keras weights.\")\n",
    "        return model_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "\n",
    "    for i in range(1, 13):  # Assuming 12 conv layers\n",
    "        conv_key = f'model_weights/conv1d_{i}/conv1d_{i}'\n",
    "        bn_key = f'model_weights/batch_normalization_{i}/batch_normalization_{i}'\n",
    "\n",
    "        if conv_key in keras_weights and bn_key in keras_weights:\n",
    "            print(f\"Loading weights for {conv_key} and {bn_key}\")\n",
    "            conv_weights = keras_weights[conv_key]\n",
    "            bn_weights = keras_weights[bn_key]\n",
    "\n",
    "            state_dict[f'conv_blocks.{i-1}.0.weight'] = torch.from_numpy(conv_weights['kernel:0'].transpose(2, 1, 0))\n",
    "            state_dict[f'conv_blocks.{i-1}.0.bias'] = torch.from_numpy(conv_weights['bias:0'])\n",
    "            state_dict[f'conv_blocks.{i-1}.1.weight'] = torch.from_numpy(bn_weights['gamma:0'])\n",
    "            state_dict[f'conv_blocks.{i-1}.1.bias'] = torch.from_numpy(bn_weights['beta:0'])\n",
    "            state_dict[f'conv_blocks.{i-1}.1.running_mean'] = torch.from_numpy(bn_weights['moving_mean:0'])\n",
    "            state_dict[f'conv_blocks.{i-1}.1.running_var'] = torch.from_numpy(bn_weights['moving_variance:0'])\n",
    "        else:\n",
    "            print(f\"Warning: Missing {conv_key} or {bn_key} in Keras weights.\")\n",
    "\n",
    "    dense_key = 'model_weights/dense_1/dense_1'\n",
    "    if dense_key in keras_weights:\n",
    "        print(f\"Loading weights for {dense_key}\")\n",
    "        dense_weights = keras_weights[dense_key]\n",
    "        state_dict['fc.weight'] = torch.from_numpy(dense_weights['kernel:0'].T)\n",
    "        state_dict['fc.bias'] = torch.from_numpy(dense_weights['bias:0'])\n",
    "    else:\n",
    "        print(f\"Warning: Missing {dense_key} in Keras weights.\")\n",
    "\n",
    "    pytorch_model.load_state_dict(state_dict, strict=False)\n",
    "    return pytorch_model\n",
    "\n",
    "def convert_lstm_weights(keras_weights, pytorch_model):\n",
    "    state_dict = pytorch_model.state_dict()\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        lstm_key = f'model_weights/bidirectional_{i}/bidirectional_{i}'\n",
    "        if lstm_key in keras_weights:\n",
    "            print(f\"Loading weights for {lstm_key}\")\n",
    "            forward_key = f'{lstm_key}/forward_lstm_{i}'\n",
    "            backward_key = f'{lstm_key}/backward_lstm_{i}'\n",
    "\n",
    "            if forward_key in keras_weights and backward_key in keras_weights:\n",
    "                forward_weights = keras_weights[forward_key]\n",
    "                backward_weights = keras_weights[backward_key]\n",
    "\n",
    "                state_dict[f'lstm_blocks.{i-1}.weight_ih_l0'] = torch.from_numpy(forward_weights['kernel:0'].T)\n",
    "                state_dict[f'lstm_blocks.{i-1}.weight_hh_l0'] = torch.from_numpy(forward_weights['recurrent_kernel:0'].T)\n",
    "                state_dict[f'lstm_blocks.{i-1}.bias_ih_l0'] = torch.from_numpy(forward_weights['bias:0'])\n",
    "\n",
    "                state_dict[f'lstm_blocks.{i-1}.weight_ih_l0_reverse'] = torch.from_numpy(backward_weights['kernel:0'].T)\n",
    "                state_dict[f'lstm_blocks.{i-1}.weight_hh_l0_reverse'] = torch.from_numpy(backward_weights['recurrent_kernel:0'].T)\n",
    "                state_dict[f'lstm_blocks.{i-1}.bias_ih_l0_reverse'] = torch.from_numpy(backward_weights['bias:0'])\n",
    "            else:\n",
    "                print(f\"Warning: Missing {forward_key} or {backward_key} in Keras weights.\")\n",
    "        else:\n",
    "            print(f\"Warning: Missing {lstm_key} in Keras weights.\")\n",
    "\n",
    "    dense_key = 'model_weights/time_distributed_1/time_distributed_1'\n",
    "    if dense_key in keras_weights:\n",
    "        print(f\"Loading weights for {dense_key}\")\n",
    "        dense_weights = keras_weights[dense_key]\n",
    "        state_dict['fc.weight'] = torch.from_numpy(dense_weights['kernel:0'].T)\n",
    "        state_dict['fc.bias'] = torch.from_numpy(dense_weights['bias:0'])\n",
    "    else:\n",
    "        print(f\"Warning: Missing {dense_key} in Keras weights.\")\n",
    "\n",
    "    pytorch_model.load_state_dict(state_dict, strict=False)\n",
    "    return pytorch_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing conv1d_1/conv1d_1 in Keras weights.\n",
      "Warning: Missing conv1d_2/conv1d_2 in Keras weights.\n",
      "Warning: Missing conv1d_3/conv1d_3 in Keras weights.\n",
      "Warning: Missing conv1d_4/conv1d_4 in Keras weights.\n",
      "Warning: Missing conv1d_5/conv1d_5 in Keras weights.\n",
      "Warning: Missing conv1d_6/conv1d_6 in Keras weights.\n",
      "Warning: Missing conv1d_7/conv1d_7 in Keras weights.\n",
      "Warning: Missing conv1d_8/conv1d_8 in Keras weights.\n",
      "Warning: Missing conv1d_9/conv1d_9 in Keras weights.\n",
      "Warning: Missing conv1d_10/conv1d_10 in Keras weights.\n",
      "Warning: Missing conv1d_11/conv1d_11 in Keras weights.\n",
      "Warning: Missing conv1d_12/conv1d_12 in Keras weights.\n",
      "Warning: Missing dense_1/dense_1 in Keras weights.\n",
      "Warning: Missing batch_normalization_1/batch_normalization_1 in Keras weights.\n",
      "Warning: Missing batch_normalization_2/batch_normalization_2 in Keras weights.\n",
      "Warning: Missing batch_normalization_3/batch_normalization_3 in Keras weights.\n",
      "Warning: Missing batch_normalization_4/batch_normalization_4 in Keras weights.\n",
      "Warning: Missing batch_normalization_5/batch_normalization_5 in Keras weights.\n",
      "Warning: Missing batch_normalization_6/batch_normalization_6 in Keras weights.\n",
      "Warning: Missing batch_normalization_7/batch_normalization_7 in Keras weights.\n",
      "Warning: Missing batch_normalization_8/batch_normalization_8 in Keras weights.\n",
      "Warning: Missing batch_normalization_9/batch_normalization_9 in Keras weights.\n",
      "Warning: Missing batch_normalization_10/batch_normalization_10 in Keras weights.\n",
      "Warning: Missing batch_normalization_11/batch_normalization_11 in Keras weights.\n",
      "Warning: Missing batch_normalization_12/batch_normalization_12 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/forward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/backward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/forward_lstm_3 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/backward_lstm_3 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/forward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_1/bidirectional_1/backward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/forward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/backward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/forward_lstm_3 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/backward_lstm_3 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/forward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_2/bidirectional_2/backward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/forward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/backward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/forward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/backward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/forward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_3/bidirectional_3/backward_lstm_4 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/forward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/backward_lstm_1 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/forward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/backward_lstm_2 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/forward_lstm_3 in Keras weights.\n",
      "Warning: Missing bidirectional_4/bidirectional_4/backward_lstm_3 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_1/conv1d_1 or model_weights/batch_normalization_1/batch_normalization_1 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_2/conv1d_2 or model_weights/batch_normalization_2/batch_normalization_2 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_3/conv1d_3 or model_weights/batch_normalization_3/batch_normalization_3 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_4/conv1d_4 or model_weights/batch_normalization_4/batch_normalization_4 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_5/conv1d_5 or model_weights/batch_normalization_5/batch_normalization_5 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_6/conv1d_6 or model_weights/batch_normalization_6/batch_normalization_6 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_7/conv1d_7 or model_weights/batch_normalization_7/batch_normalization_7 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_8/conv1d_8 or model_weights/batch_normalization_8/batch_normalization_8 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_9/conv1d_9 or model_weights/batch_normalization_9/batch_normalization_9 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_10/conv1d_10 or model_weights/batch_normalization_10/batch_normalization_10 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_11/conv1d_11 or model_weights/batch_normalization_11/batch_normalization_11 in Keras weights.\n",
      "Warning: Missing model_weights/conv1d_12/conv1d_12 or model_weights/batch_normalization_12/batch_normalization_12 in Keras weights.\n",
      "Warning: Missing model_weights/dense_1/dense_1 in Keras weights.\n",
      "Warning: Missing model_weights/bidirectional_1/bidirectional_1 in Keras weights.\n",
      "Warning: Missing model_weights/bidirectional_2/bidirectional_2 in Keras weights.\n",
      "Warning: Missing model_weights/bidirectional_3/bidirectional_3 in Keras weights.\n",
      "Warning: Missing model_weights/bidirectional_4/bidirectional_4 in Keras weights.\n",
      "Warning: Missing model_weights/time_distributed_1/time_distributed_1 in Keras weights.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load Keras weights\n",
    "cnn_keras_weights = load_keras_weights_cnn('cnn_weights.hdf5')\n",
    "lstm_keras_weights = load_keras_weights_lstm('lstm_weights.h5')\n",
    "\n",
    "# Initialize PyTorch models\n",
    "cnn_model = SleepDetectorCNN()\n",
    "lstm_model = SleepDetectorLSTM()\n",
    "\n",
    "# Convert weights\n",
    "cnn_model = convert_cnn_weights(cnn_keras_weights, cnn_model)\n",
    "lstm_model = convert_lstm_weights(lstm_keras_weights, lstm_model)\n",
    "\n",
    "# Save PyTorch weights\n",
    "if cnn_model:\n",
    "    torch.save(cnn_model.state_dict(), 'cnn_weights.pth')\n",
    "else:\n",
    "    print(\"Error: CNN model is None. Conversion failed.\")\n",
    "\n",
    "if lstm_model:\n",
    "    torch.save(lstm_model.state_dict(), 'lstm_weights.pth')\n",
    "else:\n",
    "    print(\"Error: LSTM model is None. Conversion failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Weights Structure:\n",
      "model_weights\n",
      "model_weights/activation_1\n",
      "model_weights/activation_10\n",
      "model_weights/activation_11\n",
      "model_weights/activation_12\n",
      "model_weights/activation_2\n",
      "model_weights/activation_3\n",
      "model_weights/activation_4\n",
      "model_weights/activation_5\n",
      "model_weights/activation_6\n",
      "model_weights/activation_7\n",
      "model_weights/activation_8\n",
      "model_weights/activation_9\n",
      "model_weights/batch_normalization_1\n",
      "model_weights/batch_normalization_1/batch_normalization_1\n",
      "model_weights/batch_normalization_1/batch_normalization_1/beta:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/gamma:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/moving_mean:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/moving_variance:0\n",
      "model_weights/batch_normalization_10\n",
      "model_weights/batch_normalization_10/batch_normalization_10\n",
      "model_weights/batch_normalization_10/batch_normalization_10/beta:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/gamma:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/moving_mean:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/moving_variance:0\n",
      "model_weights/batch_normalization_11\n",
      "model_weights/batch_normalization_11/batch_normalization_11\n",
      "model_weights/batch_normalization_11/batch_normalization_11/beta:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/gamma:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/moving_mean:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/moving_variance:0\n",
      "model_weights/batch_normalization_12\n",
      "model_weights/batch_normalization_12/batch_normalization_12\n",
      "model_weights/batch_normalization_12/batch_normalization_12/beta:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/gamma:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/moving_mean:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/moving_variance:0\n",
      "model_weights/batch_normalization_2\n",
      "model_weights/batch_normalization_2/batch_normalization_2\n",
      "model_weights/batch_normalization_2/batch_normalization_2/beta:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/gamma:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/moving_mean:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/moving_variance:0\n",
      "model_weights/batch_normalization_3\n",
      "model_weights/batch_normalization_3/batch_normalization_3\n",
      "model_weights/batch_normalization_3/batch_normalization_3/beta:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/gamma:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/moving_mean:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/moving_variance:0\n",
      "model_weights/batch_normalization_4\n",
      "model_weights/batch_normalization_4/batch_normalization_4\n",
      "model_weights/batch_normalization_4/batch_normalization_4/beta:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/gamma:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/moving_mean:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/moving_variance:0\n",
      "model_weights/batch_normalization_5\n",
      "model_weights/batch_normalization_5/batch_normalization_5\n",
      "model_weights/batch_normalization_5/batch_normalization_5/beta:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/gamma:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/moving_mean:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/moving_variance:0\n",
      "model_weights/batch_normalization_6\n",
      "model_weights/batch_normalization_6/batch_normalization_6\n",
      "model_weights/batch_normalization_6/batch_normalization_6/beta:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/gamma:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/moving_mean:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/moving_variance:0\n",
      "model_weights/batch_normalization_7\n",
      "model_weights/batch_normalization_7/batch_normalization_7\n",
      "model_weights/batch_normalization_7/batch_normalization_7/beta:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/gamma:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/moving_mean:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/moving_variance:0\n",
      "model_weights/batch_normalization_8\n",
      "model_weights/batch_normalization_8/batch_normalization_8\n",
      "model_weights/batch_normalization_8/batch_normalization_8/beta:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/gamma:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/moving_mean:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/moving_variance:0\n",
      "model_weights/batch_normalization_9\n",
      "model_weights/batch_normalization_9/batch_normalization_9\n",
      "model_weights/batch_normalization_9/batch_normalization_9/beta:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/gamma:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/moving_mean:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/moving_variance:0\n",
      "model_weights/concatenate_1\n",
      "model_weights/conv1d_1\n",
      "model_weights/conv1d_1/conv1d_1\n",
      "model_weights/conv1d_1/conv1d_1/bias:0\n",
      "model_weights/conv1d_1/conv1d_1/kernel:0\n",
      "model_weights/conv1d_10\n",
      "model_weights/conv1d_10/conv1d_10\n",
      "model_weights/conv1d_10/conv1d_10/bias:0\n",
      "model_weights/conv1d_10/conv1d_10/kernel:0\n",
      "model_weights/conv1d_11\n",
      "model_weights/conv1d_11/conv1d_11\n",
      "model_weights/conv1d_11/conv1d_11/bias:0\n",
      "model_weights/conv1d_11/conv1d_11/kernel:0\n",
      "model_weights/conv1d_12\n",
      "model_weights/conv1d_12/conv1d_12\n",
      "model_weights/conv1d_12/conv1d_12/bias:0\n",
      "model_weights/conv1d_12/conv1d_12/kernel:0\n",
      "model_weights/conv1d_2\n",
      "model_weights/conv1d_2/conv1d_2\n",
      "model_weights/conv1d_2/conv1d_2/bias:0\n",
      "model_weights/conv1d_2/conv1d_2/kernel:0\n",
      "model_weights/conv1d_3\n",
      "model_weights/conv1d_3/conv1d_3\n",
      "model_weights/conv1d_3/conv1d_3/bias:0\n",
      "model_weights/conv1d_3/conv1d_3/kernel:0\n",
      "model_weights/conv1d_4\n",
      "model_weights/conv1d_4/conv1d_4\n",
      "model_weights/conv1d_4/conv1d_4/bias:0\n",
      "model_weights/conv1d_4/conv1d_4/kernel:0\n",
      "model_weights/conv1d_5\n",
      "model_weights/conv1d_5/conv1d_5\n",
      "model_weights/conv1d_5/conv1d_5/bias:0\n",
      "model_weights/conv1d_5/conv1d_5/kernel:0\n",
      "model_weights/conv1d_6\n",
      "model_weights/conv1d_6/conv1d_6\n",
      "model_weights/conv1d_6/conv1d_6/bias:0\n",
      "model_weights/conv1d_6/conv1d_6/kernel:0\n",
      "model_weights/conv1d_7\n",
      "model_weights/conv1d_7/conv1d_7\n",
      "model_weights/conv1d_7/conv1d_7/bias:0\n",
      "model_weights/conv1d_7/conv1d_7/kernel:0\n",
      "model_weights/conv1d_8\n",
      "model_weights/conv1d_8/conv1d_8\n",
      "model_weights/conv1d_8/conv1d_8/bias:0\n",
      "model_weights/conv1d_8/conv1d_8/kernel:0\n",
      "model_weights/conv1d_9\n",
      "model_weights/conv1d_9/conv1d_9\n",
      "model_weights/conv1d_9/conv1d_9/bias:0\n",
      "model_weights/conv1d_9/conv1d_9/kernel:0\n",
      "model_weights/dense_1\n",
      "model_weights/dense_1/dense_1\n",
      "model_weights/dense_1/dense_1/bias:0\n",
      "model_weights/dense_1/dense_1/kernel:0\n",
      "model_weights/dropout_1\n",
      "model_weights/dropout_10\n",
      "model_weights/dropout_11\n",
      "model_weights/dropout_12\n",
      "model_weights/dropout_2\n",
      "model_weights/dropout_3\n",
      "model_weights/dropout_4\n",
      "model_weights/dropout_5\n",
      "model_weights/dropout_6\n",
      "model_weights/dropout_7\n",
      "model_weights/dropout_8\n",
      "model_weights/dropout_9\n",
      "model_weights/flatten_1\n",
      "model_weights/input_1\n",
      "model_weights/input_2\n",
      "model_weights/input_3\n",
      "model_weights/input_4\n",
      "model_weights/max_pooling1d_1\n",
      "model_weights/max_pooling1d_10\n",
      "model_weights/max_pooling1d_11\n",
      "model_weights/max_pooling1d_12\n",
      "model_weights/max_pooling1d_2\n",
      "model_weights/max_pooling1d_3\n",
      "model_weights/max_pooling1d_4\n",
      "model_weights/max_pooling1d_5\n",
      "model_weights/max_pooling1d_6\n",
      "model_weights/max_pooling1d_7\n",
      "model_weights/max_pooling1d_8\n",
      "model_weights/max_pooling1d_9\n",
      "LSTM Weights Structure:\n",
      "bidirectional_1\n",
      "bidirectional_1/bidirectional_1\n",
      "bidirectional_1/bidirectional_1/backward_lstm_1\n",
      "bidirectional_1/bidirectional_1/backward_lstm_1/bias:0\n",
      "bidirectional_1/bidirectional_1/backward_lstm_1/kernel:0\n",
      "bidirectional_1/bidirectional_1/backward_lstm_1/recurrent_kernel:0\n",
      "bidirectional_1/bidirectional_1/forward_lstm_1\n",
      "bidirectional_1/bidirectional_1/forward_lstm_1/bias:0\n",
      "bidirectional_1/bidirectional_1/forward_lstm_1/kernel:0\n",
      "bidirectional_1/bidirectional_1/forward_lstm_1/recurrent_kernel:0\n",
      "bidirectional_2\n",
      "bidirectional_2/bidirectional_2\n",
      "bidirectional_2/bidirectional_2/backward_lstm_2\n",
      "bidirectional_2/bidirectional_2/backward_lstm_2/bias:0\n",
      "bidirectional_2/bidirectional_2/backward_lstm_2/kernel:0\n",
      "bidirectional_2/bidirectional_2/backward_lstm_2/recurrent_kernel:0\n",
      "bidirectional_2/bidirectional_2/forward_lstm_2\n",
      "bidirectional_2/bidirectional_2/forward_lstm_2/bias:0\n",
      "bidirectional_2/bidirectional_2/forward_lstm_2/kernel:0\n",
      "bidirectional_2/bidirectional_2/forward_lstm_2/recurrent_kernel:0\n",
      "bidirectional_3\n",
      "bidirectional_3/bidirectional_3\n",
      "bidirectional_3/bidirectional_3/backward_lstm_3\n",
      "bidirectional_3/bidirectional_3/backward_lstm_3/bias:0\n",
      "bidirectional_3/bidirectional_3/backward_lstm_3/kernel:0\n",
      "bidirectional_3/bidirectional_3/backward_lstm_3/recurrent_kernel:0\n",
      "bidirectional_3/bidirectional_3/forward_lstm_3\n",
      "bidirectional_3/bidirectional_3/forward_lstm_3/bias:0\n",
      "bidirectional_3/bidirectional_3/forward_lstm_3/kernel:0\n",
      "bidirectional_3/bidirectional_3/forward_lstm_3/recurrent_kernel:0\n",
      "bidirectional_4\n",
      "bidirectional_4/bidirectional_4\n",
      "bidirectional_4/bidirectional_4/backward_lstm_4\n",
      "bidirectional_4/bidirectional_4/backward_lstm_4/bias:0\n",
      "bidirectional_4/bidirectional_4/backward_lstm_4/kernel:0\n",
      "bidirectional_4/bidirectional_4/backward_lstm_4/recurrent_kernel:0\n",
      "bidirectional_4/bidirectional_4/forward_lstm_4\n",
      "bidirectional_4/bidirectional_4/forward_lstm_4/bias:0\n",
      "bidirectional_4/bidirectional_4/forward_lstm_4/kernel:0\n",
      "bidirectional_4/bidirectional_4/forward_lstm_4/recurrent_kernel:0\n",
      "input_1\n",
      "time_distributed_1\n",
      "time_distributed_1/time_distributed_1\n",
      "time_distributed_1/time_distributed_1/bias:0\n",
      "time_distributed_1/time_distributed_1/kernel:0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_hdf5_structure(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        def print_structure(name, obj):\n",
    "            print(name)\n",
    "        f.visititems(print_structure)\n",
    "\n",
    "print(\"CNN Weights Structure:\")\n",
    "print_hdf5_structure('cnn_weights.hdf5')\n",
    "\n",
    "print(\"LSTM Weights Structure:\")\n",
    "print_hdf5_structure('lstm_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Open the H5 file\n",
    "with h5py.File('./lstm_weights.h5', 'r') as f:\n",
    "    # Initialize your PyTorch model\n",
    "    model = SleepDetectorLSTM()\n",
    "    \n",
    "    # Load weights for each LSTM layer\n",
    "    for i, lstm in enumerate(model.lstm_layers):\n",
    "        # Extract weights from H5 file\n",
    "        kernel = f[f'bidirectional_{i+1}/bidirectional_{i+1}/forward_lstm_{i+1}/kernel:0'][:]\n",
    "        recurrent_kernel = f[f'bidirectional_{i+1}/bidirectional_{i+1}/forward_lstm_{i+1}/recurrent_kernel:0'][:]\n",
    "        bias = f[f'bidirectional_{i+1}/bidirectional_{i+1}/forward_lstm_{i+1}/bias:0'][:]\n",
    "        \n",
    "        # Convert weights to PyTorch format\n",
    "        kernel = torch.tensor(kernel).unsqueeze(2).permute(1, 0, 2).contiguous()\n",
    "        recurrent_kernel = torch.tensor(recurrent_kernel).unsqueeze(2).permute(1, 0, 2).contiguous()\n",
    "\n",
    "        \n",
    "        bias = torch.tensor(bias)\n",
    "        \n",
    "        # Load weights into PyTorch LSTM layer\n",
    "        lstm.weight_ih_l0 = nn.Parameter(kernel[:64, :, :])\n",
    "        lstm.weight_hh_l0 = nn.Parameter(recurrent_kernel[:64, :, :])\n",
    "        lstm.bias_ih_l0 = nn.Parameter(bias[:64])\n",
    "        lstm.bias_hh_l0 = nn.Parameter(bias[64:128])\n",
    "        \n",
    "        # Repeat for backward LSTM\n",
    "        kernel = f[f'bidirectional_{i+1}/bidirectional_{i+1}/backward_lstm_{i+1}/kernel:0'][:]\n",
    "        recurrent_kernel = f[f'bidirectional_{i+1}/bidirectional_{i+1}/backward_lstm_{i+1}/recurrent_kernel:0'][:]\n",
    "        bias = f[f'bidirectional_{i+1}/bidirectional_{i+1}/backward_lstm_{i+1}/bias:0'][:]\n",
    "        \n",
    "        kernel = torch.tensor(kernel).unsqueeze(2).permute(1, 0, 2).contiguous()\n",
    "        recurrent_kernel = torch.tensor(recurrent_kernel).unsqueeze(2).permute(1, 0, 2).contiguous()\n",
    "        bias = torch.tensor(bias)\n",
    "        \n",
    "        lstm.weight_ih_l0_reverse = nn.Parameter(kernel[:64, :, :])\n",
    "        lstm.weight_hh_l0_reverse = nn.Parameter(recurrent_kernel[:64, :, :])\n",
    "        lstm.bias_ih_l0_reverse = nn.Parameter(bias[:64])\n",
    "        lstm.bias_hh_l0_reverse = nn.Parameter(bias[64:128])\n",
    "    \n",
    "    # Load weights for the final FC layer\n",
    "    kernel = f['time_distributed_1/time_distributed_1/kernel:0'][:]\n",
    "    bias = f['time_distributed_1/time_distributed_1/bias:0'][:]\n",
    "    \n",
    "    model.fc.weight = nn.Parameter(torch.tensor(kernel).permute(1, 0).contiguous())\n",
    "    model.fc.bias = nn.Parameter(torch.tensor(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file structure:\n",
      "<KeysViewHDF5 ['model_weights']>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accessing a group is done with bytes or str, not <class 'slice'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1d_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m model_weights:\n\u001b[0;32m---> 23\u001b[0m     conv_block[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mmodel_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/h5py/_hl/group.py:359\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m h5o\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e(name), lapl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lapl)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n\u001b[1;32m    362\u001b[0m otype \u001b[38;5;241m=\u001b[39m h5i\u001b[38;5;241m.\u001b[39mget_type(oid)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mGROUP:\n",
      "\u001b[0;31mTypeError\u001b[0m: Accessing a group is done with bytes or str, not <class 'slice'>"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Open the H5 file\n",
    "with h5py.File('./cnn_weights.hdf5', 'r') as f:\n",
    "    print(\"HDF5 file structure:\")\n",
    "    print(f.keys())\n",
    "\n",
    "    # Initialize your PyTorch model\n",
    "    model = SleepDetectorCNN()\n",
    "\n",
    "    # Access the 'model_weights' key\n",
    "    model_weights = f['model_weights']\n",
    "\n",
    "    # Load weights for each convolutional block\n",
    "    for i in range(4):\n",
    "        conv_block = model.conv_blocks[i]\n",
    "\n",
    "        # Load weights for the first convolutional layer\n",
    "        key = f'conv1d_{i+1}'\n",
    "        if key in model_weights:\n",
    "            conv_block[0].weight = nn.Parameter(torch.tensor(model_weights[key][:]))\n",
    "        else:\n",
    "            print(f\"Key '{key}' not found in 'model_weights'.\")\n",
    "\n",
    "    # Load weights for the final dense layer\n",
    "    key = 'dense_1'\n",
    "    if key in model_weights:\n",
    "        model.fc.weight = nn.Parameter(torch.tensor(model_weights[key][:]))\n",
    "    else:\n",
    "        print(f\"Key '{key}' not found in 'model_weights'.\")\n",
    "\n",
    "# Open the H5 file\n",
    "with h5py.File('./cnn_weights.hdf5', 'r') as f:\n",
    "    # Initialize your PyTorch model\n",
    "    model = SleepDetectorCNN()\n",
    "    \n",
    "    # Load weights for each convolutional block\n",
    "    for i in range(4):\n",
    "        conv_block = model.conv_blocks[i]\n",
    "        \n",
    "        # Load weights for the first convolutional layer\n",
    "        kernel = f[f'conv1d_{i+1}/conv1d_{i+1}/kernel:0'][:]\n",
    "        bias = f[f'conv1d_{i+1}/conv1d_{i+1}/bias:0'][:]\n",
    "        conv_block[0].weight = nn.Parameter(torch.tensor(kernel).permute(2, 1, 0).contiguous())\n",
    "        conv_block[0].bias = nn.Parameter(torch.tensor(bias))\n",
    "        \n",
    "        # Load weights for the second convolutional layer\n",
    "        kernel = f[f'conv1d_{i+5}/conv1d_{i+5}/kernel:0'][:]\n",
    "        bias = f[f'conv1d_{i+5}/conv1d_{i+5}/bias:0'][:]\n",
    "        conv_block[4].weight = nn.Parameter(torch.tensor(kernel).permute(2, 1, 0).contiguous())\n",
    "        conv_block[4].bias = nn.Parameter(torch.tensor(bias))\n",
    "        \n",
    "        # Load weights for the third convolutional layer\n",
    "        kernel = f[f'conv1d_{i+9}/conv1d_{i+9}/kernel:0'][:]\n",
    "        bias = f[f'conv1d_{i+9}/conv1d_{i+9}/bias:0'][:]\n",
    "        conv_block[8].weight = nn.Parameter(torch.tensor(kernel).permute(2, 1, 0).contiguous())\n",
    "        conv_block[8].bias = nn.Parameter(torch.tensor(bias))\n",
    "    \n",
    "    # Load weights for the final dense layer\n",
    "    kernel = f['dense_1/dense_1/kernel:0'][:]\n",
    "    bias = f['dense_1/dense_1/bias:0'][:]\n",
    "    model.fc.weight = nn.Parameter(torch.tensor(kernel).permute(1, 0).contiguous())\n",
    "    model.fc.bias = nn.Parameter(torch.tensor(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file structure:\n",
      "<KeysViewHDF5 ['model_weights']>\n",
      "Contents of 'conv1d_1':\n",
      "<KeysViewHDF5 ['conv1d_1']>\n",
      "Contents of 'conv1d_1':\n",
      "(8,)\n",
      "(50, 1, 8)\n",
      "Contents of 'conv1d_2':\n",
      "<KeysViewHDF5 ['conv1d_2']>\n",
      "Contents of 'conv1d_2':\n",
      "(16,)\n",
      "(8, 8, 16)\n",
      "Contents of 'conv1d_3':\n",
      "<KeysViewHDF5 ['conv1d_3']>\n",
      "Contents of 'conv1d_3':\n",
      "(32,)\n",
      "(8, 16, 32)\n",
      "Contents of 'conv1d_4':\n",
      "<KeysViewHDF5 ['conv1d_4']>\n",
      "Contents of 'conv1d_4':\n",
      "(8,)\n",
      "(50, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('./cnn_weights.hdf5', 'r') as f:\n",
    "    print(\"HDF5 file structure:\")\n",
    "    print(f.keys())\n",
    "\n",
    "    # Initialize your PyTorch model\n",
    "    model = SleepDetectorCNN()\n",
    "\n",
    "    # Access the 'model_weights' key\n",
    "    model_weights = f['model_weights']\n",
    "\n",
    "    # Load weights for each convolutional block\n",
    "    for i in range(4):\n",
    "        conv_block = model.conv_blocks[i]\n",
    "\n",
    "        # Load weights for the first convolutional layer\n",
    "        key = f'conv1d_{i+1}'\n",
    "        if key in model_weights:\n",
    "            print(f\"Contents of '{key}':\")\n",
    "            print(model_weights[key].keys())\n",
    "            # Try to access the weights using the actual key\n",
    "            for weight_key in model_weights[key].keys():\n",
    "                print(f\"Contents of '{weight_key}':\")\n",
    "                # Access the Dataset object inside the Group\n",
    "                group = model_weights[key][weight_key]\n",
    "                for dataset_key in group.keys():\n",
    "                    dataset = group[dataset_key]\n",
    "                    print(dataset.shape)\n",
    "                    weight = dataset[:]\n",
    "                    conv_block[0].weight = nn.Parameter(torch.tensor(weight))\n",
    "        else:\n",
    "            print(f\"Key '{key}' not found in 'model_weights'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../cnn_weights.hdf5\n",
      "Path: model_weights\n",
      "  Attribute: backend = tensorflow\n",
      "  Attribute: keras_version = 2.2.4\n",
      "  Attribute: layer_names = [b'input_1' b'input_2' b'input_3' b'input_4' b'conv1d_1' b'conv1d_4'\n",
      " b'conv1d_7' b'conv1d_10' b'batch_normalization_1'\n",
      " b'batch_normalization_4' b'batch_normalization_7'\n",
      " b'batch_normalization_10' b'activation_1' b'activation_4' b'activation_7'\n",
      " b'activation_10' b'max_pooling1d_1' b'max_pooling1d_4' b'max_pooling1d_7'\n",
      " b'max_pooling1d_10' b'dropout_1' b'dropout_4' b'dropout_7' b'dropout_10'\n",
      " b'conv1d_2' b'conv1d_5' b'conv1d_8' b'conv1d_11' b'batch_normalization_2'\n",
      " b'batch_normalization_5' b'batch_normalization_8'\n",
      " b'batch_normalization_11' b'activation_2' b'activation_5' b'activation_8'\n",
      " b'activation_11' b'max_pooling1d_2' b'max_pooling1d_5' b'max_pooling1d_8'\n",
      " b'max_pooling1d_11' b'dropout_2' b'dropout_5' b'dropout_8' b'dropout_11'\n",
      " b'conv1d_3' b'conv1d_6' b'conv1d_9' b'conv1d_12' b'batch_normalization_3'\n",
      " b'batch_normalization_6' b'batch_normalization_9'\n",
      " b'batch_normalization_12' b'activation_3' b'activation_6' b'activation_9'\n",
      " b'activation_12' b'max_pooling1d_3' b'max_pooling1d_6' b'max_pooling1d_9'\n",
      " b'max_pooling1d_12' b'dropout_3' b'dropout_6' b'dropout_9' b'dropout_12'\n",
      " b'concatenate_1' b'flatten_1' b'dense_1']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_10\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_11\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_12\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_2\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_3\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_4\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_5\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_6\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_7\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_8\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/activation_9\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1\n",
      "  Attribute: weight_names = [b'batch_normalization_1/gamma:0' b'batch_normalization_1/beta:0'\n",
      " b'batch_normalization_1/moving_mean:0'\n",
      " b'batch_normalization_1/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1/batch_normalization_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1/batch_normalization_1/beta:0\n",
      "  Dataset: model_weights/batch_normalization_1/batch_normalization_1/beta:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.06768847  1.5841566  -0.09953334  0.8036495   0.71850336]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1/batch_normalization_1/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_1/batch_normalization_1/gamma:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[1.2271454  0.91281945 1.1925584  1.1744822  1.137436  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1/batch_normalization_1/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_1/batch_normalization_1/moving_mean:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.01144787  0.0066984  -0.00129866  0.00598103 -0.01243325]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_1/batch_normalization_1/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_1/batch_normalization_1/moving_variance:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 1810.561     680.97864  4047.452   17951.947    3073.718  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10\n",
      "  Attribute: weight_names = [b'batch_normalization_10/gamma:0' b'batch_normalization_10/beta:0'\n",
      " b'batch_normalization_10/moving_mean:0'\n",
      " b'batch_normalization_10/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10/batch_normalization_10\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10/batch_normalization_10/beta:0\n",
      "  Dataset: model_weights/batch_normalization_10/batch_normalization_10/beta:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.7069737  0.7115857  0.16252734 0.46342373 0.6617983 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10/batch_normalization_10/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_10/batch_normalization_10/gamma:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[1.1110476 1.4905013 1.0588514 1.1761941 1.575337 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10/batch_normalization_10/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_10/batch_normalization_10/moving_mean:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.01251248 -0.01038347  0.01672211 -0.00205127 -0.00321438]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_10/batch_normalization_10/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_10/batch_normalization_10/moving_variance:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[  496.83392 25411.691     955.96423  6183.464    5942.4565 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11\n",
      "  Attribute: weight_names = [b'batch_normalization_11/gamma:0' b'batch_normalization_11/beta:0'\n",
      " b'batch_normalization_11/moving_mean:0'\n",
      " b'batch_normalization_11/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11/batch_normalization_11\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11/batch_normalization_11/beta:0\n",
      "  Dataset: model_weights/batch_normalization_11/batch_normalization_11/beta:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.17346449 -0.4399936   1.4004076  -0.56094456 -0.14278576]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11/batch_normalization_11/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_11/batch_normalization_11/gamma:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[1.1448581  1.3940556  0.61360615 1.3747292  1.5772138 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11/batch_normalization_11/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_11/batch_normalization_11/moving_mean:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-17.94948   -14.6966095   5.595603  -11.378358  -20.105223 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_11/batch_normalization_11/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_11/batch_normalization_11/moving_variance:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[57.82877  60.5289   95.06888  58.71172  94.817184]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12\n",
      "  Attribute: weight_names = [b'batch_normalization_12/gamma:0' b'batch_normalization_12/beta:0'\n",
      " b'batch_normalization_12/moving_mean:0'\n",
      " b'batch_normalization_12/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12/batch_normalization_12\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12/batch_normalization_12/beta:0\n",
      "  Dataset: model_weights/batch_normalization_12/batch_normalization_12/beta:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.45445576 -0.27850217 -0.39634174 -0.07019189 -0.90404075]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12/batch_normalization_12/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_12/batch_normalization_12/gamma:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.36857077 0.47595695 0.82274896 0.35450977 0.5790989 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12/batch_normalization_12/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_12/batch_normalization_12/moving_mean:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-11.663777 -23.884048 -44.105495  24.463938   7.701648]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_12/batch_normalization_12/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_12/batch_normalization_12/moving_variance:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 29.030455  72.334885 107.21313   51.521515  26.768978]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2\n",
      "  Attribute: weight_names = [b'batch_normalization_2/gamma:0' b'batch_normalization_2/beta:0'\n",
      " b'batch_normalization_2/moving_mean:0'\n",
      " b'batch_normalization_2/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2/batch_normalization_2\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2/batch_normalization_2/beta:0\n",
      "  Dataset: model_weights/batch_normalization_2/batch_normalization_2/beta:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 3.1302083  -0.14751817 -1.4975498   0.22290373 -0.3637837 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2/batch_normalization_2/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_2/batch_normalization_2/gamma:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.5474818 1.1433617 1.1963384 0.7323674 0.7625842]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2/batch_normalization_2/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_2/batch_normalization_2/moving_mean:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 16.887394  -15.971461    1.4528476   1.464731  -21.333378 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_2/batch_normalization_2/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_2/batch_normalization_2/moving_variance:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[35.75038  56.77389  29.241873 66.948494 69.05088 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3\n",
      "  Attribute: weight_names = [b'batch_normalization_3/gamma:0' b'batch_normalization_3/beta:0'\n",
      " b'batch_normalization_3/moving_mean:0'\n",
      " b'batch_normalization_3/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3/batch_normalization_3\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3/batch_normalization_3/beta:0\n",
      "  Dataset: model_weights/batch_normalization_3/batch_normalization_3/beta:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.4159518  -0.336239   -0.01006471 -0.11722706 -0.0942691 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3/batch_normalization_3/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_3/batch_normalization_3/gamma:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.54336137 0.91835713 0.6703856  0.90870136 0.75168926]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3/batch_normalization_3/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_3/batch_normalization_3/moving_mean:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-19.902142 -26.0128   -45.043427 -21.984337 -30.84165 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_3/batch_normalization_3/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_3/batch_normalization_3/moving_variance:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 33.49813  128.72728   97.28194   43.257946  89.086   ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4\n",
      "  Attribute: weight_names = [b'batch_normalization_4/gamma:0' b'batch_normalization_4/beta:0'\n",
      " b'batch_normalization_4/moving_mean:0'\n",
      " b'batch_normalization_4/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4/batch_normalization_4\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4/batch_normalization_4/beta:0\n",
      "  Dataset: model_weights/batch_normalization_4/batch_normalization_4/beta:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.30799744 0.49873632 0.4834105  0.5170747  0.95263577]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4/batch_normalization_4/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_4/batch_normalization_4/gamma:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[1.0538573 0.8914732 0.9846805 1.1451311 1.0710987]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4/batch_normalization_4/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_4/batch_normalization_4/moving_mean:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.04221039 -0.0151764  -0.0045764   0.0020438  -0.00493952]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_4/batch_normalization_4/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_4/batch_normalization_4/moving_variance:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[   60.34934  1493.9807   4648.444    7360.651   25361.19   ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5\n",
      "  Attribute: weight_names = [b'batch_normalization_5/gamma:0' b'batch_normalization_5/beta:0'\n",
      " b'batch_normalization_5/moving_mean:0'\n",
      " b'batch_normalization_5/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5/batch_normalization_5\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5/batch_normalization_5/beta:0\n",
      "  Dataset: model_weights/batch_normalization_5/batch_normalization_5/beta:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.30538777 -0.48743388  0.24711035 -0.16804974 -0.9415298 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5/batch_normalization_5/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_5/batch_normalization_5/gamma:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[1.1081206  1.6406546  1.1582786  0.85412514 0.98717195]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5/batch_normalization_5/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_5/batch_normalization_5/moving_mean:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-22.307684  -17.04902    -7.6834736  -7.8797364  19.096966 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_5/batch_normalization_5/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_5/batch_normalization_5/moving_variance:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[102.70988   97.261795  17.34024   37.230743  37.753223]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6\n",
      "  Attribute: weight_names = [b'batch_normalization_6/gamma:0' b'batch_normalization_6/beta:0'\n",
      " b'batch_normalization_6/moving_mean:0'\n",
      " b'batch_normalization_6/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6/batch_normalization_6\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6/batch_normalization_6/beta:0\n",
      "  Dataset: model_weights/batch_normalization_6/batch_normalization_6/beta:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-1.5367361  -1.0386213  -0.36916113 -0.67038876 -1.2350368 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6/batch_normalization_6/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_6/batch_normalization_6/gamma:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.6768208  0.4834913  0.34877312 0.5003913  0.5556232 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6/batch_normalization_6/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_6/batch_normalization_6/moving_mean:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-39.6861     -2.6047022   3.430549  -30.067316  -27.519234 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_6/batch_normalization_6/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_6/batch_normalization_6/moving_variance:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[133.62115   23.139511  45.548733  37.950577  30.424932]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7\n",
      "  Attribute: weight_names = [b'batch_normalization_7/gamma:0' b'batch_normalization_7/beta:0'\n",
      " b'batch_normalization_7/moving_mean:0'\n",
      " b'batch_normalization_7/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7/batch_normalization_7\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7/batch_normalization_7/beta:0\n",
      "  Dataset: model_weights/batch_normalization_7/batch_normalization_7/beta:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.30874518 -0.04920463 -0.06490578  0.08702656  0.7883291 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7/batch_normalization_7/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_7/batch_normalization_7/gamma:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.8302726 1.4582256 1.0210886 1.4311837 1.2543169]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7/batch_normalization_7/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_7/batch_normalization_7/moving_mean:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.0047451   0.04068741  0.01184612  0.02856752 -0.02689249]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_7/batch_normalization_7/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_7/batch_normalization_7/moving_variance:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[  632.27277  6882.9785   2982.5845    636.8468  10982.891  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8\n",
      "  Attribute: weight_names = [b'batch_normalization_8/gamma:0' b'batch_normalization_8/beta:0'\n",
      " b'batch_normalization_8/moving_mean:0'\n",
      " b'batch_normalization_8/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8/batch_normalization_8\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8/batch_normalization_8/beta:0\n",
      "  Dataset: model_weights/batch_normalization_8/batch_normalization_8/beta:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.4591178   0.1680384   0.12973194 -0.01417518 -0.55399853]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8/batch_normalization_8/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_8/batch_normalization_8/gamma:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.99810904 1.3037437  1.9209194  1.2663538  0.6609099 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8/batch_normalization_8/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_8/batch_normalization_8/moving_mean:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ -4.486668 -10.404425 -31.4119    -9.727415  14.305512]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_8/batch_normalization_8/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_8/batch_normalization_8/moving_variance:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 32.57078   23.339312 138.03784   42.70614   59.770615]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9\n",
      "  Attribute: weight_names = [b'batch_normalization_9/gamma:0' b'batch_normalization_9/beta:0'\n",
      " b'batch_normalization_9/moving_mean:0'\n",
      " b'batch_normalization_9/moving_variance:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9/batch_normalization_9\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9/batch_normalization_9/beta:0\n",
      "  Dataset: model_weights/batch_normalization_9/batch_normalization_9/beta:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.4991984  -0.16383603 -0.01739433 -0.6131603  -0.6974779 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9/batch_normalization_9/gamma:0\n",
      "  Dataset: model_weights/batch_normalization_9/batch_normalization_9/gamma:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.45105213 1.0550331  0.22629668 0.67122304 0.6449478 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9/batch_normalization_9/moving_mean:0\n",
      "  Dataset: model_weights/batch_normalization_9/batch_normalization_9/moving_mean:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 27.658602  -30.312576    6.3130226 -18.779366   -3.7677648]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/batch_normalization_9/batch_normalization_9/moving_variance:0\n",
      "  Dataset: model_weights/batch_normalization_9/batch_normalization_9/moving_variance:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[43.11525  71.91678  34.613834 74.02245  43.378914]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/concatenate_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_1\n",
      "  Attribute: weight_names = [b'conv1d_1/kernel:0' b'conv1d_1/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_1/conv1d_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_1/conv1d_1/bias:0\n",
      "  Dataset: model_weights/conv1d_1/conv1d_1/bias:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.01027202  0.00717643 -0.00445655  0.00265498 -0.01119904]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_1/conv1d_1/kernel:0\n",
      "  Dataset: model_weights/conv1d_1/conv1d_1/kernel:0\n",
      "    Shape: (50, 1, 8)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 0.09496428  0.18514208  0.04331885 -0.38146317 -0.25717077\n",
      "   -0.30167767 -0.51602125  0.04146184]]\n",
      "\n",
      " [[-0.16385552  0.06940433  0.17602623 -0.32674572 -0.11492731\n",
      "    0.50357527  0.01051379  0.46466842]]\n",
      "\n",
      " [[ 0.3537068  -0.03132432  0.05806072  0.1764577  -0.32488063\n",
      "    0.30964428  0.06345681  0.33300036]]\n",
      "\n",
      " [[ 1.0605714  -0.12569036  0.15673332 -0.04128966 -0.135392\n",
      "   -0.88120335 -0.19604501  0.41924426]]\n",
      "\n",
      " [[ 0.7962953  -0.10991948 -0.13158187 -0.1627066  -0.4201107\n",
      "   -0.3737089   0.5898992   0.83856773]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_10\n",
      "  Attribute: weight_names = [b'conv1d_10/kernel:0' b'conv1d_10/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_10/conv1d_10\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_10/conv1d_10/bias:0\n",
      "  Dataset: model_weights/conv1d_10/conv1d_10/bias:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.0125543   0.00260771  0.0178514   0.00212144  0.00161085]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_10/conv1d_10/kernel:0\n",
      "  Dataset: model_weights/conv1d_10/conv1d_10/kernel:0\n",
      "    Shape: (50, 1, 8)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[-0.26310283 -0.5085664  -0.022684   -0.20136186 -0.22978452\n",
      "   -0.6970984   0.50904506 -0.3926437 ]]\n",
      "\n",
      " [[ 0.15365264 -0.11009232  1.080526    0.19750325  0.35321814\n",
      "   -0.15047587 -1.2142287  -0.02819864]]\n",
      "\n",
      " [[ 0.4615356   0.12014171 -0.71176684  0.53191006 -0.07839087\n",
      "   -0.09432572 -0.53160596  0.34248263]]\n",
      "\n",
      " [[-0.07119317 -0.09169972 -0.51736903  0.00175934  0.02448377\n",
      "    0.29434302  1.4306601   0.10174651]]\n",
      "\n",
      " [[-0.4921118  -0.38700336  0.5836794  -0.506191    0.08551213\n",
      "    0.23316653 -0.13056767 -0.09520612]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_11\n",
      "  Attribute: weight_names = [b'conv1d_11/kernel:0' b'conv1d_11/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_11/conv1d_11\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_11/conv1d_11/bias:0\n",
      "  Dataset: model_weights/conv1d_11/conv1d_11/bias:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.0062463  -0.01930965 -0.03611019  0.00366496  0.04097676]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_11/conv1d_11/kernel:0\n",
      "  Dataset: model_weights/conv1d_11/conv1d_11/kernel:0\n",
      "    Shape: (8, 8, 16)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[-4.20003414e-01 -8.56694132e-02 -3.18452984e-01  5.30868173e-01\n",
      "    7.59477794e-01 -2.89895415e-01  3.44947785e-01 -1.35755897e-01\n",
      "   -9.02387574e-02  1.07542537e-01 -1.37925297e-01 -9.63817060e-01\n",
      "    2.69094825e+00 -7.27361202e-01 -8.29844713e-01 -1.64123344e+00]\n",
      "  [ 6.23534620e-03 -6.08851969e-01 -9.53138545e-02 -2.25496221e+00\n",
      "   -7.24537447e-02 -6.66187286e-01  2.90676598e-02 -6.15036011e-01\n",
      "   -1.53830528e+00 -1.96480191e+00 -2.05932236e+00 -1.60703397e+00\n",
      "    1.97835624e-01 -5.37546575e-01 -5.61130702e-01 -1.03465331e+00]\n",
      "  [ 3.91868919e-01 -5.63245475e-01  4.76190120e-01 -3.07070352e-02\n",
      "   -9.90611672e-01 -2.05952778e-01 -3.80157351e-01  3.70236710e-02\n",
      "   -2.14232415e-01 -1.26382017e+00  2.73110032e-01 -6.51326537e-01\n",
      "    1.78378567e-01 -4.28437516e-02  1.04039088e-02 -2.45663807e-01]\n",
      "  [ 4.44731206e-01 -1.18144406e-02 -1.89440191e-01 -7.18306303e-01\n",
      "    8.52282226e-01  5.59851646e-01  1.97106993e+00  9.68518019e-01\n",
      "    4.48500253e-02 -1.93048012e+00 -9.34530675e-01  1.93469644e-01\n",
      "    2.90142149e-01 -3.24604660e-01  2.51096427e-01 -1.16291380e+00]\n",
      "  [ 1.37414724e-01  4.70117241e-01  5.20715177e-01  6.47352219e-01\n",
      "   -3.48655760e-01 -1.39189735e-01  2.85354435e-01 -1.62115538e+00\n",
      "    1.72406030e+00  5.83471060e-01 -2.24264359e+00  5.79043627e-02\n",
      "   -1.82701290e-01  8.62762392e-01  1.29547780e-02  4.21475261e-01]\n",
      "  [-2.55734384e-01 -5.85418344e-01  4.33750153e-01 -1.10917485e+00\n",
      "   -3.21355373e-01  4.77283746e-01 -1.34059298e+00  9.32979882e-02\n",
      "   -3.36507559e-01 -8.21950808e-02 -1.40891564e+00  7.56668687e-01\n",
      "   -3.02377433e-01 -1.36505947e-01 -6.31996512e-01 -8.62701416e-01]\n",
      "  [-9.07676160e-01  8.86135399e-02  7.68858910e-01 -1.05802774e+00\n",
      "    1.60919413e-01 -8.46295536e-01 -1.19120574e+00  5.48865199e-01\n",
      "    3.73621099e-02 -6.69217408e-01 -8.29218607e-03 -1.23442852e+00\n",
      "   -3.53012741e-01 -7.29246259e-01 -1.21279013e+00 -1.91284627e-01]\n",
      "  [-4.94743466e-01  1.56069875e-01  2.63000429e-01  5.93922317e-01\n",
      "   -1.03278363e+00 -6.31055832e-01 -9.70894396e-01  1.86027765e-01\n",
      "   -3.48237008e-01  6.89520001e-01  2.54110426e-01  2.25214139e-01\n",
      "   -6.82874441e-01  9.82369244e-01 -7.46212780e-01  9.85731006e-01]]\n",
      "\n",
      " [[-8.15438986e-01  1.68105155e-01 -3.07346016e-01  4.71974045e-01\n",
      "    4.42872733e-01 -1.34661958e-01  5.79637587e-01  1.71227723e-01\n",
      "    2.05944851e-02  6.97031105e-03 -5.45343384e-02 -8.47676277e-01\n",
      "    7.99393952e-01 -3.80236059e-01 -7.66064525e-01 -4.32806778e+00]\n",
      "  [ 1.84635222e-01 -2.81749129e-01  2.73536325e-01  2.17980072e-01\n",
      "   -1.90405563e-01 -2.16183364e-01  2.26912454e-01  9.65482071e-02\n",
      "   -2.11366749e+00 -8.89468491e-01  3.26921642e-02 -1.61114073e+00\n",
      "    1.94800124e-01 -5.44836164e-01 -1.00635481e+00  1.06825031e-01]\n",
      "  [-7.90923476e-01 -6.59249067e-01  9.68086571e-02 -6.17392480e-01\n",
      "   -4.10346001e-01 -4.06047910e-01 -4.29185838e-01 -7.99457412e-05\n",
      "    1.89623147e-01 -9.26334560e-01  4.16335370e-03 -5.22920489e-01\n",
      "    3.97925556e-01 -3.29178572e-01 -1.82735715e-02 -3.91219914e-01]\n",
      "  [ 7.23636270e-01 -8.47024471e-03  3.59665632e-01 -6.59933627e-01\n",
      "    2.37327829e-01  1.50574732e+00  8.98530066e-01  4.33233142e-01\n",
      "   -2.36722440e-01 -3.45521760e+00 -6.56535149e-01 -5.80302060e-01\n",
      "    3.60744059e-01 -2.64975548e-01  4.05137450e-01 -1.29827452e+00]\n",
      "  [-1.34422505e+00  3.85640919e-01  3.99282932e-01 -3.19210917e-01\n",
      "   -3.01225275e-01 -2.17057243e-01 -4.40639742e-02 -1.69049025e+00\n",
      "    5.00202119e-01 -2.87717581e-02 -1.00875366e+00  1.73751026e-01\n",
      "   -1.16448581e-01  6.33837163e-01 -3.92630622e-02 -4.15269911e-01]\n",
      "  [ 5.28846681e-01 -5.18624365e-01  5.62542856e-01  3.74919921e-02\n",
      "   -6.80982828e-01  1.15098846e+00 -1.16291845e+00  9.22064297e-03\n",
      "   -8.88428211e-01 -7.01312006e-01 -9.78970945e-01 -3.22900087e-01\n",
      "   -4.05041039e-01 -2.78987680e-02 -1.00414395e+00 -5.90809137e-02]\n",
      "  [-6.07994318e-01  3.19581866e-01  3.74445230e-01 -1.71720898e+00\n",
      "    1.03239305e-01 -4.00235653e-01 -3.69362772e-01  1.36406437e-01\n",
      "    5.29874377e-02 -7.04730809e-01 -1.33105293e-01 -1.02548242e+00\n",
      "   -1.00077979e-01 -2.76390582e-01 -2.62080193e-01 -1.05584908e+00]\n",
      "  [-1.34192538e-02  1.67839065e-01  5.56997001e-01  8.77445996e-01\n",
      "   -8.41806471e-01 -2.82864958e-01 -2.34616026e-01 -3.39017063e-01\n",
      "   -5.91003537e-01  1.06050348e+00 -5.58631599e-01  3.92582297e-01\n",
      "   -5.55583298e-01  5.84095299e-01 -6.83219492e-01  6.25952482e-01]]\n",
      "\n",
      " [[-1.05223787e+00  6.45944402e-02 -1.86018229e-01  2.72852868e-01\n",
      "    1.57289356e-01 -3.69018018e-02  2.27332249e-01  1.30785361e-01\n",
      "    1.11531816e-01  2.24136654e-02 -3.41411710e-01 -1.03535366e+00\n",
      "    8.94730628e-01 -4.10098463e-01 -5.90721607e-01 -2.84616184e+00]\n",
      "  [-1.96783945e-01 -3.45454782e-01 -5.91555387e-02  3.69089209e-02\n",
      "   -8.50806758e-02 -4.61104512e-03  2.25854620e-01 -1.68055966e-01\n",
      "   -5.09175360e-01  4.46996331e-01  4.58120257e-01 -7.00636387e-01\n",
      "    2.27440506e-01 -9.59173918e-01 -7.45374799e-01  1.72199048e-02]\n",
      "  [-2.61347443e-01 -4.35919642e-01  2.54739285e-01 -1.87426353e+00\n",
      "   -9.22273219e-01 -9.70944762e-02 -3.74109238e-01  1.96587041e-01\n",
      "   -6.24865711e-01 -5.68366289e-01 -7.12274164e-02 -5.10064185e-01\n",
      "    9.97924283e-02 -1.18714705e-01  6.72580183e-01 -6.57658517e-01]\n",
      "  [ 4.50523525e-01 -5.97719014e-01  6.55345678e-01 -5.02577424e-01\n",
      "   -1.47606194e-01  3.85730475e-01  3.49836677e-01  1.12930439e-01\n",
      "   -4.47360635e-01 -1.03998613e+00 -8.98468196e-01 -1.72697282e+00\n",
      "    4.63987172e-01 -1.83197185e-01  4.09198642e-01  7.47866929e-02]\n",
      "  [ 1.14650834e+00  7.41107643e-01  9.63233411e-01 -1.51466161e-01\n",
      "   -1.74178648e+00  6.28440408e-03 -3.26157510e-01 -5.05257487e-01\n",
      "    1.70699790e-01  3.89948159e-01 -5.44423938e-01 -2.90038168e-01\n",
      "   -1.48906618e-01  1.18007493e+00 -2.80797124e-01 -1.91176370e-01]\n",
      "  [ 5.28949022e-01 -1.03651714e+00  1.01046108e-01 -5.14667965e-02\n",
      "    4.58617866e-01 -3.69890593e-03 -3.71201396e-01  5.08530796e-01\n",
      "   -8.76841024e-02  9.95805189e-02 -4.56082940e-01  1.43883765e-01\n",
      "    1.06845992e-02 -9.40036699e-02 -3.58694673e-01  3.43256295e-01]\n",
      "  [ 2.68561095e-01  1.23910092e-01  3.18463922e-01 -1.15038562e+00\n",
      "    1.62316248e-01 -2.27379173e-01 -5.96309721e-01  2.49171719e-01\n",
      "    4.33680564e-02 -8.12872291e-01 -7.08524644e-01 -5.94819784e-01\n",
      "   -2.23844558e-01 -4.67687964e-01  3.76630008e-01 -4.94172066e-01]\n",
      "  [ 6.52434304e-02  2.94503212e-01  2.81488001e-01  4.70196068e-01\n",
      "   -2.30931711e+00  3.18582386e-01 -8.40837121e-01 -3.43474805e-01\n",
      "   -7.89575100e-01  1.44102979e+00 -3.79733109e+00  1.50392860e-01\n",
      "   -2.05571696e-01  7.98623085e-01 -4.59884256e-01 -3.25780720e-01]]\n",
      "\n",
      " [[-9.39796209e-01  3.06566656e-01 -2.43494108e-01  3.61564219e-01\n",
      "    2.89339662e-01  6.10937029e-02  2.78779924e-01  7.13139251e-02\n",
      "    2.24763453e-01  2.93458611e-01  3.39331388e-01 -5.39631367e-01\n",
      "    1.05331099e+00 -3.07969570e-01 -5.00243008e-01 -5.85416913e-01]\n",
      "  [-9.02493358e-01 -8.22658166e-02 -6.21506691e-01  1.10416025e-01\n",
      "    2.90252745e-01  5.77615976e-01 -3.25121254e-01 -2.23900184e-01\n",
      "   -1.35604903e-01  6.78247154e-01  1.15685150e-01 -3.75282675e-01\n",
      "    5.37255406e-01 -3.25588316e-01 -2.16034126e+00 -3.82779655e-03]\n",
      "  [-4.20932353e-01 -3.62547576e-01  1.53577536e-01 -1.54142833e+00\n",
      "   -9.43411469e-01 -4.68917191e-01 -2.81475216e-01 -2.81518638e-01\n",
      "    3.67769152e-01 -4.96810645e-01  4.71132874e-01 -8.46981779e-02\n",
      "    1.73701182e-01 -5.79608157e-02  9.75750148e-01  7.55909503e-01]\n",
      "  [ 5.86241126e-01 -1.14350224e+00 -4.58044827e-01  6.04000688e-01\n",
      "   -1.27689928e-01  1.07477176e+00  3.77634168e-01 -1.52985215e-01\n",
      "    5.34722090e-01 -1.74264506e-01  9.40568805e-01 -5.08014083e-01\n",
      "   -2.31694579e-01 -9.56679642e-01  8.11038136e-01  7.06240907e-02]\n",
      "  [-1.63584471e+00 -1.46737978e-01  1.38240588e+00 -1.82852790e-01\n",
      "    1.28727928e-01 -1.73889291e+00 -4.07292917e-02 -1.70395017e+00\n",
      "    1.51664579e+00  2.69844085e-02 -4.87616241e-01  5.04182994e-01\n",
      "   -1.35636061e-01  1.51635304e-01 -3.76388520e-01  1.60031483e-01]\n",
      "  [-1.20054138e+00 -8.34599495e-01 -6.68787003e-01 -5.55303156e-01\n",
      "    4.27485853e-01  1.21445525e+00 -2.28084385e-01  3.19220483e-01\n",
      "    3.33221644e-01 -6.93861917e-02 -6.35576189e-01  9.27956820e-01\n",
      "   -1.13484800e-01 -4.83437926e-01 -8.87295663e-01 -2.95490827e-02]\n",
      "  [ 1.86809912e-01 -3.41969788e-01  9.14333940e-01 -9.19531524e-01\n",
      "   -2.37500563e-01 -1.18791275e-01 -4.90461290e-01  2.97023982e-01\n",
      "    6.08482480e-01 -9.58118320e-01  6.54734731e-01 -3.53320926e-01\n",
      "   -3.00249368e-01 -7.13609934e-01  3.23375501e-02 -3.98878336e-01]\n",
      "  [ 1.53026834e-01  5.15173614e-01  1.40549615e-01  3.10616791e-01\n",
      "   -1.07593536e+00 -6.30512178e-01 -1.75048435e+00 -6.59629464e-01\n",
      "   -6.32918119e-01  3.40464115e-01 -1.05955803e+00  6.46312594e-01\n",
      "   -8.76640603e-02  5.90852559e-01 -4.43004519e-01  1.06514752e-01]]\n",
      "\n",
      " [[-9.99696970e-01  3.75483513e-01 -2.50986636e-01  4.01272804e-01\n",
      "    3.94434363e-01 -8.06094930e-02  2.32881501e-01  6.35862425e-02\n",
      "    2.19352350e-01  2.00828165e-01  3.06733191e-01 -2.78719455e-01\n",
      "    5.32371223e-01 -4.18644905e-01 -2.58930653e-01 -1.07762054e-01]\n",
      "  [-2.30127767e-01  4.50469941e-01 -3.47326875e-01 -1.72141016e+00\n",
      "   -8.32854807e-01  7.44629622e-01 -1.58067584e-01 -5.87344885e-01\n",
      "   -1.26504016e+00  5.54373145e-01  3.96712810e-01 -2.84963734e-02\n",
      "    9.67809260e-02 -1.51691958e-01 -3.15396810e+00 -1.59886137e-01]\n",
      "  [-6.33352816e-01 -9.48842824e-01  9.11593497e-01 -7.31612504e-01\n",
      "   -4.96953815e-01 -9.01027203e-01 -3.33082736e-01 -1.39299348e-01\n",
      "    3.35800707e-01 -7.48622537e-01  3.64659637e-01 -1.99497379e-02\n",
      "   -3.30876261e-01 -6.01600170e-01  7.50381947e-01  2.63393462e-01]\n",
      "  [ 5.73771358e-01 -2.11596638e-02 -1.43702972e+00 -4.08249140e-01\n",
      "    2.93776900e-01  6.64500952e-01  5.07448554e-01  4.38138396e-01\n",
      "   -2.76848018e-01 -1.12483256e-01  8.40656817e-01 -7.88186729e-01\n",
      "   -2.57233828e-01 -3.92903000e-01  4.41160560e-01 -2.01061040e-01]\n",
      "  [-4.33280140e-01  1.05790310e-02  4.61084604e-01  1.09048076e-01\n",
      "   -2.10484576e+00  4.37199980e-01 -6.12154961e-01 -9.90511298e-01\n",
      "   -7.93061336e-04 -1.76389396e-01 -2.13501751e-01 -2.92051643e-01\n",
      "   -2.43001089e-01  1.82738639e-02  2.37298623e-01  1.09107509e-01]\n",
      "  [ 4.31191027e-01 -7.79905796e-01 -1.63956702e+00 -2.60637283e-01\n",
      "   -3.24504882e-01  1.59988558e+00 -4.04047817e-01  6.47869587e-01\n",
      "   -4.39065516e-01  5.45900345e-01 -2.66358823e-01  3.07819813e-01\n",
      "    1.06312484e-01  4.07864481e-01 -3.27917367e-01 -3.56727839e-01]\n",
      "  [-4.66775522e-02 -4.63714331e-01  8.04594830e-02 -1.06075072e+00\n",
      "    7.37259388e-02  3.26197475e-01 -5.53985536e-01  2.36886993e-01\n",
      "    6.09898493e-02 -4.69350368e-02  6.92970037e-01  2.54752070e-01\n",
      "    1.48548186e-01 -5.94442725e-01 -3.68709832e-01  1.20088100e-01]\n",
      "  [-2.96584427e-01  1.96617246e-02  3.90465036e-02  3.07429045e-01\n",
      "   -3.79087418e-01 -1.72172105e+00 -1.74175656e+00 -1.22354507e+00\n",
      "   -3.69283855e-01  4.50437039e-01 -1.36005509e+00  7.81407505e-02\n",
      "   -4.97369319e-01  5.88812120e-02 -6.05998859e-02  1.39171809e-01]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_12\n",
      "  Attribute: weight_names = [b'conv1d_12/kernel:0' b'conv1d_12/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_12/conv1d_12\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_12/conv1d_12/bias:0\n",
      "  Dataset: model_weights/conv1d_12/conv1d_12/bias:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.03297776  0.03606345 -0.00915137 -0.06291968 -0.02564393]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_12/conv1d_12/kernel:0\n",
      "  Dataset: model_weights/conv1d_12/conv1d_12/kernel:0\n",
      "    Shape: (8, 16, 32)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 0.3595363   0.08827418 -0.1893062  ... -0.3317555   0.70753235\n",
      "   -0.4396216 ]\n",
      "  [ 0.30780995 -0.1062973   0.5626369  ... -2.6399531   0.6900036\n",
      "    0.45028389]\n",
      "  [-0.22260697  0.11637314 -0.7829866  ... -0.42438564  0.2794269\n",
      "   -0.46105003]\n",
      "  ...\n",
      "  [-0.06722091 -0.13241336 -0.7444858  ... -1.8233267  -0.75556767\n",
      "   -0.41260022]\n",
      "  [ 0.15987521  0.07101141  0.15641811 ...  0.2483258   0.33889124\n",
      "    0.01247816]\n",
      "  [-0.5082402   0.3641986  -0.12684518 ... -4.048742    0.82300663\n",
      "    0.55023825]]\n",
      "\n",
      " [[ 0.10338504  0.39413333 -0.37434173 ... -0.10353693  0.24915203\n",
      "   -0.00770928]\n",
      "  [ 0.36617488  0.14691125  0.09335777 ... -1.4332349   0.7276945\n",
      "    0.10748228]\n",
      "  [-0.19575545  0.28920433  0.0829922  ... -0.37788293  0.20654604\n",
      "   -0.47702423]\n",
      "  ...\n",
      "  [-0.17589937 -0.21576257 -0.6903718  ... -0.72246414 -0.50485516\n",
      "   -0.3712563 ]\n",
      "  [-0.46643013  1.0223895   0.37762997 ...  0.24220382  0.50256145\n",
      "   -0.06227389]\n",
      "  [-0.05688891  0.24591884  0.1367335  ... -3.969839    0.2336327\n",
      "    0.33060423]]\n",
      "\n",
      " [[-0.04004362  0.41985425 -0.6660902  ... -0.24037261 -0.15249528\n",
      "   -0.09791273]\n",
      "  [-0.00529701 -0.2303799   0.21255139 ... -1.6327263  -0.09862\n",
      "    0.20827541]\n",
      "  [-0.11160193  0.23113918 -0.11279204 ... -0.50973094  0.3513548\n",
      "   -0.5585334 ]\n",
      "  ...\n",
      "  [-0.10316366 -0.18519534 -0.05795559 ... -2.2202528  -0.4402426\n",
      "   -0.15035641]\n",
      "  [ 0.01599902  0.94994193  0.8563943  ...  0.15897863  0.17304862\n",
      "   -0.27220207]\n",
      "  [-0.2077845   0.34077242  0.14633517 ... -4.5918365  -0.5140069\n",
      "    0.14629944]]\n",
      "\n",
      " [[ 0.03805301  0.53076446 -0.56217533 ...  0.06357404 -0.40985847\n",
      "   -0.08479407]\n",
      "  [ 0.53232723 -0.28142077 -0.6228042  ... -0.29996565 -0.22464046\n",
      "    0.17791232]\n",
      "  [ 0.19208145  0.32147187  0.4370091  ...  0.17111862 -0.22257188\n",
      "   -0.59300905]\n",
      "  ...\n",
      "  [-0.2049817  -0.13422212  0.15998523 ... -0.2723064  -1.9778409\n",
      "   -0.04319929]\n",
      "  [ 0.15099558  0.6234893   0.41735846 ...  0.37650925 -0.6850669\n",
      "   -0.43768492]\n",
      "  [-0.41916302  0.10704148  0.03159495 ... -0.10259583 -0.01584233\n",
      "    0.02360856]]\n",
      "\n",
      " [[ 0.08430976  0.45509288 -0.7162907  ... -0.00569475 -0.42770106\n",
      "    0.01790743]\n",
      "  [ 0.6524625   0.90986156  0.65726775 ... -0.58927095 -0.13268581\n",
      "   -0.06258333]\n",
      "  [ 0.09571063 -0.26263645 -0.5088236  ...  0.26902962 -0.10050764\n",
      "   -0.9575526 ]\n",
      "  ...\n",
      "  [-0.18855928 -0.14471224 -0.05274515 ... -0.45409563  0.19471468\n",
      "    0.00840374]\n",
      "  [ 0.12150639 -0.33969122 -0.72579366 ... -0.04820102  0.21609132\n",
      "   -0.42890716]\n",
      "  [-0.24878423  0.51574296  0.4181373  ... -0.34612447 -0.67294985\n",
      "    0.13938567]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_2\n",
      "  Attribute: weight_names = [b'conv1d_2/kernel:0' b'conv1d_2/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_2/conv1d_2\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_2/conv1d_2/bias:0\n",
      "  Dataset: model_weights/conv1d_2/conv1d_2/bias:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.03702624  0.04743814 -0.00541258  0.01407324 -0.00306423]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_2/conv1d_2/kernel:0\n",
      "  Dataset: model_weights/conv1d_2/conv1d_2/kernel:0\n",
      "    Shape: (8, 8, 16)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[-4.27648783e-01 -4.63839799e-01  7.91676521e-01  7.90170312e-01\n",
      "   -1.02942026e+00 -3.27444106e-01 -7.08210096e-02  5.30176401e-01\n",
      "    1.10070407e+00 -1.44856721e-01 -8.02589357e-01 -6.08705163e-01\n",
      "   -7.66189158e-01 -4.77394432e-01 -7.79559672e-01  4.72700924e-01]\n",
      "  [ 8.33071470e-02  3.63269180e-01  1.05071759e+00  7.03235447e-01\n",
      "   -3.03143829e-01  1.18268633e+00  3.05904299e-01  1.37308383e+00\n",
      "    1.93547592e-01 -1.03064489e+00 -7.70513937e-02  4.36014473e-01\n",
      "   -5.09718359e-01 -3.98389935e-01 -1.11019425e-01  2.09799215e-01]\n",
      "  [ 1.30270049e-01 -1.78823665e-01  6.28226399e-01 -8.33220854e-02\n",
      "   -2.05880538e-01 -1.58282566e+00  1.71081141e-01 -3.81404638e-01\n",
      "   -3.17647159e-02 -2.08242446e-01 -1.11937141e+00  5.08561969e-01\n",
      "   -2.54200578e-01 -1.48546064e+00 -6.45186827e-02  4.30199295e-01]\n",
      "  [ 1.32663548e+00  1.37253985e-01 -6.69156909e-01 -7.24250376e-02\n",
      "   -2.54017323e-01 -1.02581108e+00 -7.21981525e-01  3.07716755e-03\n",
      "   -3.46248075e-02 -1.29708052e+00 -2.95994473e+00  1.21794081e+00\n",
      "   -2.07449719e-01  1.12860672e-01 -9.61136632e-03  1.59793478e-02]\n",
      "  [ 9.94032174e-02  2.08431616e-01  7.08278865e-02  1.63800776e-01\n",
      "   -3.84143323e-01  3.25700194e-01  8.44899952e-01  2.85301447e-01\n",
      "   -5.96723318e-01  2.09682375e-01 -5.80734722e-02 -5.92950702e-01\n",
      "   -1.04769483e-01 -1.44887507e+00 -4.82093900e-01 -2.87356675e-01]\n",
      "  [ 7.74479508e-01 -1.16972125e+00 -7.09877849e-01  2.84036815e-01\n",
      "   -9.85181093e-01  4.97715920e-01  7.96559919e-03  1.60515755e-01\n",
      "   -4.62997854e-01 -3.11667293e-01 -1.97789952e-01 -1.18338145e-01\n",
      "    2.95772523e-01  2.48379663e-01  2.51487136e-01 -1.43121338e+00]\n",
      "  [-5.02203286e-01 -2.34149680e-01 -2.67650366e-01 -1.71316707e+00\n",
      "    1.85757071e-01  3.74399871e-01 -3.93506885e-01  1.37157261e-01\n",
      "   -3.93726677e-01 -3.94282967e-01  3.52295250e-01  4.69230920e-01\n",
      "   -3.33493114e-01  1.39114991e-01  4.01037365e-01 -4.70052361e-01]\n",
      "  [ 1.40624881e-01 -8.97857130e-01 -8.31975043e-02 -1.81647360e+00\n",
      "   -1.43180323e+00  6.64381027e-01 -1.27801359e+00 -4.12529975e-01\n",
      "   -5.52363396e-01  1.82440817e-01  8.53249907e-01  2.47691125e-01\n",
      "   -6.00801468e-01 -1.07750094e+00 -7.23155379e-01  1.85519874e-01]]\n",
      "\n",
      " [[ 1.35588527e-01  7.58811310e-02  8.47945094e-01  7.36415327e-01\n",
      "   -3.87819648e-01 -1.90937653e-01 -2.44755477e-01  2.88722843e-01\n",
      "    7.17413485e-01 -7.68410414e-02 -4.31662023e-01  5.68724563e-03\n",
      "   -3.34724963e-01 -3.00892815e-02 -2.76491698e-02  7.26240993e-01]\n",
      "  [ 4.24211137e-02  5.21922648e-01  5.73855340e-01  4.00663823e-01\n",
      "    8.90993625e-02  3.13687325e-01  2.80169904e-01  4.08044636e-01\n",
      "    3.59178722e-01 -1.23692952e-01 -2.89942294e-01  2.50898331e-01\n",
      "   -4.93985146e-01 -1.08853567e+00  7.41001889e-02  1.26562551e-01]\n",
      "  [-4.88326669e-01 -5.79119682e-01  2.29394883e-01 -3.61315459e-01\n",
      "    1.33386087e+00 -1.31911707e+00 -8.10383499e-01 -8.80418718e-01\n",
      "   -2.17243880e-01 -9.60789680e-01 -1.60788313e-01  5.78847706e-01\n",
      "    5.06131947e-02 -1.55732334e+00  6.28149390e-01 -3.22814658e-02]\n",
      "  [ 9.70143914e-01 -1.54728472e+00 -2.28219002e-01  7.97620267e-02\n",
      "   -1.70635498e+00 -4.91811961e-01 -6.14558876e-01  1.41010612e-01\n",
      "   -1.64128125e-01  1.02588423e-01 -7.97287583e-01  7.95727074e-01\n",
      "   -2.69212157e-01 -5.22344172e-01  4.92421836e-02  1.24369338e-01]\n",
      "  [-4.21883136e-01 -2.15906858e-01 -1.25827149e-01  5.30435555e-02\n",
      "    6.46491885e-01 -3.45826477e-01  7.60441303e-01  1.25420594e+00\n",
      "   -4.23197627e-01  1.46776557e-01 -2.33522967e-01 -8.23764920e-01\n",
      "    1.41807675e-01 -6.81285322e-01  1.01995456e+00 -2.20256612e-01]\n",
      "  [ 7.50689983e-01 -5.50818801e-01 -1.00490236e+00  1.56102732e-01\n",
      "   -6.22595847e-01 -1.89835161e-01 -2.40907565e-01 -2.34378964e-01\n",
      "   -7.87205815e-01 -7.69407898e-02 -2.50240535e-01 -3.07208002e-01\n",
      "    2.50970513e-01  5.88158034e-02  3.49680394e-01 -1.60889721e+00]\n",
      "  [-1.77977696e-01  1.71335340e-01 -7.12122500e-01 -1.82776976e+00\n",
      "    4.54278529e-01  2.06531063e-02 -1.98333710e-01 -8.51285681e-02\n",
      "   -1.18973888e-01  9.41137552e-01 -9.26911011e-02  1.20510861e-01\n",
      "   -4.30321723e-01  5.49602032e-01  3.39585394e-01  1.56380296e-01]\n",
      "  [ 8.88179302e-01 -5.47715724e-02 -6.09339103e-02 -1.42046058e+00\n",
      "   -1.35217810e+00  4.60160822e-02  3.49987268e-01 -2.66871184e-01\n",
      "   -5.18500924e-01  6.12928867e-02  9.40053225e-01 -6.23915613e-01\n",
      "    1.41900793e-01  3.08975607e-01 -1.06763852e+00  1.56917363e-01]]\n",
      "\n",
      " [[ 2.00558722e-01 -5.32496609e-02  1.40289629e+00  6.50467038e-01\n",
      "   -2.51549453e-01 -7.24393874e-02  9.95082110e-02 -1.26223322e-02\n",
      "    8.37372467e-02  4.65973258e-01 -2.65698224e-01  3.10367018e-01\n",
      "   -2.98893183e-01 -5.43280691e-03 -8.46173644e-01  1.34318864e+00]\n",
      "  [ 2.47765243e-01  5.44039547e-01  6.30129695e-01  2.05130622e-01\n",
      "   -7.85321116e-01  9.42622006e-01 -3.73396486e-01  7.63334274e-01\n",
      "    2.96830714e-01 -8.01586986e-01 -6.43459141e-01  1.61603898e-01\n",
      "   -4.35753077e-01 -1.68415904e-01 -9.94640291e-01  1.27922937e-01]\n",
      "  [-2.92035890e+00 -1.21283069e-01 -8.76991600e-02 -2.95446813e-02\n",
      "    1.12044251e+00 -3.17069918e-01 -3.22842002e-02  4.16992813e-01\n",
      "    1.77432805e-01  5.71488738e-02 -4.37659025e-01  9.56529737e-01\n",
      "   -8.81042108e-02 -7.39117026e-01  1.73263222e-01  1.78349316e-01]\n",
      "  [-1.12765014e+00 -1.73475254e+00  3.49807322e-01  1.12843059e-01\n",
      "   -2.45533490e+00  8.35049570e-01 -5.26823759e-01 -2.67818213e-01\n",
      "    1.60508901e-01  5.68900168e-01 -7.24050701e-01  2.40930706e-01\n",
      "    1.46414086e-01  2.89358288e-01 -2.84877252e-02  1.14584081e-01]\n",
      "  [ 1.44308925e-01  5.87157607e-01 -1.66963130e-01 -1.34847358e-01\n",
      "    1.19554365e+00  1.16356991e-01  1.14310831e-01  8.18893731e-01\n",
      "   -8.82576644e-01 -6.62791967e-01  8.53073373e-02 -5.73332429e-01\n",
      "    1.57737017e-01 -8.31788123e-01  1.27289820e+00 -2.43369445e-01]\n",
      "  [ 8.15038502e-01 -7.92417943e-01 -1.36312771e+00 -5.69817603e-01\n",
      "   -3.84982884e-01 -3.25610131e-01  4.48861539e-01 -3.25061470e-01\n",
      "   -5.02159119e-01 -4.66881484e-01  2.09985048e-01 -3.32312077e-01\n",
      "    2.75995791e-01  2.24526167e-01  5.02466023e-01 -1.44179749e+00]\n",
      "  [ 3.22663814e-01 -1.17920451e-01 -2.84908831e-01 -7.33421922e-01\n",
      "    1.83128521e-01  1.20270029e-01  3.77517936e-05 -3.49718988e-01\n",
      "   -8.92934740e-01  1.91348052e+00  9.05454010e-02  7.04879045e-01\n",
      "   -4.21233475e-01 -3.50638390e-01 -4.79811966e-01  7.17534125e-01]\n",
      "  [ 9.92062807e-01 -1.70344636e-01  9.04462039e-02 -7.53955483e-01\n",
      "   -2.95594394e-01  5.10166943e-01 -1.97077274e-01  6.70046031e-01\n",
      "   -2.57554173e-01  6.59027547e-02  9.43059087e-01 -9.03290451e-01\n",
      "    3.12787414e-01  1.06840551e-01 -1.42521918e-01  2.82465875e-01]]\n",
      "\n",
      " [[ 4.40894067e-02  1.71504185e-01  1.22457409e+00  6.09933734e-01\n",
      "   -6.31113425e-02  1.81761272e-02 -3.37099642e-01  3.43096823e-01\n",
      "    3.36925298e-01  6.26604021e-01  1.25718847e-01 -2.78150856e-01\n",
      "   -4.78966862e-01  4.05365750e-02 -6.27461433e-01  1.45008433e+00]\n",
      "  [ 8.98869634e-02  6.56412542e-01  7.46997654e-01  6.64883137e-01\n",
      "   -4.83808905e-01  6.54679418e-01 -4.90818173e-01  6.05856121e-01\n",
      "    5.98895967e-01 -4.42625582e-01  5.54974079e-02  3.96004140e-01\n",
      "   -8.73456955e-01  6.81369752e-02 -5.38827956e-01  5.39565504e-01]\n",
      "  [-4.75782156e-01  5.96776567e-02 -3.13671619e-01  1.14915773e-01\n",
      "    5.28810501e-01 -1.23433399e+00  1.93537086e-01 -5.17243221e-02\n",
      "   -8.19238275e-02 -1.25378013e-01  7.03911841e-01 -2.07436264e-01\n",
      "    1.07149087e-01 -1.05118561e+00  1.64535016e-01  8.03974271e-02]\n",
      "  [ 5.58100879e-01 -6.37480378e-01  9.33319107e-02  6.47625446e-01\n",
      "   -1.31112540e+00  8.14248025e-01  5.07631779e-01 -9.21930134e-01\n",
      "    1.31902117e-02 -4.15288098e-02 -1.33026469e+00 -1.68249178e+00\n",
      "   -9.18122828e-02 -2.49400049e-01 -1.14551449e+00 -4.90944162e-02]\n",
      "  [ 5.44599354e-01  1.07550228e+00  1.22072756e-01 -1.50717115e-02\n",
      "    2.84787923e-01  6.83879673e-01 -8.20335329e-01 -8.73582304e-01\n",
      "   -1.18276942e+00 -1.30005980e+00  5.34373462e-01 -2.87312627e-01\n",
      "    3.10122997e-01 -3.93248826e-01 -4.31819081e-01 -1.66390806e-01]\n",
      "  [ 7.63819158e-01 -9.44362104e-01 -9.36236024e-01 -1.26658324e-02\n",
      "   -4.56600457e-01 -3.47696096e-02 -1.41278759e-01 -8.95497128e-02\n",
      "   -6.60362482e-01 -1.61465958e-01 -2.71226287e-01 -3.50765556e-01\n",
      "   -1.04908749e-01  2.72128701e-01  5.02587676e-01 -7.63976812e-01]\n",
      "  [ 5.78659117e-01 -6.62632585e-01 -9.53639746e-02 -4.99580592e-01\n",
      "    1.45015165e-01  1.84591457e-01 -1.68622747e-01 -5.75771809e-01\n",
      "   -1.20433974e+00  1.39639473e+00  3.21460396e-01  4.84818965e-01\n",
      "   -1.76754153e+00 -1.39572755e-01  4.56370972e-03  5.34640074e-01]\n",
      "  [ 3.65040958e-01 -7.56918967e-01 -6.76260814e-02 -8.29062343e-01\n",
      "    3.39081645e-01 -8.26052278e-02 -1.27404541e-01  7.74108944e-03\n",
      "   -7.06333160e-01 -4.75276917e-01  1.02179162e-01 -6.03867732e-02\n",
      "   -2.25428358e-01  2.46426221e-02  6.68169856e-01 -1.89332530e-01]]\n",
      "\n",
      " [[-3.98719907e-01  3.82394701e-01  9.71486926e-01  7.54260898e-01\n",
      "    5.67169413e-02 -3.68658274e-01 -4.45296854e-01  4.55128908e-01\n",
      "    4.19357270e-01  8.47371854e-03  5.32634258e-01 -1.77733764e-01\n",
      "   -3.89289975e-01 -1.30433965e+00 -6.48667157e-01  9.30544138e-01]\n",
      "  [-3.61211240e-01  5.13476551e-01  7.88162947e-01  3.69305611e-01\n",
      "    4.28219289e-02  8.61025512e-01  6.37003541e-01  5.15966415e-01\n",
      "    5.72782397e-01 -3.48950267e-01 -1.44866109e-01  7.92482436e-01\n",
      "   -9.47818756e-01  6.55294955e-01 -7.12837100e-01  5.47086239e-01]\n",
      "  [ 3.45813304e-01 -4.77716774e-01 -4.97688204e-01  1.70038909e-01\n",
      "    3.08830887e-01 -2.12446356e+00  5.65724850e-01 -2.19752774e-01\n",
      "   -6.08318686e-01 -7.81076550e-01  2.17561364e-01  6.78698003e-01\n",
      "   -2.02081636e-01 -1.62350774e+00 -3.18035990e-01 -5.89867353e-01]\n",
      "  [ 1.11790860e+00 -2.27853701e-01  2.78422147e-01  7.18274593e-01\n",
      "   -3.68404210e-01 -8.14793646e-01  5.45829058e-01 -1.22232485e+00\n",
      "    1.42126575e-01 -5.12850285e-01 -2.04458880e+00 -2.76248455e+00\n",
      "   -9.14561510e-01 -2.33045332e-02 -5.13331056e-01 -6.80168811e-03]\n",
      "  [ 5.51817939e-02 -2.94880662e-02  3.74278948e-02  1.43507287e-01\n",
      "   -4.32412267e-01 -2.81822503e-01 -1.38798451e+00 -1.15575409e+00\n",
      "   -7.69324243e-01  4.37467396e-01 -4.25316095e-01 -2.33114157e-02\n",
      "   -2.28721157e-01 -6.98518097e-01 -1.97807863e-01 -1.54874370e-01]\n",
      "  [ 4.05040175e-01 -9.07375038e-01 -5.43103814e-01  1.21003941e-01\n",
      "   -6.00013196e-01 -1.86809629e-01 -5.38717031e-01 -4.38833058e-01\n",
      "   -1.29385042e+00 -2.13138625e-01 -1.03532422e+00 -4.00773704e-01\n",
      "    2.72246152e-02  5.07690430e-01  7.99215496e-01 -2.09651232e-01]\n",
      "  [ 3.29822093e-01 -7.69623935e-01 -4.96813059e-01 -5.05341053e-01\n",
      "   -1.49582803e-01 -6.34770691e-01 -2.80498087e-01 -8.79010081e-01\n",
      "   -8.88675630e-01  4.07013834e-01 -2.19528019e-01  5.97143471e-01\n",
      "   -1.44644642e+00 -8.27087343e-01 -5.22744894e-01  3.31487179e-01]\n",
      "  [ 1.22746266e-01 -1.96512848e-01 -8.06796312e-01 -8.20146799e-01\n",
      "   -1.22142501e-01  3.19508046e-01 -2.97526717e-01 -6.54399157e-01\n",
      "   -5.98885715e-01 -1.25812674e+00  9.08915758e-01  2.11458668e-01\n",
      "    7.57341981e-02 -5.86565658e-02  1.14088044e-01 -8.73097360e-01]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_3\n",
      "  Attribute: weight_names = [b'conv1d_3/kernel:0' b'conv1d_3/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_3/conv1d_3\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_3/conv1d_3/bias:0\n",
      "  Dataset: model_weights/conv1d_3/conv1d_3/bias:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.02000746 -0.06636185  0.00707336  0.02705122  0.06569461]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_3/conv1d_3/kernel:0\n",
      "  Dataset: model_weights/conv1d_3/conv1d_3/kernel:0\n",
      "    Shape: (8, 16, 32)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[-2.55474299e-01  7.73787871e-02 -4.11944389e-01 ... -2.36092448e-01\n",
      "    1.50485216e-02  1.99217685e-02]\n",
      "  [ 3.76583099e-01 -3.02420169e-01 -3.27370375e-01 ...  6.88413143e-01\n",
      "   -1.63845509e-01  3.84549014e-02]\n",
      "  [ 3.10986876e-01 -2.87342477e+00 -1.58272922e-01 ... -6.87279522e-01\n",
      "    3.09759378e-03  1.77905932e-01]\n",
      "  ...\n",
      "  [-9.91439760e-01  4.04930189e-02  1.74218923e-01 ... -2.10765076e+00\n",
      "    1.50661156e-01 -2.07860613e+00]\n",
      "  [-7.71014169e-02 -3.20765316e-01 -4.36695635e-01 ...  1.96470767e-01\n",
      "   -1.33959398e-01 -4.11681414e-01]\n",
      "  [ 2.47105479e-01 -1.97478563e-01  2.81267632e-02 ...  8.93673241e-01\n",
      "   -1.65571854e-03  1.12529778e+00]]\n",
      "\n",
      " [[-4.71044779e-02  2.46313125e-01 -3.21681112e-01 ... -1.99118897e-01\n",
      "   -1.78243205e-01 -3.01534981e-01]\n",
      "  [ 6.22713089e-01 -1.45615786e-01 -3.60393614e-01 ...  7.24893093e-01\n",
      "    2.24000931e-01 -1.58890319e+00]\n",
      "  [ 6.48298979e-01 -3.31622052e+00  1.73098266e-01 ... -7.22842753e-01\n",
      "    1.27923831e-01  3.55777532e-01]\n",
      "  ...\n",
      "  [-1.83768541e-01 -1.12491399e-02  1.84465617e-01 ...  6.56272650e-01\n",
      "    5.27153492e-01 -1.06278909e-02]\n",
      "  [-5.48511267e-01 -2.04204172e-01 -1.95454389e-01 ...  7.51247823e-01\n",
      "    9.27521884e-02  7.55243525e-02]\n",
      "  [-1.20825633e-01 -3.64545614e-01  5.85496612e-02 ...  3.66491288e-01\n",
      "   -3.12815644e-02  4.71432537e-01]]\n",
      "\n",
      " [[ 7.14161843e-02 -4.07190293e-01 -6.55627191e-01 ... -5.95365644e-01\n",
      "   -2.23422989e-01 -4.05652493e-01]\n",
      "  [ 7.35000014e-01  4.32833463e-01 -1.59021392e-01 ... -8.55167955e-02\n",
      "    4.04349297e-01 -3.22356915e+00]\n",
      "  [ 5.67060769e-01 -1.03002238e+00  1.13377929e-01 ...  1.98568195e-01\n",
      "    2.24932432e-01  3.40072989e-01]\n",
      "  ...\n",
      "  [ 1.24815248e-01 -7.59345293e-01  6.21374212e-02 ... -2.59959120e-02\n",
      "    5.43828070e-01  6.39771998e-01]\n",
      "  [ 1.50808632e-01 -1.13455154e-01 -5.01560196e-02 ... -9.19777632e-01\n",
      "    2.13856652e-01  2.62525767e-01]\n",
      "  [ 3.02661985e-01  6.03924468e-02 -1.04729407e-01 ... -2.18732536e-01\n",
      "    2.37114608e-01 -7.94637680e-01]]\n",
      "\n",
      " [[-5.01058757e-01 -7.68652678e-01 -2.76430696e-01 ...  1.25123575e-01\n",
      "    7.38809630e-02  1.34265557e-01]\n",
      "  [ 6.19863391e-01  1.04613170e-01 -4.70240891e-01 ... -8.31612647e-02\n",
      "    6.65964127e-01 -3.02932829e-01]\n",
      "  [ 2.93388590e-02 -7.45498002e-01  9.93440300e-02 ... -1.95012018e-02\n",
      "    5.10774553e-01  1.15042046e-01]\n",
      "  ...\n",
      "  [-1.54301655e+00 -5.04909635e-01  3.17929983e-01 ... -4.70273644e-01\n",
      "    8.23085964e-01  6.83922350e-01]\n",
      "  [-2.63115644e-01 -2.29229048e-01 -7.61044264e-01 ... -5.26968502e-02\n",
      "    2.09376901e-01  3.60827953e-01]\n",
      "  [ 1.34017065e-01  1.97400182e-01 -3.35664451e-01 ...  5.38716972e-01\n",
      "    2.88038939e-01  3.43003660e-01]]\n",
      "\n",
      " [[-4.44415629e-01 -2.29979411e-01 -8.10896397e-01 ... -1.06078200e-01\n",
      "    3.12197339e-02 -1.92252159e-01]\n",
      "  [-7.88983285e-01 -5.48171699e-01 -8.54361117e-01 ... -3.16828601e-02\n",
      "    6.48612022e-01 -1.95522964e-01]\n",
      "  [ 1.32555097e-01 -4.56180096e+00  1.87676340e-01 ... -2.82353789e-01\n",
      "    4.08004262e-02  4.24291492e-01]\n",
      "  ...\n",
      "  [-9.93618488e-01 -4.65610623e-02  1.12453997e-01 ... -9.78096902e-01\n",
      "    4.71839517e-01  2.56728262e-01]\n",
      "  [-4.81020033e-01 -3.74495536e-01 -9.32064593e-01 ... -5.85456908e-01\n",
      "    3.26656610e-01  4.84141380e-01]\n",
      "  [-6.60768822e-02 -3.48719895e-01 -2.10786849e-01 ...  3.21450472e-01\n",
      "    2.53898948e-01  6.95884451e-02]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_4\n",
      "  Attribute: weight_names = [b'conv1d_4/kernel:0' b'conv1d_4/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_4/conv1d_4\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_4/conv1d_4/bias:0\n",
      "  Dataset: model_weights/conv1d_4/conv1d_4/bias:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.04192141 -0.01536965 -0.00587428 -0.00100589 -0.00465512]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_4/conv1d_4/kernel:0\n",
      "  Dataset: model_weights/conv1d_4/conv1d_4/kernel:0\n",
      "    Shape: (50, 1, 8)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 0.06574031  0.31769508  0.27609783 -1.1427283  -0.09990641\n",
      "    0.01312429 -0.30145615  0.30398545]]\n",
      "\n",
      " [[-0.25221092  0.959457    0.21044348 -1.3571343  -0.17480132\n",
      "    0.29675844  0.537789    0.06499722]]\n",
      "\n",
      " [[ 0.33401227  0.9719373   0.0077884  -1.6127483  -0.05604669\n",
      "    0.80065185 -0.23088455 -0.544829  ]]\n",
      "\n",
      " [[-0.32808843 -0.32562092 -0.14809047 -1.5100904   0.0215169\n",
      "    1.2645706  -0.3232751   0.35605192]]\n",
      "\n",
      " [[ 0.1078391  -1.6054729  -0.21731927 -1.0975145  -0.15077505\n",
      "    1.5632461   0.00467872 -0.9311888 ]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_5\n",
      "  Attribute: weight_names = [b'conv1d_5/kernel:0' b'conv1d_5/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_5/conv1d_5\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_5/conv1d_5/bias:0\n",
      "  Dataset: model_weights/conv1d_5/conv1d_5/bias:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.00590394  0.03466997 -0.0152177  -0.01085887  0.06336946]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_5/conv1d_5/kernel:0\n",
      "  Dataset: model_weights/conv1d_5/conv1d_5/kernel:0\n",
      "    Shape: (8, 8, 16)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[-2.85222232e-01 -2.27863407e+00 -2.63407886e-01 -8.04966867e-01\n",
      "    4.00118440e-01  5.73080420e-01  3.74676704e-01 -9.58000243e-01\n",
      "   -1.90614074e-01  1.25682664e+00 -2.07259631e+00 -1.57324934e+00\n",
      "    7.31335521e-01 -1.13142025e+00 -1.58603698e-01  1.51908785e-01]\n",
      "  [-9.59421813e-01  2.04326123e-01 -6.34884119e-01 -6.18913293e-01\n",
      "    3.78882468e-01  6.43477082e-01 -1.52289420e-01 -9.90760401e-02\n",
      "   -6.21957481e-01 -8.80560756e-01  3.52225065e-01 -2.76362085e+00\n",
      "    2.56384432e-01 -2.16292962e-01  3.01550865e-01 -8.26272368e-01]\n",
      "  [ 2.37120464e-01  2.47374952e-01 -5.94020724e-01  1.07895863e+00\n",
      "    6.74676478e-01 -1.37387887e-01 -4.06835169e-01 -2.00335577e-01\n",
      "    4.02587265e-01  5.65678954e-01  3.12406063e-01  7.18669534e-01\n",
      "    1.08486138e-01 -1.08676247e-01 -1.09315189e-02  4.42088753e-01]\n",
      "  [ 1.83865935e-01 -2.80235678e-01  1.53483838e-01 -2.38834485e-01\n",
      "    9.82856631e-01 -1.13232780e+00 -7.93711126e-01 -4.35145289e-01\n",
      "    1.12749442e-01 -3.49228591e-01  1.80126932e-02 -1.99853420e+00\n",
      "   -8.75793248e-02  6.91469237e-02 -3.24119717e-01 -9.19222772e-01]\n",
      "  [-2.08863080e-01 -4.62707967e-01  7.58862376e-01  7.92164865e-05\n",
      "   -2.89269030e-01 -1.15034270e+00 -7.49197304e-01  1.00726664e+00\n",
      "   -1.92092910e-01 -4.51635897e-01  9.42971483e-02  1.60489559e-01\n",
      "   -8.02972913e-01 -5.75437732e-02 -3.05766314e-01 -2.22026750e-01]\n",
      "  [-3.96967030e+00  2.89047301e-01  4.51502064e-03  9.47945714e-01\n",
      "    1.21669674e+00 -8.51878047e-01 -2.58323878e-01 -7.79283881e-01\n",
      "   -3.11776638e+00 -5.97330093e-01  7.28181750e-02 -3.56007874e-01\n",
      "   -4.81107384e-01 -5.63887119e-01 -2.44954243e-01 -5.51472306e-01]\n",
      "  [ 4.26153511e-01  5.00882387e-01 -2.26765454e-01  2.60711581e-01\n",
      "    4.84154671e-01  2.53982306e-01 -4.23781931e-01 -3.52284849e-01\n",
      "    6.26842976e-01 -1.96996972e-01  1.55185664e+00  8.14542830e-01\n",
      "   -1.63995910e+00  9.40551400e-01  1.34401047e+00  5.94332553e-02]\n",
      "  [ 7.08392680e-01  2.58445382e-01 -1.02564085e+00 -1.02106619e+00\n",
      "   -4.19923693e-01  8.04992616e-01  2.39884043e+00 -3.19398105e-01\n",
      "    6.96251333e-01  1.75469771e-01 -1.17216676e-01 -1.94721714e-01\n",
      "    4.56797987e-01 -2.26521492e-01 -1.40031457e+00 -1.06214654e+00]]\n",
      "\n",
      " [[-4.32709008e-02 -1.13258457e+00 -9.93521586e-02  1.11396439e-01\n",
      "    7.92216659e-02  3.03357057e-02  8.54639634e-02 -7.89949417e-01\n",
      "   -2.72061881e-02  1.25315750e+00 -1.58166683e+00 -2.10751772e+00\n",
      "    2.64819503e-01 -7.45539963e-01 -1.03643730e-01  9.67578143e-02]\n",
      "  [-3.81518984e+00  2.89205879e-01 -5.30325651e-01 -6.22408092e-01\n",
      "    2.52361387e-01  9.11024928e-01 -8.97793293e-01 -1.51190266e-01\n",
      "   -3.52451301e+00  2.61789024e-01  4.26853657e-01 -6.79170430e-01\n",
      "    5.58423460e-01 -9.65052009e-01 -3.47897530e-01 -5.24913192e-01]\n",
      "  [ 2.09986851e-01 -4.31434512e-01 -3.77309710e-01  1.83461457e-01\n",
      "    8.39098215e-01 -2.85071999e-01 -3.13915551e-01  4.84864593e-01\n",
      "    6.24907911e-01 -1.61195457e-01  3.04373324e-01 -5.17028332e-01\n",
      "   -3.76447052e-01  1.29426003e-01 -4.07446772e-01  6.42926931e-01]\n",
      "  [ 3.17075968e-01 -4.26822603e-01  4.83386397e-01  8.05097297e-02\n",
      "   -3.48375857e-01 -8.91897082e-01  6.02475643e-01 -3.34166586e-01\n",
      "    3.48228350e-04  3.80646414e-03 -1.82319656e-01 -2.43904090e+00\n",
      "   -1.36508673e-01  1.55165782e-02 -3.03206623e-01 -6.11923516e-01]\n",
      "  [-2.64060676e-01 -2.60095865e-01  6.97207600e-02 -1.11416556e-01\n",
      "   -1.26878619e+00 -7.79391170e-01 -3.27788025e-01  4.93790060e-01\n",
      "   -4.70712870e-01 -5.50665736e-01  4.19629574e-01 -6.40682042e-01\n",
      "   -8.60221207e-01 -1.47077486e-01 -1.17753685e+00  2.05466971e-01]\n",
      "  [-1.31328630e+00 -5.09804860e-02 -1.75963730e-01  3.84618253e-01\n",
      "    7.84544528e-01 -8.44045401e-01 -8.52570906e-02 -3.16191494e-01\n",
      "   -3.18214118e-01  2.27294296e-01 -7.81610683e-02  2.00935200e-01\n",
      "   -2.25883827e-01 -1.06472075e+00 -4.65102732e-01 -4.17345732e-01]\n",
      "  [-2.22389981e-01  2.98718184e-01  1.15893316e+00  3.32214653e-01\n",
      "    8.59794378e-01  3.19635719e-02  6.57295763e-01  4.78060842e-01\n",
      "    2.81068087e-02 -7.48238027e-01 -7.66393304e-01  1.10875690e+00\n",
      "   -1.03352201e+00  1.56461373e-01  4.51331675e-01  6.13115847e-01]\n",
      "  [ 7.67649040e-02  1.48662731e-01 -4.99256015e-01 -1.64883345e-01\n",
      "   -2.08701283e-01  7.82538176e-01  2.13936281e+00 -3.54295403e-01\n",
      "   -4.73443344e-02  5.73227048e-01  1.31622463e-01 -3.60612154e-01\n",
      "    2.80311882e-01  5.81780612e-01 -6.03627861e-01 -5.37344396e-01]]\n",
      "\n",
      " [[ 1.36208057e-01 -7.85498977e-01 -1.25257194e-01 -9.36492234e-02\n",
      "   -3.34531397e-01 -2.53276497e-01  1.46626411e-02 -3.84246469e-01\n",
      "   -6.65283620e-01  1.44345593e+00 -2.83636284e+00 -2.20229220e+00\n",
      "   -3.73971350e-02 -4.69689786e-01 -1.82084385e-02  3.22515845e-01]\n",
      "  [-1.27130163e+00  3.50824803e-01 -2.45676741e-01 -8.12112629e-01\n",
      "   -2.12227613e-01  8.06692898e-01 -2.04595223e-01  7.79442564e-02\n",
      "   -5.79179376e-02  1.07108839e-02 -2.00344846e-01 -6.64851308e-01\n",
      "    4.71439958e-01 -8.37351859e-01 -1.94991574e-01 -6.32701397e-01]\n",
      "  [-1.82150507e+00  8.64728332e-01  7.34751642e-01  4.51250821e-01\n",
      "    7.38496065e-01 -2.30953932e-01 -3.49762708e-01 -3.39794338e-01\n",
      "   -6.03874159e+00 -2.59659380e-01  3.80826801e-01  1.42080724e-01\n",
      "   -2.36990973e-01 -1.26528619e-02 -1.50330603e-01  6.06688321e-01]\n",
      "  [-1.37653494e+00 -6.62871063e-01 -1.61493674e-01  2.53792346e-01\n",
      "   -9.13698673e-02  2.95073122e-01 -2.00918704e-01 -1.63351104e-01\n",
      "   -3.90612721e+00  2.26882070e-01 -1.95028588e-01 -1.86770356e+00\n",
      "    1.01652399e-01 -8.70824516e-01 -5.79407699e-02 -7.93656558e-02]\n",
      "  [-3.54597330e-01  9.01663005e-01  1.29814947e+00 -1.45635319e+00\n",
      "   -7.64668703e-01 -8.15683901e-01 -8.61174345e-01  9.96113598e-01\n",
      "   -2.15875340e+00  5.39525926e-01 -6.22410178e-01 -4.83455062e-01\n",
      "   -1.19982672e+00 -7.62768030e-01 -1.21615040e+00 -8.39378834e-02]\n",
      "  [-9.73845497e-02 -1.01546216e+00 -2.27735668e-01  2.30548039e-01\n",
      "    4.21401709e-01 -4.52435575e-02 -1.48820356e-01 -1.07898057e-01\n",
      "   -5.53663112e-02  2.82408684e-01 -4.56364453e-01  1.55549690e-01\n",
      "   -6.73207164e-01 -5.41352034e-01  2.72131473e-01 -9.52940583e-01]\n",
      "  [-1.80172503e-01 -3.23063433e-01  6.64784238e-02  1.09535620e-01\n",
      "    3.41601521e-01 -7.49279916e-01 -7.09569454e-01  5.11286378e-01\n",
      "    1.13569193e-01 -1.86187327e-01 -1.01800370e+00 -3.64956796e-01\n",
      "    1.43190056e-01 -2.81560004e-01  1.77613401e+00  8.48409981e-02]\n",
      "  [ 1.16434373e-01  1.81746066e-01 -4.50566620e-01 -7.05541000e-02\n",
      "   -2.27877662e-01  1.03542686e+00  2.07732582e+00 -9.80675519e-01\n",
      "   -3.26489002e-01  1.85811788e-01  1.98782384e-01 -3.95810932e-01\n",
      "    6.22122049e-01  3.87798816e-01 -2.77551651e-01  7.22470284e-02]]\n",
      "\n",
      " [[ 2.21225470e-02 -7.38311529e-01 -1.29448101e-01 -2.37139374e-01\n",
      "   -4.99438345e-01 -1.19827867e-01  2.59871364e-01 -1.21817796e-03\n",
      "    6.55388599e-03  2.76248908e+00 -1.72989225e+00 -8.09756294e-02\n",
      "   -4.76805866e-01 -4.50659841e-01 -1.15415834e-01 -3.69827121e-01]\n",
      "  [-4.12540101e-02 -3.76537025e-01 -3.97364557e-01 -8.07019591e-01\n",
      "   -3.47352594e-01  8.08722615e-01 -1.96989164e-01 -2.58791208e-01\n",
      "   -7.87325129e-02 -3.22179943e-01  4.80384320e-01  3.88818048e-02\n",
      "    3.59083951e-01 -6.99605882e-01 -7.85782784e-02 -1.04396868e+00]\n",
      "  [-1.23579435e-01  3.02561224e-01 -1.40443552e+00  2.97133476e-01\n",
      "    2.92422205e-01 -2.61386573e-01 -1.16363324e-01 -8.24450791e-01\n",
      "    4.43847358e-01  4.71751004e-01 -8.73099387e-01  7.16256440e-01\n",
      "    5.75633585e-01 -7.99830794e-01  6.41458511e-01  7.67247498e-01]\n",
      "  [-1.51257825e+00  2.35221282e-01  2.43036658e-01 -3.36195566e-02\n",
      "    5.25514424e-01 -6.33432746e-01 -4.84979153e-01 -5.39203942e-01\n",
      "   -1.76998749e-01  2.82595545e-01 -4.11116689e-01  3.88911545e-01\n",
      "    1.01570688e-01 -2.19146824e+00 -3.47597778e-01  3.83540273e-01]\n",
      "  [-2.73581028e-01  7.15975404e-01 -6.98013008e-01 -1.36204767e+00\n",
      "   -7.96858311e-01 -1.60838652e+00 -2.94677466e-01  6.88019872e-01\n",
      "   -6.58217147e-02  1.28016567e+00 -1.74059904e+00  2.69796073e-01\n",
      "   -1.60175645e+00 -7.45865852e-02 -3.17794502e-01 -1.44672382e+00]\n",
      "  [ 2.43727997e-01 -1.97369659e+00  2.63733953e-01  3.75607214e-03\n",
      "    2.37065688e-01 -9.79153395e-01  1.85264394e-01 -8.97451341e-01\n",
      "   -3.73723470e-02  1.33377656e-01  2.35243991e-01 -2.96845257e-01\n",
      "   -2.08732772e+00 -3.18507522e-01 -2.19331995e-01 -8.62575591e-01]\n",
      "  [-1.38190806e-01 -2.89078087e-01  8.57029200e-01  1.38297096e-01\n",
      "    5.70170343e-01 -9.01101470e-01 -2.43898019e-01 -2.19694704e-01\n",
      "    1.79844841e-01 -6.24784172e-01  5.00547290e-01  8.29652190e-01\n",
      "    1.36054146e+00 -1.13518989e+00 -7.34163597e-02  1.59832343e-01]\n",
      "  [-6.48163185e-02 -2.18665674e-01 -7.05291748e-01  5.30818217e-02\n",
      "   -7.82967061e-02  1.23014140e+00  1.93700409e+00 -1.76151597e+00\n",
      "    1.64435700e-01 -1.72579557e-01  5.00659943e-01 -1.83334678e-01\n",
      "    9.82855618e-01 -1.46591440e-01 -1.05191804e-01  1.48514193e-02]]\n",
      "\n",
      " [[ 3.50366272e-02 -9.52085316e-01  1.77585721e-01 -3.56271088e-01\n",
      "   -3.56732666e-01 -5.01407012e-02  1.32469609e-02 -9.44308117e-02\n",
      "    1.25483364e-01  3.64604503e-01 -2.37256661e-01 -1.09900713e-01\n",
      "   -5.59111297e-01 -4.52204466e-01 -1.11566529e-01  4.22503531e-01]\n",
      "  [-2.46217117e-01  5.08166075e-01 -7.84312129e-01 -4.83063936e-01\n",
      "    2.07769826e-01 -1.04657153e-03 -3.08116347e-01 -2.17214406e-01\n",
      "   -7.64785558e-02 -4.90673333e-02  5.27391970e-01  8.05452093e-02\n",
      "    1.43246889e-01 -9.41400945e-01  6.53330386e-02 -1.98308602e-01]\n",
      "  [ 9.02166665e-02 -3.82770836e-01 -1.26136363e-01  3.71332020e-01\n",
      "   -7.17195272e-02 -2.56332427e-01 -2.76335329e-01  3.04447990e-02\n",
      "    6.29218221e-02  7.85650983e-02 -3.10580283e-01  1.97881758e-01\n",
      "    4.14634794e-01  3.34147364e-01  1.14858329e-01  3.02945644e-01]\n",
      "  [-3.22092712e-01  2.88922697e-01  1.31374329e-01  1.03613102e+00\n",
      "    8.57238472e-01 -9.74724472e-01  1.88925222e-01 -2.73226295e-02\n",
      "    1.63474351e-01  2.54670680e-01 -1.61683753e-01  3.63506496e-01\n",
      "   -1.62434597e-02 -7.63839543e-01  1.65545702e-01  2.56398559e-01]\n",
      "  [-3.95735689e-02 -8.02972168e-03  3.27684611e-01 -1.17426109e+00\n",
      "   -5.13605118e-01 -9.58954811e-01 -8.04717361e-04  7.49856710e-01\n",
      "   -2.95189440e-01 -1.35192260e-01 -6.29937291e-01 -3.58634114e-01\n",
      "   -4.04745162e-01  2.18805984e-01 -7.48754919e-01 -1.20323193e+00]\n",
      "  [ 6.30955279e-01  1.05731934e-01 -7.97689736e-01 -3.93788964e-01\n",
      "   -2.74501536e-02 -4.06724244e-01 -1.19889334e-01 -9.03409660e-01\n",
      "   -4.41979289e-01  1.82052180e-01  2.41764188e-01 -3.49828660e-01\n",
      "   -1.47124529e+00 -6.15910470e-01  8.15297961e-02 -8.61390352e-01]\n",
      "  [-2.75343895e-01 -3.21407795e-01 -6.70817196e-01  2.51138955e-01\n",
      "    4.94711071e-01  1.14900947e-01  7.51656070e-02 -2.83336818e-01\n",
      "    1.55529305e-01 -4.19910580e-01  6.90267310e-02  1.88523576e-01\n",
      "   -7.69982219e-01  1.09910548e+00  3.77620190e-01  2.63805002e-01]\n",
      "  [ 2.79678434e-01 -3.96035165e-01 -7.55710900e-01 -7.89649435e-04\n",
      "   -5.74096143e-01  1.13563406e+00  9.88710403e-01 -1.94107628e+00\n",
      "   -1.65741574e-02 -1.42755687e-01  1.08269647e-01 -2.42209211e-01\n",
      "    6.75882220e-01 -5.67832768e-01  4.37299907e-02 -2.51506865e-01]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_6\n",
      "  Attribute: weight_names = [b'conv1d_6/kernel:0' b'conv1d_6/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_6/conv1d_6\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_6/conv1d_6/bias:0\n",
      "  Dataset: model_weights/conv1d_6/conv1d_6/bias:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[0.16987623 0.04774356 0.09674425 0.01205599 0.44299862]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_6/conv1d_6/kernel:0\n",
      "  Dataset: model_weights/conv1d_6/conv1d_6/kernel:0\n",
      "    Shape: (8, 16, 32)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 2.40362599e-01  6.44662827e-02 -7.47087479e-01 ...  1.70634344e-01\n",
      "   -6.80519462e-01 -5.68372965e-01]\n",
      "  [-2.94300270e+00  6.70605719e-01  2.94967264e-01 ...  2.92662755e-02\n",
      "   -3.02216649e-01 -9.61097181e-02]\n",
      "  [-2.27199841e+00 -1.35147780e-01  1.05782419e-01 ... -1.05029500e+00\n",
      "   -6.88118398e-01 -1.90956640e+00]\n",
      "  ...\n",
      "  [-1.31936705e+00 -1.96225435e-01  1.73436999e-01 ...  1.45186210e+00\n",
      "   -8.29919517e-01 -1.54509270e+00]\n",
      "  [-9.09692526e-01 -8.50158453e-01 -8.46743405e-01 ... -4.28342909e-01\n",
      "    5.01280665e-01 -7.71167994e-01]\n",
      "  [-3.25326473e-01 -4.68283594e-01 -8.71572793e-01 ... -4.47480291e-01\n",
      "    1.05190551e+00  2.21915022e-02]]\n",
      "\n",
      " [[ 3.82700384e-01  3.16963363e+00  6.22791313e-02 ...  9.98311713e-02\n",
      "   -1.99268639e-01 -6.05415463e-01]\n",
      "  [-1.86435771e+00  6.85195923e-02  8.29254031e-01 ... -2.41830260e-01\n",
      "   -2.60674432e-02 -2.24497154e-01]\n",
      "  [-1.73917973e+00 -2.39605457e-01 -1.88630953e-01 ... -6.22828662e-01\n",
      "   -4.36515272e-01 -1.59818769e+00]\n",
      "  ...\n",
      "  [-2.23521471e+00  1.68555588e-01  1.12528348e+00 ...  9.51735198e-01\n",
      "   -5.32362163e-01 -1.91693234e+00]\n",
      "  [-7.34717250e-01 -6.13985717e-01 -6.23416066e-01 ... -4.48372513e-01\n",
      "    1.68623477e-01 -6.42072260e-01]\n",
      "  [-3.94910425e-01 -3.56477678e-01 -8.14438760e-01 ... -4.50490892e-01\n",
      "    3.14188957e-01 -1.53793976e-01]]\n",
      "\n",
      " [[ 1.07509620e-01 -1.27044013e-02 -2.99949590e-02 ...  2.11840296e+00\n",
      "    2.23766416e-02  1.90769713e-02]\n",
      "  [-1.68820024e+00  1.32611850e-02  7.83213973e-01 ... -2.93775707e-01\n",
      "   -7.61198476e-02  1.45362645e-01]\n",
      "  [-1.40136278e+00 -1.87594429e-04  9.52700749e-02 ... -1.40562415e-01\n",
      "    2.62885898e-01 -4.96858507e-01]\n",
      "  ...\n",
      "  [-3.13914347e+00 -2.06357822e-01  6.01498902e-01 ...  1.04694271e+00\n",
      "    5.29779017e-01  8.18423480e-02]\n",
      "  [-7.70538092e-01 -5.26493907e-01  1.12641051e-01 ...  5.04716039e-01\n",
      "   -3.51748019e-01 -3.80670041e-01]\n",
      "  [-3.00531924e-01 -2.23119169e-01  3.28294218e-01 ...  6.01144075e-01\n",
      "    9.15357053e-01  7.40638748e-02]]\n",
      "\n",
      " [[ 8.08612555e-02  8.03888142e-02 -3.83905053e-01 ...  2.26289049e-01\n",
      "   -5.81738260e-03  1.37347147e-01]\n",
      "  [-1.11059081e-02 -1.20349345e-03  9.78124321e-01 ...  2.01378971e-01\n",
      "   -5.12980700e-01  9.01163459e-01]\n",
      "  [-5.02165616e-01  1.16963014e-02  4.45458293e-01 ... -2.69198596e-01\n",
      "   -8.74388874e-01  1.01975369e+00]\n",
      "  ...\n",
      "  [ 1.01642851e-02 -3.88398111e-01  1.78740378e-02 ...  7.42785931e-01\n",
      "    8.59035194e-01  1.21736787e-01]\n",
      "  [-5.28378725e-01 -3.09377491e-01  5.45290858e-02 ...  2.96196222e-01\n",
      "   -5.10238409e-01 -1.81788236e-01]\n",
      "  [-2.83313453e-01 -6.80419952e-02 -1.05224609e+00 ...  1.17745794e-01\n",
      "   -6.75108582e-02 -4.95179296e-02]]\n",
      "\n",
      " [[ 5.71492732e-01  9.38337743e-02 -3.60244542e-01 ...  1.82208180e-01\n",
      "    1.43757090e-01  8.54995310e-01]\n",
      "  [-5.61168969e-01 -2.88647562e-02  1.04134202e+00 ... -1.20703764e-01\n",
      "   -8.04523289e-01  8.89406681e-01]\n",
      "  [-3.35978717e-01  1.24160707e-01  3.38185113e-03 ... -3.71691436e-01\n",
      "   -4.85266805e-01  3.67753416e-01]\n",
      "  ...\n",
      "  [-8.75724018e-01 -3.96907181e-01  6.62629366e-01 ...  9.02447045e-01\n",
      "    8.50974679e-01  3.60858679e-01]\n",
      "  [-3.43083411e-01 -3.20237160e-01  3.90595019e-01 ...  3.10414284e-01\n",
      "   -6.04413688e-01 -8.31768364e-02]\n",
      "  [-1.88002378e-01 -2.93667807e-04  3.59701253e-02 ...  1.11385798e-02\n",
      "   -1.30789721e+00 -3.67309183e-01]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_7\n",
      "  Attribute: weight_names = [b'conv1d_7/kernel:0' b'conv1d_7/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_7/conv1d_7\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_7/conv1d_7/bias:0\n",
      "  Dataset: model_weights/conv1d_7/conv1d_7/bias:0\n",
      "    Shape: (8,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.00431551  0.04016116  0.01654938  0.03048671 -0.01397276]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_7/conv1d_7/kernel:0\n",
      "  Dataset: model_weights/conv1d_7/conv1d_7/kernel:0\n",
      "    Shape: (50, 1, 8)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 0.03979348  0.6692164  -0.2917886  -0.25431588 -0.08387803\n",
      "    0.0019721  -0.73914844 -0.23150344]]\n",
      "\n",
      " [[ 0.06065623  0.5179755   0.11608855  0.50195086  0.3238954\n",
      "    0.05827936 -0.14301825  0.7115992 ]]\n",
      "\n",
      " [[-0.01754944  0.8269352  -0.63334477 -0.22947073  0.36470225\n",
      "   -0.10969609 -0.584911    0.06466799]]\n",
      "\n",
      " [[ 0.10672908  0.93777907 -0.80262995 -0.23374534  0.23520987\n",
      "   -0.08954538  0.21462244 -0.7837997 ]]\n",
      "\n",
      " [[-0.06389689  0.83958495  0.07786213  0.8808438   0.7068247\n",
      "   -0.34264898 -0.14959568 -0.0849199 ]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_8\n",
      "  Attribute: weight_names = [b'conv1d_8/kernel:0' b'conv1d_8/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_8/conv1d_8\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_8/conv1d_8/bias:0\n",
      "  Dataset: model_weights/conv1d_8/conv1d_8/bias:0\n",
      "    Shape: (16,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.03006437 -0.10269627 -0.01823063 -0.05262578 -0.0143279 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_8/conv1d_8/kernel:0\n",
      "  Dataset: model_weights/conv1d_8/conv1d_8/kernel:0\n",
      "    Shape: (8, 8, 16)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 5.35984457e-01 -5.40273309e-01 -4.88316379e-02  3.62679422e-01\n",
      "    1.00390553e+00 -1.84625757e+00  3.75489116e-01  3.00521523e-01\n",
      "    5.81443727e-01  1.05162980e-02 -3.03182870e-01 -1.04770124e+00\n",
      "    8.28043580e-01 -5.53045094e-01 -7.09336400e-01 -7.36448944e-01]\n",
      "  [-4.38763648e-01 -1.51695514e+00  7.85206199e-01 -1.08215368e+00\n",
      "    4.32679296e-01 -1.63813639e+00  2.72778422e-01 -4.64935064e-01\n",
      "    1.42380431e-01 -1.40933216e-01 -2.71273792e-01  1.97474718e-01\n",
      "   -6.49064183e-01 -1.61570862e-01  3.73489670e-02  2.42213011e-01]\n",
      "  [-1.29066443e+00  1.52565271e-01 -5.84906014e-03 -1.21831214e+00\n",
      "    1.58206797e+00 -5.20236492e-01  1.01857424e+00 -7.95019686e-01\n",
      "   -4.55496907e-02  1.75949752e-01  5.87100804e-01 -1.05226291e-02\n",
      "    8.09843302e-01  5.32943666e-01  1.47065270e+00 -7.98460364e-01]\n",
      "  [-5.31739414e-01  1.28023517e+00  6.48937523e-01  1.53454930e-01\n",
      "   -1.45135522e-02 -8.31184566e-01  6.76596403e-01 -1.39383301e-01\n",
      "   -1.17077708e+00 -1.44922197e+00 -1.68894112e+00 -3.66328537e-01\n",
      "    5.20017087e-01 -1.05428360e-01  1.84287593e-01 -2.95540422e-01]\n",
      "  [-1.88172191e-01 -7.86036909e-01 -6.27404928e-01  8.30921113e-01\n",
      "    6.83432817e-02 -2.38067135e-01  4.85379338e-01 -4.71349731e-02\n",
      "   -2.07180217e-01 -5.04516840e-01 -1.49293554e+00 -1.00679919e-01\n",
      "    9.25583243e-01 -1.43616021e-01  1.48148268e-01  5.46967804e-01]\n",
      "  [-4.46146637e-01 -4.88541067e-01 -7.60468934e-03 -1.25665593e+00\n",
      "    1.52793956e+00 -2.01372147e+00  4.94397670e-01 -7.31837630e-01\n",
      "    2.62168854e-01 -1.25786185e+00  2.41830006e-01 -8.48372698e-01\n",
      "   -1.46541405e+00  4.40023363e-01 -8.32967341e-01  4.18116689e-01]\n",
      "  [ 4.89003599e-01 -7.88844585e-01 -2.68941641e+00  2.76794620e-02\n",
      "   -2.25234870e-02  1.24252237e-01  9.08186197e-01  9.76693556e-02\n",
      "    6.14974260e-01 -3.83604765e-01 -1.17094174e-01 -2.12782174e-01\n",
      "   -6.18943989e-01 -5.92700064e-01 -5.41944146e-01 -8.91430020e-01]\n",
      "  [ 2.38520339e-01 -3.63021761e-01 -7.25444779e-02  3.69742781e-01\n",
      "    3.65538985e-01  3.24041784e-01  5.13651431e-01  2.20108777e-01\n",
      "    1.39718723e+00  5.67425668e-01  4.60485905e-01 -1.17022246e-01\n",
      "   -1.37389338e+00  2.17376024e-01  7.71864593e-01 -7.91665852e-01]]\n",
      "\n",
      " [[-1.82498306e-01 -6.53420925e-01 -8.66688602e-03  2.32103974e-01\n",
      "    8.13691616e-01 -7.55601585e-01  4.78833318e-01  4.33369905e-01\n",
      "    6.84218347e-01  2.80494452e-01 -1.62544310e-01 -8.48210931e-01\n",
      "    2.64689922e-01 -4.17144269e-01 -2.33716875e-01  4.97930348e-02]\n",
      "  [ 1.77256361e-01 -4.32129592e-01 -7.21336663e-01 -5.84068418e-01\n",
      "    4.92123812e-01 -1.00464690e+00  3.10586452e-01 -2.06665352e-01\n",
      "   -6.16736174e-01 -5.14901459e-01 -1.35742843e-01 -1.55883387e-01\n",
      "    8.89323354e-01 -1.28758565e-01  1.77914679e-01  3.65940839e-01]\n",
      "  [-7.57360041e-01  7.32673764e-01  2.31907904e-01 -9.02201772e-01\n",
      "    7.94676304e-01 -1.15065682e+00 -8.13459009e-02 -6.82127416e-01\n",
      "   -4.89315987e-01 -4.13939208e-01  1.17318451e-01 -7.77444541e-02\n",
      "   -3.08612317e-01  3.08899820e-01  1.05569935e+00  5.56258261e-01]\n",
      "  [ 2.21986100e-01 -1.07222600e-02  1.08926371e-01  2.55631655e-02\n",
      "    1.79733157e-01 -9.14154172e-01 -4.50861007e-01 -9.76638496e-02\n",
      "   -8.98776591e-01 -2.09475923e+00 -1.28786552e+00 -1.65813714e-01\n",
      "    2.97550350e-01  8.55480850e-01 -2.83430755e-01  6.24783576e-01]\n",
      "  [-7.33577549e-01 -4.62234229e-01  8.43646526e-01  7.64165044e-01\n",
      "   -3.72385621e-01  1.37732401e-01  4.80327308e-01  3.13342273e-01\n",
      "   -1.84735477e-01 -1.81609303e-01 -4.12298143e-01 -3.41870517e-01\n",
      "    6.63403749e-01  1.06988363e-01  8.67677107e-02 -2.00697109e-01]\n",
      "  [ 3.52794826e-01  5.06372988e-01 -3.58728617e-01 -4.88329202e-01\n",
      "    1.00039923e+00 -3.48073065e-01 -3.31268311e-02  1.73312449e+00\n",
      "   -1.55985725e+00 -1.52111614e+00  1.77256316e-01 -1.05593324e+00\n",
      "   -8.40095341e-01  1.19415417e-01 -1.85359526e+00 -3.85520637e-01]\n",
      "  [-1.27080131e+00 -6.15277588e-01 -2.77998137e+00 -5.28105050e-02\n",
      "   -1.14603639e-01 -7.17994392e-01  9.28213716e-01  6.34095222e-02\n",
      "   -1.71679273e-01 -8.97759259e-01  7.38511324e-01 -3.70985687e-01\n",
      "   -2.15647727e-01 -9.42906797e-01 -1.19468474e+00  7.15485439e-02]\n",
      "  [ 7.03559637e-01 -9.16091383e-01 -5.17844439e-01  1.14163049e-01\n",
      "   -2.85017252e-01  6.05018616e-01  8.12631667e-01 -2.83589810e-01\n",
      "    1.05309892e+00  5.88384420e-02  7.94418037e-01  1.03303846e-02\n",
      "   -5.68722904e-01  8.11629593e-02  3.87577504e-01  4.70205069e-01]]\n",
      "\n",
      " [[ 1.68644637e-01 -1.01175211e-01 -7.63562396e-02  5.07950127e-01\n",
      "   -6.99569061e-02 -1.61054051e+00  4.86613393e-01  5.49364865e-01\n",
      "    1.18755937e+00  3.93464744e-01 -2.10416004e-01 -6.18173718e-01\n",
      "    7.48794973e-01 -4.54073578e-01 -8.31543803e-01 -1.11393309e+00]\n",
      "  [-5.58038391e-02 -1.52257466e+00 -1.87540162e+00 -5.97358167e-01\n",
      "    3.86871248e-01 -9.87186283e-03  1.06360388e+00 -6.37717545e-01\n",
      "   -6.18847460e-02 -4.51163411e-01 -1.20737612e+00 -4.54254359e-01\n",
      "   -6.15992732e-02  5.20647541e-02 -3.42682540e-01 -9.07208323e-02]\n",
      "  [-5.07759809e-01  5.94874263e-01  3.31999987e-01 -6.82076737e-02\n",
      "    9.49857533e-01 -3.32162976e-01 -1.74158379e-01 -1.94912910e-01\n",
      "   -3.86463225e-01  1.63828850e-01  2.18098819e-01  7.97808096e-02\n",
      "   -4.66577038e-02  3.03968936e-01  5.02294719e-01 -3.72728482e-02]\n",
      "  [ 1.04221188e-01 -2.44429618e-01  8.41795281e-02  6.39262572e-02\n",
      "    2.05463454e-01 -1.43188462e-01 -6.66708276e-02 -5.84258854e-01\n",
      "   -1.83353186e-01 -9.87281382e-01 -1.69612491e+00 -2.08358452e-01\n",
      "    3.75705659e-01  8.47320437e-01  4.53987837e-01  1.25286579e-01]\n",
      "  [-7.78201461e-01 -9.14982557e-01  1.78081200e-01 -3.66917431e-01\n",
      "    9.12814796e-01  8.56001139e-01  2.24339649e-01  5.47541678e-01\n",
      "    1.23227157e-01  5.73284924e-01  1.42431334e-01 -1.96481124e-01\n",
      "    9.04589295e-01 -6.83662966e-02  6.30200446e-01 -1.14040124e+00]\n",
      "  [ 5.69148697e-02  3.84029746e-02 -1.64995953e-01 -2.04627484e-01\n",
      "    6.59030557e-01  1.12431265e-01 -1.04951382e+00 -1.49708641e+00\n",
      "   -6.63415730e-01  5.90650856e-01 -1.43706322e+00  1.43259063e-01\n",
      "   -1.49094775e-01  2.10902035e-01 -1.67898536e+00 -3.48433256e-01]\n",
      "  [ 6.82185590e-01 -5.60429931e-01 -2.01071978e+00 -2.48087853e-01\n",
      "    3.16908032e-01 -3.67403805e-01  6.75058126e-01  9.03052568e-01\n",
      "   -7.77769864e-01 -3.14004987e-01  4.36362773e-01 -1.75640658e-01\n",
      "   -6.52443469e-01  3.82409066e-01 -2.98676491e-01  6.60615504e-01]\n",
      "  [ 1.46305811e+00 -1.36355948e+00 -7.25887954e-01  6.70234486e-02\n",
      "   -2.02920139e-01 -4.59970742e-01 -1.13531137e-02  4.63846534e-01\n",
      "    2.52398401e-01  3.68128456e-02  3.66557837e-01  9.01471078e-02\n",
      "   -3.61105800e-02  2.55604357e-01 -4.54612151e-02  2.76439220e-01]]\n",
      "\n",
      " [[ 7.79311359e-02 -1.87888771e-01 -2.89020911e-02  4.80801821e-01\n",
      "   -1.10215008e-01  6.87763020e-02  4.75919574e-01  7.98581123e-01\n",
      "    8.54484975e-01  4.48749632e-01  2.45143339e-01 -1.35795438e+00\n",
      "    5.08666217e-01 -1.44407749e-01 -1.50174260e+00 -7.42486656e-01]\n",
      "  [-9.28010404e-01 -1.12417805e+00 -1.03700078e+00 -1.25886309e+00\n",
      "   -5.98596215e-01  3.12868580e-02 -9.61650759e-02  2.37408414e-01\n",
      "    7.83904254e-01 -6.32102013e-01 -1.64223087e+00 -4.98785451e-02\n",
      "   -3.32685322e-01 -1.10640883e-01  1.16972521e-01 -9.09076929e-01]\n",
      "  [-5.83397686e-01  1.20285940e+00  2.29183987e-01 -6.76396728e-01\n",
      "    6.85266852e-01 -6.32423460e-01 -3.34579468e-01 -4.28450018e-01\n",
      "   -3.78109157e-01  5.40510356e-01  4.30230498e-01 -8.27583745e-02\n",
      "   -3.16724509e-01 -2.17809260e-01  1.06656981e+00 -1.76641241e-01]\n",
      "  [ 2.73392498e-01 -1.04211114e-01 -8.99106488e-02  9.63195860e-02\n",
      "    2.92364448e-01 -7.35018015e-01  2.13466749e-01 -1.07757795e+00\n",
      "    8.40640292e-02  1.34556517e-02 -1.04252681e-01 -2.42797256e-01\n",
      "    5.46431899e-01  1.17505848e+00 -3.17637846e-02  4.79164720e-01]\n",
      "  [-1.93049371e+00  1.40502408e-01  5.96924365e-01  5.88203184e-02\n",
      "    3.45354915e-01  1.18005466e+00  2.41441563e-01  2.41312355e-01\n",
      "   -2.66889948e-02  2.52074259e-03  7.92867064e-01 -5.37082493e-01\n",
      "    4.86600608e-01 -9.34503615e-01 -2.88289666e-01 -1.68273842e+00]\n",
      "  [ 2.70774215e-01  1.28315583e-01 -2.80259699e-01  1.34769052e-01\n",
      "    6.80313528e-01  4.11911637e-01 -1.47178924e+00 -3.29217553e-01\n",
      "   -8.33063364e-01  8.45925212e-01 -2.85163552e-01  1.26817238e+00\n",
      "    9.52820033e-02  3.79759490e-01 -2.43190512e-01  3.09841424e-01]\n",
      "  [ 8.26158822e-01  6.78497180e-02  1.15796901e-01  1.45115569e-01\n",
      "    3.84371430e-01 -3.86938274e-01  5.08266211e-01  5.09367228e-01\n",
      "   -3.14207792e-01 -7.77578130e-02 -9.52537835e-01 -5.69420815e-01\n",
      "   -9.81108069e-01  4.79996800e-01 -8.87525454e-02  6.13853261e-02]\n",
      "  [ 1.16658247e+00 -4.21447963e-01 -5.50614476e-01  7.68588722e-01\n",
      "    6.64055254e-03 -6.90853238e-01  4.25193667e-01  8.99312794e-01\n",
      "    2.55444590e-02  4.96172041e-01 -2.04558611e-01 -2.88500398e-01\n",
      "   -3.21581930e-01 -6.35154307e-01  8.04969668e-01  1.34839654e-01]]\n",
      "\n",
      " [[ 5.19483447e-01 -5.19682705e-01  9.00521055e-02  3.37644108e-02\n",
      "    7.20230211e-03 -1.78595558e-01  6.45101368e-01  6.55606687e-01\n",
      "    9.60557044e-01  6.77601874e-01  6.95187449e-02 -1.01180768e+00\n",
      "    6.51010990e-01  6.66631479e-03 -1.01482809e+00  2.72124261e-01]\n",
      "  [ 1.29378843e+00 -8.12000275e-01 -1.55644858e+00 -7.24448919e-01\n",
      "   -3.00175190e-01 -2.36661062e-01  6.75704718e-01  2.04172418e-01\n",
      "    5.22288382e-01 -3.39240789e-01  4.05686110e-01  1.15950488e-01\n",
      "   -4.78719205e-01  7.25477099e-01  4.05312628e-02  4.99145716e-01]\n",
      "  [-8.58020604e-01  1.08991218e+00 -2.11206779e-01 -6.15764797e-01\n",
      "    1.57429492e+00  2.20760959e-03 -4.65256602e-01 -5.39306030e-02\n",
      "   -7.76043236e-01  1.14129591e+00 -2.64720500e-01 -2.02485800e-01\n",
      "   -1.69539109e-01  3.67459089e-01  3.85031402e-01  3.14978391e-01]\n",
      "  [ 2.89372981e-01 -1.60615012e-01 -2.26453289e-01 -2.50007033e-01\n",
      "    5.50580323e-01 -1.27398241e+00 -8.45710039e-01 -1.96459484e+00\n",
      "   -2.15593398e-01 -2.41636321e-01  4.94223207e-01  8.43308151e-01\n",
      "    9.89170015e-01  7.08355665e-01  2.95004934e-01 -1.94118142e-01]\n",
      "  [-1.07082939e+00  2.83609211e-01 -2.84515843e-02 -2.10900992e-01\n",
      "    2.68294036e-01 -3.11764061e-01  9.39338565e-01  4.07101121e-03\n",
      "   -9.01309922e-02 -2.75519013e-01 -1.45917213e+00 -9.62503374e-01\n",
      "    9.87548053e-01 -8.05479646e-01 -6.15365148e-01  5.98527730e-01]\n",
      "  [ 5.35850190e-02 -5.24302907e-02 -1.39599919e-01 -3.43517095e-01\n",
      "    1.72156811e-01  5.86149096e-01 -1.42360198e+00 -2.58040577e-01\n",
      "   -2.07912937e-01  4.35761511e-01 -1.51006174e+00  8.80158424e-01\n",
      "   -2.04079255e-01 -5.09593904e-01  2.59245068e-01 -1.04451764e+00]\n",
      "  [ 7.13852718e-02 -2.68469512e-01  1.64138436e-01 -1.62446126e-01\n",
      "    2.66415626e-01  1.94065064e-01  3.07044864e-01 -6.22120798e-02\n",
      "    2.94967681e-01  4.74452823e-01 -3.93668503e-01  1.58497125e-01\n",
      "   -8.16786945e-01 -1.06705815e-01 -4.59513962e-01 -4.05058771e-01]\n",
      "  [-7.58312106e-01  4.25368488e-01 -4.76895899e-01  4.30236936e-01\n",
      "    1.94535360e-01  6.54193908e-02  4.95555043e-01  1.82969198e-01\n",
      "    6.35823131e-01  8.96816671e-01  1.81776106e-01  9.46521342e-01\n",
      "   -9.01538134e-01 -1.53872764e+00  4.26127352e-02 -6.16781175e-01]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_9\n",
      "  Attribute: weight_names = [b'conv1d_9/kernel:0' b'conv1d_9/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_9/conv1d_9\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_9/conv1d_9/bias:0\n",
      "  Dataset: model_weights/conv1d_9/conv1d_9/bias:0\n",
      "    Shape: (32,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.00097041  0.08626677 -0.01383911  0.03063971  0.02676679]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/conv1d_9/conv1d_9/kernel:0\n",
      "  Dataset: model_weights/conv1d_9/conv1d_9/kernel:0\n",
      "    Shape: (8, 16, 32)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[[[ 1.30498028e+00  2.43273512e-01  3.21206003e-01 ...  4.97755706e-01\n",
      "    2.26342566e-02 -3.73736948e-01]\n",
      "  [-8.67273331e-01 -1.12970844e-02 -3.77069592e-01 ... -7.04411417e-02\n",
      "    2.40357205e-01 -4.07891162e-02]\n",
      "  [-8.33218098e-01  1.81929395e-01  4.85704541e-02 ...  3.77070487e-01\n",
      "    5.10477126e-01 -3.25251967e-01]\n",
      "  ...\n",
      "  [ 9.43659902e-01 -7.71715716e-02  8.50894600e-02 ... -2.09083214e-01\n",
      "   -8.66123199e-01  5.01800179e-01]\n",
      "  [ 4.31425065e-01  9.74956062e-03 -3.59576434e-01 ...  9.78774652e-02\n",
      "   -3.97098213e-01 -1.04902935e+00]\n",
      "  [ 1.30788219e+00 -1.07414071e-02 -5.73004067e-01 ...  3.57173644e-02\n",
      "   -1.46739170e-01  1.44444898e-01]]\n",
      "\n",
      " [[-5.31792581e-01 -5.33894360e-01 -1.18112907e-01 ...  7.52897784e-02\n",
      "    2.07634494e-01  8.31012905e-01]\n",
      "  [-7.85252810e-01  1.99984387e-01  1.39939621e-01 ... -3.92398089e-01\n",
      "    3.36010337e-01 -1.13080405e-01]\n",
      "  [-2.40710542e-01  1.10886216e-01 -3.73733342e-01 ...  4.59395289e-01\n",
      "    2.29328766e-01 -5.92525862e-02]\n",
      "  ...\n",
      "  [-4.67470549e-02 -3.27825584e-02  2.76023522e-02 ... -6.58712536e-02\n",
      "   -6.32196248e-01  2.87808776e-01]\n",
      "  [ 6.97127104e-01 -1.78578645e-02 -2.63425648e-01 ... -2.80902609e-02\n",
      "   -3.16543549e-01  1.60104170e-01]\n",
      "  [ 7.80262172e-01 -3.15917552e-01 -6.76291227e-01 ...  2.00230375e-01\n",
      "    3.77585590e-02  5.85016124e-02]]\n",
      "\n",
      " [[ 5.06370187e-01  6.94295585e-01  2.86776841e-01 ...  2.06362624e-02\n",
      "   -5.34918010e-01 -2.08501235e-01]\n",
      "  [-5.07051110e-01 -1.50558993e-03 -9.73600745e-01 ... -2.10476294e-01\n",
      "    5.16408756e-02 -2.18086764e-01]\n",
      "  [-3.30157936e-01 -1.62609071e-01 -9.78243276e-02 ... -4.89951074e-02\n",
      "    3.29214007e-01  2.01589204e-02]\n",
      "  ...\n",
      "  [ 2.63420224e-01  2.95390248e-01  3.74803960e-01 ... -4.43952717e-02\n",
      "   -6.57365978e-01  1.02598248e-02]\n",
      "  [ 5.69617093e-01 -2.70343900e-01 -1.04702488e-01 ... -3.35688114e-01\n",
      "   -2.39666700e-01  2.27908388e-01]\n",
      "  [ 2.32793391e-01  3.48245144e-01 -5.00518739e-01 ... -1.23897649e-01\n",
      "    2.69377112e-01  5.99081777e-02]]\n",
      "\n",
      " [[ 1.93913117e-01 -1.38698936e-01  1.40080810e-01 ... -1.45381123e-01\n",
      "   -9.81237710e-01  1.60354376e-01]\n",
      "  [-7.36223400e-01 -8.96910578e-03 -3.55297893e-01 ... -6.96651399e-01\n",
      "   -4.54368591e-01 -1.75146878e-01]\n",
      "  [-2.22060218e-01 -1.40177265e-01  2.91461945e-01 ...  4.85336920e-03\n",
      "   -3.10238600e-01 -2.96397824e-02]\n",
      "  ...\n",
      "  [-2.29703691e-02 -1.21055986e-03  1.42412022e-01 ...  6.47568554e-02\n",
      "   -4.31201467e-03 -2.78821647e-01]\n",
      "  [ 2.75803953e-01  8.40576053e-01  1.52931497e-01 ...  5.02336286e-02\n",
      "    5.05726457e-01 -2.77489960e-01]\n",
      "  [ 2.11508840e-01 -4.71998453e-02 -1.20662761e+00 ... -4.41415876e-01\n",
      "    2.04590037e-01 -1.76712111e-01]]\n",
      "\n",
      " [[ 5.66130579e-01 -3.57037634e-01 -3.07228088e-01 ... -5.67078233e-01\n",
      "   -9.83624399e-01 -2.33329907e-01]\n",
      "  [ 8.20313022e-02 -9.38051343e-02 -4.45039600e-01 ... -3.20540965e-01\n",
      "   -4.12745118e-01 -3.15113246e-01]\n",
      "  [ 3.91752928e-01  6.16526976e-02 -5.65005898e-01 ...  2.16960177e-01\n",
      "   -2.50617713e-01  4.64713387e-02]\n",
      "  ...\n",
      "  [-3.87890697e-01  9.57835466e-04  2.64991194e-01 ...  7.50278458e-02\n",
      "   -2.61204749e-01 -2.51095593e-01]\n",
      "  [ 7.89126933e-01 -7.65676975e-01 -1.45847559e-01 ... -1.27886462e+00\n",
      "   -4.73632783e-01 -7.96508372e-01]\n",
      "  [ 3.27007085e-01 -6.29500970e-02 -3.98599058e-01 ...  1.11908905e-01\n",
      "    3.12409438e-02  4.24231577e-04]]]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dense_1\n",
      "  Attribute: weight_names = [b'dense_1/kernel:0' b'dense_1/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dense_1/dense_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dense_1/dense_1/bias:0\n",
      "  Dataset: model_weights/dense_1/dense_1/bias:0\n",
      "    Shape: (5,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.22309946 -0.30387652  0.1400367  -0.25352427  0.32266742]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dense_1/dense_1/kernel:0\n",
      "  Dataset: model_weights/dense_1/dense_1/kernel:0\n",
      "    Shape: (640, 5)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.276090</td>\n",
       "      <td>-0.039933</td>\n",
       "      <td>0.109879</td>\n",
       "      <td>-0.305728</td>\n",
       "      <td>-0.257162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141432</td>\n",
       "      <td>0.050986</td>\n",
       "      <td>-0.569477</td>\n",
       "      <td>-0.297230</td>\n",
       "      <td>-0.531911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.452152</td>\n",
       "      <td>-0.465296</td>\n",
       "      <td>0.308620</td>\n",
       "      <td>-0.463383</td>\n",
       "      <td>0.059689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.453587</td>\n",
       "      <td>-0.383473</td>\n",
       "      <td>-0.426326</td>\n",
       "      <td>-0.745240</td>\n",
       "      <td>0.391168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.174907</td>\n",
       "      <td>-0.045816</td>\n",
       "      <td>0.157646</td>\n",
       "      <td>0.074909</td>\n",
       "      <td>-0.356310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>-0.100073</td>\n",
       "      <td>-0.153990</td>\n",
       "      <td>-0.299080</td>\n",
       "      <td>-0.441452</td>\n",
       "      <td>-0.211434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>-0.373313</td>\n",
       "      <td>-0.415656</td>\n",
       "      <td>-0.248971</td>\n",
       "      <td>0.091971</td>\n",
       "      <td>0.017377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>-0.123263</td>\n",
       "      <td>-0.032579</td>\n",
       "      <td>-0.003443</td>\n",
       "      <td>0.201987</td>\n",
       "      <td>-0.190803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>-0.197383</td>\n",
       "      <td>-0.089567</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>-0.255160</td>\n",
       "      <td>-0.223014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>-1.617024</td>\n",
       "      <td>-1.639784</td>\n",
       "      <td>-3.832370</td>\n",
       "      <td>-1.726606</td>\n",
       "      <td>0.726585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4\n",
       "0   -0.276090 -0.039933  0.109879 -0.305728 -0.257162\n",
       "1    0.141432  0.050986 -0.569477 -0.297230 -0.531911\n",
       "2   -0.452152 -0.465296  0.308620 -0.463383  0.059689\n",
       "3   -0.453587 -0.383473 -0.426326 -0.745240  0.391168\n",
       "4   -0.174907 -0.045816  0.157646  0.074909 -0.356310\n",
       "..        ...       ...       ...       ...       ...\n",
       "635 -0.100073 -0.153990 -0.299080 -0.441452 -0.211434\n",
       "636 -0.373313 -0.415656 -0.248971  0.091971  0.017377\n",
       "637 -0.123263 -0.032579 -0.003443  0.201987 -0.190803\n",
       "638 -0.197383 -0.089567  0.119149 -0.255160 -0.223014\n",
       "639 -1.617024 -1.639784 -3.832370 -1.726606  0.726585\n",
       "\n",
       "[640 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_10\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_11\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_12\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_2\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_3\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_4\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_5\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_6\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_7\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_8\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/dropout_9\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/flatten_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/input_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/input_2\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/input_3\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/input_4\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_10\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_11\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_12\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_2\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_3\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_4\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_5\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_6\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_7\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_8\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: model_weights/max_pooling1d_9\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def print_h5_file_structure(file_name):\n",
    "    \"\"\"\n",
    "    Prints the structure of an HDF5 file, including groups, datasets, \n",
    "    and attributes. Datasets are printed as DataFrames if they are 2D.\n",
    "    \n",
    "    Parameters:\n",
    "    file_name (str): The name or path of the HDF5 file to print.\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_structure(name, obj):\n",
    "        \"\"\"\n",
    "        A helper function to recursively print the structure of the HDF5 file.\n",
    "        \n",
    "        Parameters:\n",
    "        name (str): The name of the object in the HDF5 file.\n",
    "        obj (h5py.Group or h5py.Dataset): The object itself (Group or Dataset).\n",
    "        \"\"\"\n",
    "        print(f\"Path: {name}\")\n",
    "        \n",
    "        # Print attributes of the group or dataset\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(f\"  Attribute: {key} = {val}\")\n",
    "        \n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  Dataset: {name}\")\n",
    "            print(f\"    Shape: {obj.shape}\")\n",
    "            print(f\"    Data type: {obj.dtype}\")\n",
    "            # If the dataset is 2D, display it as a DataFrame\n",
    "            if len(obj.shape) == 2:\n",
    "                df = pd.DataFrame(obj[:])\n",
    "                display(df)\n",
    "            else:\n",
    "                # Print dataset contents (avoid large data dumps)\n",
    "                print(\"    Data preview (first 5 elements):\")\n",
    "                print(obj[:5])  # Modify as needed to avoid too large outputs\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Open the file and print the structure\n",
    "    with h5py.File(file_name, 'r') as hdf:\n",
    "        print(f\"File: {file_name}\")\n",
    "        hdf.visititems(print_structure)\n",
    "\n",
    "# Usage\n",
    "filename = '../cnn_weights.hdf5'  # Replace with your actual file name\n",
    "print_h5_file_structure(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../lstm_weights.h5\n",
      "Path: bidirectional_1\n",
      "  Attribute: weight_names = [b'bidirectional_1/forward_lstm_1/kernel:0'\n",
      " b'bidirectional_1/forward_lstm_1/recurrent_kernel:0'\n",
      " b'bidirectional_1/forward_lstm_1/bias:0'\n",
      " b'bidirectional_1/backward_lstm_1/kernel:0'\n",
      " b'bidirectional_1/backward_lstm_1/recurrent_kernel:0'\n",
      " b'bidirectional_1/backward_lstm_1/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/backward_lstm_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/backward_lstm_1/bias:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/backward_lstm_1/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.00347445 -0.03662311  0.0634757   0.04795616 -0.00308285]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/backward_lstm_1/kernel:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/backward_lstm_1/kernel:0\n",
      "    Shape: (640, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.176261</td>\n",
       "      <td>0.274861</td>\n",
       "      <td>-0.138127</td>\n",
       "      <td>-0.277579</td>\n",
       "      <td>-0.442733</td>\n",
       "      <td>-0.070752</td>\n",
       "      <td>-0.383838</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>0.316305</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329187</td>\n",
       "      <td>-0.133575</td>\n",
       "      <td>-0.088088</td>\n",
       "      <td>-0.098104</td>\n",
       "      <td>-0.209627</td>\n",
       "      <td>-0.444325</td>\n",
       "      <td>-0.157627</td>\n",
       "      <td>0.299313</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>-0.342163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.133926</td>\n",
       "      <td>-0.129495</td>\n",
       "      <td>-0.024484</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>-0.135407</td>\n",
       "      <td>-0.105738</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>-0.054728</td>\n",
       "      <td>-0.092370</td>\n",
       "      <td>0.076998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099685</td>\n",
       "      <td>0.223579</td>\n",
       "      <td>0.130118</td>\n",
       "      <td>-0.090007</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>0.065818</td>\n",
       "      <td>0.038292</td>\n",
       "      <td>0.335639</td>\n",
       "      <td>0.115631</td>\n",
       "      <td>-0.060487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.151301</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.206015</td>\n",
       "      <td>0.622240</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>-0.038796</td>\n",
       "      <td>0.374104</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>-0.058564</td>\n",
       "      <td>-0.124634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008343</td>\n",
       "      <td>-0.035854</td>\n",
       "      <td>-0.033563</td>\n",
       "      <td>-0.042072</td>\n",
       "      <td>-0.203498</td>\n",
       "      <td>0.210986</td>\n",
       "      <td>-0.204662</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>0.105798</td>\n",
       "      <td>0.214784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.327346</td>\n",
       "      <td>0.110122</td>\n",
       "      <td>-0.108434</td>\n",
       "      <td>0.024284</td>\n",
       "      <td>0.157974</td>\n",
       "      <td>0.178351</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>-0.027993</td>\n",
       "      <td>-0.189940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052020</td>\n",
       "      <td>-0.213478</td>\n",
       "      <td>-0.230888</td>\n",
       "      <td>-0.083633</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>-0.011167</td>\n",
       "      <td>0.305376</td>\n",
       "      <td>0.127093</td>\n",
       "      <td>-0.265998</td>\n",
       "      <td>-0.022025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.031257</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>0.049443</td>\n",
       "      <td>-0.187058</td>\n",
       "      <td>-0.119258</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.063719</td>\n",
       "      <td>-0.020115</td>\n",
       "      <td>0.019460</td>\n",
       "      <td>0.126755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069328</td>\n",
       "      <td>0.217145</td>\n",
       "      <td>0.097533</td>\n",
       "      <td>0.190884</td>\n",
       "      <td>-0.078464</td>\n",
       "      <td>-0.096607</td>\n",
       "      <td>-0.147019</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>-0.118983</td>\n",
       "      <td>-0.061447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.047387</td>\n",
       "      <td>-0.154403</td>\n",
       "      <td>0.074030</td>\n",
       "      <td>-0.260571</td>\n",
       "      <td>0.079817</td>\n",
       "      <td>-0.085060</td>\n",
       "      <td>0.208358</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>-0.342457</td>\n",
       "      <td>0.081094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>-0.026974</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>-0.034708</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.066202</td>\n",
       "      <td>0.257222</td>\n",
       "      <td>-0.218403</td>\n",
       "      <td>-0.121587</td>\n",
       "      <td>0.217538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>-0.026852</td>\n",
       "      <td>-0.298795</td>\n",
       "      <td>-0.116309</td>\n",
       "      <td>-0.208658</td>\n",
       "      <td>0.124127</td>\n",
       "      <td>0.124169</td>\n",
       "      <td>0.222196</td>\n",
       "      <td>0.227999</td>\n",
       "      <td>0.045528</td>\n",
       "      <td>0.135460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250552</td>\n",
       "      <td>-0.099919</td>\n",
       "      <td>0.187546</td>\n",
       "      <td>0.177491</td>\n",
       "      <td>-0.126667</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>-0.306992</td>\n",
       "      <td>-0.344585</td>\n",
       "      <td>-0.072293</td>\n",
       "      <td>0.130667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0.173579</td>\n",
       "      <td>-0.322781</td>\n",
       "      <td>-0.154780</td>\n",
       "      <td>-0.212935</td>\n",
       "      <td>-0.061978</td>\n",
       "      <td>-0.167670</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>0.293084</td>\n",
       "      <td>-0.009753</td>\n",
       "      <td>-0.013316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227759</td>\n",
       "      <td>-0.078679</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.312806</td>\n",
       "      <td>-0.396999</td>\n",
       "      <td>-0.106194</td>\n",
       "      <td>-0.048532</td>\n",
       "      <td>-0.173334</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.203382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>-0.488247</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.206572</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>-0.072903</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>0.290486</td>\n",
       "      <td>0.178368</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.078929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315948</td>\n",
       "      <td>0.279956</td>\n",
       "      <td>0.138566</td>\n",
       "      <td>0.016883</td>\n",
       "      <td>-0.017648</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>-0.103261</td>\n",
       "      <td>-0.049749</td>\n",
       "      <td>-0.313445</td>\n",
       "      <td>0.219289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.355194</td>\n",
       "      <td>0.435907</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>-0.024537</td>\n",
       "      <td>-0.047651</td>\n",
       "      <td>0.412463</td>\n",
       "      <td>0.250985</td>\n",
       "      <td>-0.011330</td>\n",
       "      <td>0.385752</td>\n",
       "      <td>0.076381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085940</td>\n",
       "      <td>-0.148868</td>\n",
       "      <td>0.224207</td>\n",
       "      <td>-0.529639</td>\n",
       "      <td>-0.056534</td>\n",
       "      <td>-0.116210</td>\n",
       "      <td>-0.560230</td>\n",
       "      <td>-0.464446</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.355117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.176261  0.274861 -0.138127 -0.277579 -0.442733 -0.070752 -0.383838   \n",
       "1   -0.133926 -0.129495 -0.024484  0.031271 -0.135407 -0.105738  0.024323   \n",
       "2   -0.151301  0.079245  0.206015  0.622240  0.086668 -0.038796  0.374104   \n",
       "3    0.038164  0.327346  0.110122 -0.108434  0.024284  0.157974  0.178351   \n",
       "4   -0.031257 -0.004356  0.049443 -0.187058 -0.119258  0.101420  0.063719   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "635  0.047387 -0.154403  0.074030 -0.260571  0.079817 -0.085060  0.208358   \n",
       "636 -0.026852 -0.298795 -0.116309 -0.208658  0.124127  0.124169  0.222196   \n",
       "637  0.173579 -0.322781 -0.154780 -0.212935 -0.061978 -0.167670  0.059913   \n",
       "638 -0.488247  0.016329  0.206572  0.015172 -0.072903  0.057609  0.290486   \n",
       "639  0.355194  0.435907  0.031394 -0.024537 -0.047651  0.412463  0.250985   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.082484  0.316305  0.090950  ...  0.329187 -0.133575 -0.088088   \n",
       "1   -0.054728 -0.092370  0.076998  ...  0.099685  0.223579  0.130118   \n",
       "2    0.029696 -0.058564 -0.124634  ... -0.008343 -0.035854 -0.033563   \n",
       "3    0.057300 -0.027993 -0.189940  ... -0.052020 -0.213478 -0.230888   \n",
       "4   -0.020115  0.019460  0.126755  ...  0.069328  0.217145  0.097533   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "635  0.015132 -0.342457  0.081094  ... -0.018182 -0.026974  0.033516   \n",
       "636  0.227999  0.045528  0.135460  ... -0.250552 -0.099919  0.187546   \n",
       "637  0.293084 -0.009753 -0.013316  ... -0.227759 -0.078679  0.039750   \n",
       "638  0.178368 -0.052784 -0.078929  ... -0.315948  0.279956  0.138566   \n",
       "639 -0.011330  0.385752  0.076381  ... -0.085940 -0.148868  0.224207   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.098104 -0.209627 -0.444325 -0.157627  0.299313  0.018830 -0.342163  \n",
       "1   -0.090007 -0.108264  0.065818  0.038292  0.335639  0.115631 -0.060487  \n",
       "2   -0.042072 -0.203498  0.210986 -0.204662 -0.086783  0.105798  0.214784  \n",
       "3   -0.083633  0.052222 -0.011167  0.305376  0.127093 -0.265998 -0.022025  \n",
       "4    0.190884 -0.078464 -0.096607 -0.147019  0.025084 -0.118983 -0.061447  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "635 -0.034708 -0.090747 -0.066202  0.257222 -0.218403 -0.121587  0.217538  \n",
       "636  0.177491 -0.126667  0.065041 -0.306992 -0.344585 -0.072293  0.130667  \n",
       "637  0.312806 -0.396999 -0.106194 -0.048532 -0.173334  0.004126  0.203382  \n",
       "638  0.016883 -0.017648 -0.088190 -0.103261 -0.049749 -0.313445  0.219289  \n",
       "639 -0.529639 -0.056534 -0.116210 -0.560230 -0.464446  0.214535  0.355117  \n",
       "\n",
       "[640 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/backward_lstm_1/recurrent_kernel:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/backward_lstm_1/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.305631</td>\n",
       "      <td>0.204132</td>\n",
       "      <td>-0.185442</td>\n",
       "      <td>0.055527</td>\n",
       "      <td>0.081019</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>-0.051559</td>\n",
       "      <td>0.047863</td>\n",
       "      <td>0.050306</td>\n",
       "      <td>-0.326718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003206</td>\n",
       "      <td>-0.092284</td>\n",
       "      <td>-0.108016</td>\n",
       "      <td>-0.147737</td>\n",
       "      <td>0.099061</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.075180</td>\n",
       "      <td>0.103878</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.193788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038261</td>\n",
       "      <td>-0.114438</td>\n",
       "      <td>-0.039944</td>\n",
       "      <td>-0.116599</td>\n",
       "      <td>-0.098022</td>\n",
       "      <td>0.262449</td>\n",
       "      <td>-0.332723</td>\n",
       "      <td>-0.230491</td>\n",
       "      <td>-0.098174</td>\n",
       "      <td>0.098799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048367</td>\n",
       "      <td>-0.018213</td>\n",
       "      <td>0.046830</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>-0.060011</td>\n",
       "      <td>0.129983</td>\n",
       "      <td>-0.030488</td>\n",
       "      <td>-0.121900</td>\n",
       "      <td>0.257779</td>\n",
       "      <td>0.099716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034161</td>\n",
       "      <td>-0.054070</td>\n",
       "      <td>0.050930</td>\n",
       "      <td>0.510559</td>\n",
       "      <td>0.221687</td>\n",
       "      <td>0.069678</td>\n",
       "      <td>0.223793</td>\n",
       "      <td>0.241857</td>\n",
       "      <td>0.076807</td>\n",
       "      <td>-0.156749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011682</td>\n",
       "      <td>-0.012180</td>\n",
       "      <td>-0.115144</td>\n",
       "      <td>-0.138178</td>\n",
       "      <td>-0.232222</td>\n",
       "      <td>-0.254472</td>\n",
       "      <td>0.083276</td>\n",
       "      <td>-0.194816</td>\n",
       "      <td>-0.078580</td>\n",
       "      <td>-0.237556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147965</td>\n",
       "      <td>-0.203791</td>\n",
       "      <td>0.369207</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>-0.191885</td>\n",
       "      <td>0.025343</td>\n",
       "      <td>-0.030392</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>-0.106109</td>\n",
       "      <td>0.271563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252275</td>\n",
       "      <td>-0.043887</td>\n",
       "      <td>0.214039</td>\n",
       "      <td>-0.010466</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>-0.091930</td>\n",
       "      <td>0.083880</td>\n",
       "      <td>0.053416</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>0.117877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037347</td>\n",
       "      <td>-0.132170</td>\n",
       "      <td>-0.059116</td>\n",
       "      <td>-0.150844</td>\n",
       "      <td>-0.089640</td>\n",
       "      <td>-0.020224</td>\n",
       "      <td>0.096985</td>\n",
       "      <td>-0.078127</td>\n",
       "      <td>0.075234</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142673</td>\n",
       "      <td>-0.151066</td>\n",
       "      <td>-0.004169</td>\n",
       "      <td>-0.125246</td>\n",
       "      <td>-0.337666</td>\n",
       "      <td>0.282138</td>\n",
       "      <td>-0.113105</td>\n",
       "      <td>0.058543</td>\n",
       "      <td>-0.027831</td>\n",
       "      <td>-0.305158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.093568</td>\n",
       "      <td>-0.093780</td>\n",
       "      <td>-0.177702</td>\n",
       "      <td>0.047456</td>\n",
       "      <td>0.116277</td>\n",
       "      <td>0.104339</td>\n",
       "      <td>0.057605</td>\n",
       "      <td>0.027067</td>\n",
       "      <td>-0.127523</td>\n",
       "      <td>0.038719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100302</td>\n",
       "      <td>-0.005323</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>-0.185166</td>\n",
       "      <td>-0.261832</td>\n",
       "      <td>0.267408</td>\n",
       "      <td>-0.084065</td>\n",
       "      <td>-0.035940</td>\n",
       "      <td>-0.208306</td>\n",
       "      <td>-0.108384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.286472</td>\n",
       "      <td>-0.296534</td>\n",
       "      <td>-0.150108</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.275274</td>\n",
       "      <td>-0.455260</td>\n",
       "      <td>-0.004041</td>\n",
       "      <td>0.025777</td>\n",
       "      <td>-0.208424</td>\n",
       "      <td>-0.122682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.096688</td>\n",
       "      <td>-0.109762</td>\n",
       "      <td>-0.036525</td>\n",
       "      <td>-0.257897</td>\n",
       "      <td>0.280918</td>\n",
       "      <td>-0.080091</td>\n",
       "      <td>0.164395</td>\n",
       "      <td>0.150334</td>\n",
       "      <td>-0.183425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.240842</td>\n",
       "      <td>0.285802</td>\n",
       "      <td>0.325848</td>\n",
       "      <td>0.098690</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>-0.060637</td>\n",
       "      <td>-0.131103</td>\n",
       "      <td>0.253555</td>\n",
       "      <td>-0.214829</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122867</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>-0.211866</td>\n",
       "      <td>0.298930</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.061250</td>\n",
       "      <td>0.029151</td>\n",
       "      <td>0.055663</td>\n",
       "      <td>-0.062915</td>\n",
       "      <td>-0.101543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.335136</td>\n",
       "      <td>-0.288177</td>\n",
       "      <td>-0.324725</td>\n",
       "      <td>-0.052727</td>\n",
       "      <td>-0.131878</td>\n",
       "      <td>-0.132215</td>\n",
       "      <td>-0.006463</td>\n",
       "      <td>0.067573</td>\n",
       "      <td>-0.207231</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138970</td>\n",
       "      <td>-0.372886</td>\n",
       "      <td>-0.140494</td>\n",
       "      <td>-0.042570</td>\n",
       "      <td>-0.191527</td>\n",
       "      <td>0.270585</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>0.228547</td>\n",
       "      <td>-0.041519</td>\n",
       "      <td>-0.074444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.027015</td>\n",
       "      <td>-0.005766</td>\n",
       "      <td>-0.191221</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>0.164569</td>\n",
       "      <td>0.136743</td>\n",
       "      <td>-0.269202</td>\n",
       "      <td>0.143257</td>\n",
       "      <td>-0.135927</td>\n",
       "      <td>0.181120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164104</td>\n",
       "      <td>0.058715</td>\n",
       "      <td>-0.134572</td>\n",
       "      <td>-0.009644</td>\n",
       "      <td>0.131671</td>\n",
       "      <td>-0.174888</td>\n",
       "      <td>0.058629</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.054478</td>\n",
       "      <td>-0.158940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.305631  0.204132 -0.185442  0.055527  0.081019  0.018729 -0.051559   \n",
       "1   0.038261 -0.114438 -0.039944 -0.116599 -0.098022  0.262449 -0.332723   \n",
       "2  -0.034161 -0.054070  0.050930  0.510559  0.221687  0.069678  0.223793   \n",
       "3   0.147965 -0.203791  0.369207  0.001057 -0.191885  0.025343 -0.030392   \n",
       "4  -0.037347 -0.132170 -0.059116 -0.150844 -0.089640 -0.020224  0.096985   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -0.093568 -0.093780 -0.177702  0.047456  0.116277  0.104339  0.057605   \n",
       "60  0.286472 -0.296534 -0.150108  0.141128  0.275274 -0.455260 -0.004041   \n",
       "61 -0.240842  0.285802  0.325848  0.098690  0.013933 -0.060637 -0.131103   \n",
       "62 -0.335136 -0.288177 -0.324725 -0.052727 -0.131878 -0.132215 -0.006463   \n",
       "63 -0.027015 -0.005766 -0.191221  0.014438  0.164569  0.136743 -0.269202   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0   0.047863  0.050306 -0.326718  ... -0.003206 -0.092284 -0.108016 -0.147737   \n",
       "1  -0.230491 -0.098174  0.098799  ...  0.048367 -0.018213  0.046830  0.013271   \n",
       "2   0.241857  0.076807 -0.156749  ... -0.011682 -0.012180 -0.115144 -0.138178   \n",
       "3   0.103774 -0.106109  0.271563  ... -0.252275 -0.043887  0.214039 -0.010466   \n",
       "4  -0.078127  0.075234  0.119093  ...  0.142673 -0.151066 -0.004169 -0.125246   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59  0.027067 -0.127523  0.038719  ...  0.100302 -0.005323  0.058435 -0.185166   \n",
       "60  0.025777 -0.208424 -0.122682  ...  0.026822  0.096688 -0.109762 -0.036525   \n",
       "61  0.253555 -0.214829 -0.001119  ...  0.122867  0.077876 -0.211866  0.298930   \n",
       "62  0.067573 -0.207231  0.009717  ...  0.138970 -0.372886 -0.140494 -0.042570   \n",
       "63  0.143257 -0.135927  0.181120  ...  0.164104  0.058715 -0.134572 -0.009644   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.099061  0.023629  0.075180  0.103878  0.019345  0.193788  \n",
       "1  -0.060011  0.129983 -0.030488 -0.121900  0.257779  0.099716  \n",
       "2  -0.232222 -0.254472  0.083276 -0.194816 -0.078580 -0.237556  \n",
       "3   0.013796 -0.091930  0.083880  0.053416  0.348633  0.117877  \n",
       "4  -0.337666  0.282138 -0.113105  0.058543 -0.027831 -0.305158  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59 -0.261832  0.267408 -0.084065 -0.035940 -0.208306 -0.108384  \n",
       "60 -0.257897  0.280918 -0.080091  0.164395  0.150334 -0.183425  \n",
       "61  0.162344  0.061250  0.029151  0.055663 -0.062915 -0.101543  \n",
       "62 -0.191527  0.270585 -0.002319  0.228547 -0.041519 -0.074444  \n",
       "63  0.131671 -0.174888  0.058629  0.003977  0.054478 -0.158940  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/forward_lstm_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/forward_lstm_1/bias:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/forward_lstm_1/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.005997    0.03030953 -0.04766396 -0.08361317  0.12191213]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/forward_lstm_1/kernel:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/forward_lstm_1/kernel:0\n",
      "    Shape: (640, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.264382</td>\n",
       "      <td>-0.054583</td>\n",
       "      <td>-0.323049</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>0.112854</td>\n",
       "      <td>0.220214</td>\n",
       "      <td>0.167572</td>\n",
       "      <td>-0.142116</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>-0.102972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104365</td>\n",
       "      <td>-0.054899</td>\n",
       "      <td>0.327389</td>\n",
       "      <td>-0.391258</td>\n",
       "      <td>-0.199218</td>\n",
       "      <td>-0.486654</td>\n",
       "      <td>-0.043712</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.031901</td>\n",
       "      <td>0.143782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.044125</td>\n",
       "      <td>0.080291</td>\n",
       "      <td>-0.037357</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>-0.124966</td>\n",
       "      <td>0.045987</td>\n",
       "      <td>-0.036911</td>\n",
       "      <td>-0.108690</td>\n",
       "      <td>-0.059907</td>\n",
       "      <td>-0.048446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044209</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>-0.066683</td>\n",
       "      <td>0.144403</td>\n",
       "      <td>-0.063020</td>\n",
       "      <td>-0.050234</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>-0.007401</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>0.058406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059279</td>\n",
       "      <td>-0.060654</td>\n",
       "      <td>0.152521</td>\n",
       "      <td>-0.229250</td>\n",
       "      <td>0.105592</td>\n",
       "      <td>0.086530</td>\n",
       "      <td>-0.036085</td>\n",
       "      <td>-0.006910</td>\n",
       "      <td>0.163244</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>0.283548</td>\n",
       "      <td>0.122084</td>\n",
       "      <td>0.075678</td>\n",
       "      <td>0.127533</td>\n",
       "      <td>0.160219</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>-0.208804</td>\n",
       "      <td>-0.173313</td>\n",
       "      <td>-0.017991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.191073</td>\n",
       "      <td>0.280201</td>\n",
       "      <td>-0.272156</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.289201</td>\n",
       "      <td>-0.062525</td>\n",
       "      <td>0.018367</td>\n",
       "      <td>0.426867</td>\n",
       "      <td>0.146879</td>\n",
       "      <td>0.039510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491556</td>\n",
       "      <td>-0.067789</td>\n",
       "      <td>-0.459395</td>\n",
       "      <td>0.204874</td>\n",
       "      <td>0.368359</td>\n",
       "      <td>0.133716</td>\n",
       "      <td>-0.190725</td>\n",
       "      <td>0.369772</td>\n",
       "      <td>-0.349057</td>\n",
       "      <td>-0.112769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140071</td>\n",
       "      <td>-0.025367</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>-0.003436</td>\n",
       "      <td>-0.016045</td>\n",
       "      <td>-0.090616</td>\n",
       "      <td>-0.034076</td>\n",
       "      <td>0.125349</td>\n",
       "      <td>-0.071698</td>\n",
       "      <td>0.097485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040127</td>\n",
       "      <td>-0.071609</td>\n",
       "      <td>-0.216653</td>\n",
       "      <td>0.240118</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.131305</td>\n",
       "      <td>-0.045043</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.135570</td>\n",
       "      <td>0.157989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>-0.130068</td>\n",
       "      <td>-0.325616</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>-0.136542</td>\n",
       "      <td>-0.213603</td>\n",
       "      <td>-0.100166</td>\n",
       "      <td>-0.131018</td>\n",
       "      <td>0.007991</td>\n",
       "      <td>-0.080236</td>\n",
       "      <td>-0.093011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.338725</td>\n",
       "      <td>0.088291</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.206231</td>\n",
       "      <td>0.189687</td>\n",
       "      <td>-0.044557</td>\n",
       "      <td>0.167746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.185407</td>\n",
       "      <td>-0.037129</td>\n",
       "      <td>-0.065138</td>\n",
       "      <td>0.064497</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>0.109988</td>\n",
       "      <td>0.185492</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>0.232602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210097</td>\n",
       "      <td>0.445279</td>\n",
       "      <td>0.238209</td>\n",
       "      <td>0.181211</td>\n",
       "      <td>0.059573</td>\n",
       "      <td>0.079614</td>\n",
       "      <td>-0.163431</td>\n",
       "      <td>-0.350161</td>\n",
       "      <td>0.066949</td>\n",
       "      <td>-0.461773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0.386818</td>\n",
       "      <td>0.116434</td>\n",
       "      <td>-0.065599</td>\n",
       "      <td>-0.113679</td>\n",
       "      <td>0.217807</td>\n",
       "      <td>-0.171635</td>\n",
       "      <td>0.070626</td>\n",
       "      <td>0.448219</td>\n",
       "      <td>0.166976</td>\n",
       "      <td>0.086378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117789</td>\n",
       "      <td>0.047197</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.321234</td>\n",
       "      <td>0.198375</td>\n",
       "      <td>0.112645</td>\n",
       "      <td>-0.237072</td>\n",
       "      <td>-0.092736</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>-0.126972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>-0.361067</td>\n",
       "      <td>0.240326</td>\n",
       "      <td>-0.002809</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.211008</td>\n",
       "      <td>-0.056337</td>\n",
       "      <td>-0.012005</td>\n",
       "      <td>0.102136</td>\n",
       "      <td>-0.203844</td>\n",
       "      <td>-0.130101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157953</td>\n",
       "      <td>0.055804</td>\n",
       "      <td>0.104831</td>\n",
       "      <td>-0.076827</td>\n",
       "      <td>-0.045779</td>\n",
       "      <td>0.333802</td>\n",
       "      <td>0.019609</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.033666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.261944</td>\n",
       "      <td>0.289548</td>\n",
       "      <td>0.947501</td>\n",
       "      <td>-0.353931</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.342231</td>\n",
       "      <td>0.054417</td>\n",
       "      <td>0.085849</td>\n",
       "      <td>-0.581160</td>\n",
       "      <td>-0.146134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182634</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>-0.075959</td>\n",
       "      <td>0.073741</td>\n",
       "      <td>0.654971</td>\n",
       "      <td>0.162797</td>\n",
       "      <td>-0.438039</td>\n",
       "      <td>-0.265548</td>\n",
       "      <td>0.059548</td>\n",
       "      <td>0.103680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.264382 -0.054583 -0.323049  0.009637  0.112854  0.220214  0.167572   \n",
       "1   -0.044125  0.080291 -0.037357  0.079038 -0.124966  0.045987 -0.036911   \n",
       "2   -0.059279 -0.060654  0.152521 -0.229250  0.105592  0.086530 -0.036085   \n",
       "3    0.191073  0.280201 -0.272156  0.016402  0.289201 -0.062525  0.018367   \n",
       "4    0.140071 -0.025367  0.266055 -0.003436 -0.016045 -0.090616 -0.034076   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "635 -0.130068 -0.325616 -0.051346 -0.136542 -0.213603 -0.100166 -0.131018   \n",
       "636  0.149952  0.185407 -0.037129 -0.065138  0.064497  0.116293  0.109988   \n",
       "637  0.386818  0.116434 -0.065599 -0.113679  0.217807 -0.171635  0.070626   \n",
       "638 -0.361067  0.240326 -0.002809 -0.284186 -0.211008 -0.056337 -0.012005   \n",
       "639  0.261944  0.289548  0.947501 -0.353931  0.066009  0.342231  0.054417   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0   -0.142116  0.004663 -0.102972  ... -0.104365 -0.054899  0.327389   \n",
       "1   -0.108690 -0.059907 -0.048446  ... -0.044209  0.034950 -0.066683   \n",
       "2   -0.006910  0.163244 -0.006561  ...  0.050798  0.283548  0.122084   \n",
       "3    0.426867  0.146879  0.039510  ...  0.491556 -0.067789 -0.459395   \n",
       "4    0.125349 -0.071698  0.097485  ... -0.040127 -0.071609 -0.216653   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "635  0.007991 -0.080236 -0.093011  ...  0.028572  0.338725  0.088291   \n",
       "636  0.185492  0.027940  0.232602  ... -0.210097  0.445279  0.238209   \n",
       "637  0.448219  0.166976  0.086378  ... -0.117789  0.047197  0.250324   \n",
       "638  0.102136 -0.203844 -0.130101  ... -0.157953  0.055804  0.104831   \n",
       "639  0.085849 -0.581160 -0.146134  ...  0.182634  0.012935 -0.075959   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.391258 -0.199218 -0.486654 -0.043712  0.067470  0.031901  0.143782  \n",
       "1    0.144403 -0.063020 -0.050234 -0.014672 -0.007401  0.089243  0.058406  \n",
       "2    0.075678  0.127533  0.160219  0.006072 -0.208804 -0.173313 -0.017991  \n",
       "3    0.204874  0.368359  0.133716 -0.190725  0.369772 -0.349057 -0.112769  \n",
       "4    0.240118 -0.185805 -0.131305 -0.045043  0.004107  0.135570  0.157989  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "635  0.012720  0.004796  0.056003  0.206231  0.189687 -0.044557  0.167746  \n",
       "636  0.181211  0.059573  0.079614 -0.163431 -0.350161  0.066949 -0.461773  \n",
       "637  0.321234  0.198375  0.112645 -0.237072 -0.092736  0.065052 -0.126972  \n",
       "638 -0.076827 -0.045779  0.333802  0.019609  0.026509  0.064936  0.033666  \n",
       "639  0.073741  0.654971  0.162797 -0.438039 -0.265548  0.059548  0.103680  \n",
       "\n",
       "[640 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_1/bidirectional_1/forward_lstm_1/recurrent_kernel:0\n",
      "  Dataset: bidirectional_1/bidirectional_1/forward_lstm_1/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.106713</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>-0.218579</td>\n",
       "      <td>-0.018231</td>\n",
       "      <td>0.191190</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.089298</td>\n",
       "      <td>-0.193406</td>\n",
       "      <td>-0.171519</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067541</td>\n",
       "      <td>0.159987</td>\n",
       "      <td>0.086627</td>\n",
       "      <td>0.067779</td>\n",
       "      <td>0.080951</td>\n",
       "      <td>-0.174125</td>\n",
       "      <td>-0.221256</td>\n",
       "      <td>-0.200423</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>-0.075432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.121751</td>\n",
       "      <td>0.198418</td>\n",
       "      <td>-0.023927</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>-0.227335</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>0.218031</td>\n",
       "      <td>-0.363888</td>\n",
       "      <td>0.172086</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201063</td>\n",
       "      <td>0.243618</td>\n",
       "      <td>-0.083514</td>\n",
       "      <td>-0.250601</td>\n",
       "      <td>0.011265</td>\n",
       "      <td>-0.130544</td>\n",
       "      <td>0.196023</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>0.198982</td>\n",
       "      <td>0.223771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175091</td>\n",
       "      <td>0.170558</td>\n",
       "      <td>-0.138980</td>\n",
       "      <td>0.261751</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.219276</td>\n",
       "      <td>0.059769</td>\n",
       "      <td>-0.305722</td>\n",
       "      <td>0.106556</td>\n",
       "      <td>0.070285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201868</td>\n",
       "      <td>-0.096347</td>\n",
       "      <td>-0.315511</td>\n",
       "      <td>0.348843</td>\n",
       "      <td>-0.153909</td>\n",
       "      <td>-0.111612</td>\n",
       "      <td>-0.200783</td>\n",
       "      <td>-0.077548</td>\n",
       "      <td>0.036834</td>\n",
       "      <td>0.164799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.224112</td>\n",
       "      <td>0.123407</td>\n",
       "      <td>0.070151</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.049381</td>\n",
       "      <td>-0.171585</td>\n",
       "      <td>-0.486889</td>\n",
       "      <td>-0.094918</td>\n",
       "      <td>0.247746</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.108424</td>\n",
       "      <td>0.372793</td>\n",
       "      <td>0.057968</td>\n",
       "      <td>0.171814</td>\n",
       "      <td>-0.165191</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.069547</td>\n",
       "      <td>0.086304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.313060</td>\n",
       "      <td>-0.015244</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>-0.003220</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>-0.131285</td>\n",
       "      <td>0.224405</td>\n",
       "      <td>-0.011140</td>\n",
       "      <td>-0.340920</td>\n",
       "      <td>0.111596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344225</td>\n",
       "      <td>-0.307746</td>\n",
       "      <td>-0.028885</td>\n",
       "      <td>-0.089013</td>\n",
       "      <td>-0.328478</td>\n",
       "      <td>-0.065514</td>\n",
       "      <td>-0.115686</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.114530</td>\n",
       "      <td>-0.208177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.037931</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.406559</td>\n",
       "      <td>0.131527</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.073407</td>\n",
       "      <td>-0.010798</td>\n",
       "      <td>-0.354922</td>\n",
       "      <td>0.053987</td>\n",
       "      <td>0.245512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293426</td>\n",
       "      <td>-0.109730</td>\n",
       "      <td>0.104683</td>\n",
       "      <td>0.100997</td>\n",
       "      <td>-0.050625</td>\n",
       "      <td>-0.079125</td>\n",
       "      <td>-0.081122</td>\n",
       "      <td>0.035539</td>\n",
       "      <td>0.252934</td>\n",
       "      <td>-0.033826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.152721</td>\n",
       "      <td>-0.094405</td>\n",
       "      <td>0.199455</td>\n",
       "      <td>0.343669</td>\n",
       "      <td>0.194270</td>\n",
       "      <td>0.439190</td>\n",
       "      <td>0.080017</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>-0.157965</td>\n",
       "      <td>0.202827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>-0.137465</td>\n",
       "      <td>0.171617</td>\n",
       "      <td>-0.074284</td>\n",
       "      <td>0.179515</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.368472</td>\n",
       "      <td>-0.079786</td>\n",
       "      <td>0.010796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.060611</td>\n",
       "      <td>0.052528</td>\n",
       "      <td>0.303953</td>\n",
       "      <td>-0.141306</td>\n",
       "      <td>-0.064872</td>\n",
       "      <td>0.175966</td>\n",
       "      <td>-0.054674</td>\n",
       "      <td>-0.108190</td>\n",
       "      <td>0.141461</td>\n",
       "      <td>0.064406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158206</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>-0.158232</td>\n",
       "      <td>-0.219986</td>\n",
       "      <td>-0.001448</td>\n",
       "      <td>0.109494</td>\n",
       "      <td>0.180639</td>\n",
       "      <td>0.082693</td>\n",
       "      <td>-0.008966</td>\n",
       "      <td>0.053282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.151993</td>\n",
       "      <td>0.163935</td>\n",
       "      <td>0.170423</td>\n",
       "      <td>-0.054456</td>\n",
       "      <td>-0.419158</td>\n",
       "      <td>0.076890</td>\n",
       "      <td>0.540243</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>0.296384</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200716</td>\n",
       "      <td>0.118427</td>\n",
       "      <td>-0.028831</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.312032</td>\n",
       "      <td>0.334317</td>\n",
       "      <td>0.178477</td>\n",
       "      <td>0.216233</td>\n",
       "      <td>-0.039406</td>\n",
       "      <td>0.070748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.134633</td>\n",
       "      <td>0.132838</td>\n",
       "      <td>0.347180</td>\n",
       "      <td>-0.027088</td>\n",
       "      <td>-0.174801</td>\n",
       "      <td>-0.288005</td>\n",
       "      <td>0.451368</td>\n",
       "      <td>-0.205381</td>\n",
       "      <td>-0.162266</td>\n",
       "      <td>0.106098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301286</td>\n",
       "      <td>-0.011913</td>\n",
       "      <td>0.238240</td>\n",
       "      <td>-0.406419</td>\n",
       "      <td>-0.096733</td>\n",
       "      <td>-0.125076</td>\n",
       "      <td>0.268832</td>\n",
       "      <td>-0.184180</td>\n",
       "      <td>0.153798</td>\n",
       "      <td>0.049709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.106713  0.018108 -0.218579 -0.018231  0.191190  0.003728  0.089298   \n",
       "1  -0.121751  0.198418 -0.023927  0.028860 -0.227335  0.224609  0.218031   \n",
       "2   0.175091  0.170558 -0.138980  0.261751  0.113400  0.219276  0.059769   \n",
       "3  -0.224112  0.123407  0.070151  0.006794  0.049381 -0.171585 -0.486889   \n",
       "4   0.313060 -0.015244 -0.109108 -0.003220  0.011128 -0.131285  0.224405   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -0.037931  0.003501 -0.406559  0.131527  0.000770  0.073407 -0.010798   \n",
       "60 -0.152721 -0.094405  0.199455  0.343669  0.194270  0.439190  0.080017   \n",
       "61  0.060611  0.052528  0.303953 -0.141306 -0.064872  0.175966 -0.054674   \n",
       "62  0.151993  0.163935  0.170423 -0.054456 -0.419158  0.076890  0.540243   \n",
       "63 -0.134633  0.132838  0.347180 -0.027088 -0.174801 -0.288005  0.451368   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0  -0.193406 -0.171519 -0.054403  ... -0.067541  0.159987  0.086627  0.067779   \n",
       "1  -0.363888  0.172086  0.267911  ... -0.201063  0.243618 -0.083514 -0.250601   \n",
       "2  -0.305722  0.106556  0.070285  ...  0.201868 -0.096347 -0.315511  0.348843   \n",
       "3  -0.094918  0.247746  0.009182  ... -0.236000  0.208000  0.108424  0.372793   \n",
       "4  -0.011140 -0.340920  0.111596  ... -0.344225 -0.307746 -0.028885 -0.089013   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.354922  0.053987  0.245512  ... -0.293426 -0.109730  0.104683  0.100997   \n",
       "60  0.009708 -0.157965  0.202827  ...  0.040079  0.171429 -0.137465  0.171617   \n",
       "61 -0.108190  0.141461  0.064406  ... -0.158206  0.086177 -0.158232 -0.219986   \n",
       "62 -0.015206  0.296384  0.047442  ...  0.200716  0.118427 -0.028831  0.023630   \n",
       "63 -0.205381 -0.162266  0.106098  ...  0.301286 -0.011913  0.238240 -0.406419   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.080951 -0.174125 -0.221256 -0.200423  0.003362 -0.075432  \n",
       "1   0.011265 -0.130544  0.196023  0.037682  0.198982  0.223771  \n",
       "2  -0.153909 -0.111612 -0.200783 -0.077548  0.036834  0.164799  \n",
       "3   0.057968  0.171814 -0.165191  0.003438  0.069547  0.086304  \n",
       "4  -0.328478 -0.065514 -0.115686  0.018429  0.114530 -0.208177  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59 -0.050625 -0.079125 -0.081122  0.035539  0.252934 -0.033826  \n",
       "60 -0.074284  0.179515  0.006801  0.368472 -0.079786  0.010796  \n",
       "61 -0.001448  0.109494  0.180639  0.082693 -0.008966  0.053282  \n",
       "62  0.312032  0.334317  0.178477  0.216233 -0.039406  0.070748  \n",
       "63 -0.096733 -0.125076  0.268832 -0.184180  0.153798  0.049709  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2\n",
      "  Attribute: weight_names = [b'bidirectional_2/forward_lstm_2/kernel:0'\n",
      " b'bidirectional_2/forward_lstm_2/recurrent_kernel:0'\n",
      " b'bidirectional_2/forward_lstm_2/bias:0'\n",
      " b'bidirectional_2/backward_lstm_2/kernel:0'\n",
      " b'bidirectional_2/backward_lstm_2/recurrent_kernel:0'\n",
      " b'bidirectional_2/backward_lstm_2/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/backward_lstm_2\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/backward_lstm_2/bias:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/backward_lstm_2/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.14294435 -0.17009443 -0.17358845 -0.21730964 -0.15473847]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/backward_lstm_2/kernel:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/backward_lstm_2/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.042136</td>\n",
       "      <td>0.170209</td>\n",
       "      <td>-0.116848</td>\n",
       "      <td>0.156102</td>\n",
       "      <td>-0.123191</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.143633</td>\n",
       "      <td>-0.131374</td>\n",
       "      <td>0.091925</td>\n",
       "      <td>-0.084053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272565</td>\n",
       "      <td>0.036772</td>\n",
       "      <td>-0.011831</td>\n",
       "      <td>-0.243795</td>\n",
       "      <td>0.048231</td>\n",
       "      <td>0.183284</td>\n",
       "      <td>-0.025144</td>\n",
       "      <td>-0.001557</td>\n",
       "      <td>0.178474</td>\n",
       "      <td>-0.085103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197366</td>\n",
       "      <td>-0.073016</td>\n",
       "      <td>-0.100333</td>\n",
       "      <td>0.165717</td>\n",
       "      <td>-0.231966</td>\n",
       "      <td>-0.012948</td>\n",
       "      <td>0.182127</td>\n",
       "      <td>-0.189466</td>\n",
       "      <td>-0.094413</td>\n",
       "      <td>0.100834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086531</td>\n",
       "      <td>-0.129929</td>\n",
       "      <td>-0.005105</td>\n",
       "      <td>-0.149544</td>\n",
       "      <td>0.421449</td>\n",
       "      <td>0.048981</td>\n",
       "      <td>-0.285559</td>\n",
       "      <td>-0.119820</td>\n",
       "      <td>-0.304313</td>\n",
       "      <td>-0.103129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.242387</td>\n",
       "      <td>-0.124068</td>\n",
       "      <td>-0.096091</td>\n",
       "      <td>-0.155801</td>\n",
       "      <td>-0.267663</td>\n",
       "      <td>-0.040077</td>\n",
       "      <td>-0.037240</td>\n",
       "      <td>-0.410337</td>\n",
       "      <td>0.326404</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>-0.077504</td>\n",
       "      <td>-0.203179</td>\n",
       "      <td>0.162364</td>\n",
       "      <td>0.052292</td>\n",
       "      <td>0.112479</td>\n",
       "      <td>-0.014697</td>\n",
       "      <td>-0.009259</td>\n",
       "      <td>0.090702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159948</td>\n",
       "      <td>0.118588</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.141161</td>\n",
       "      <td>-0.139110</td>\n",
       "      <td>-0.257860</td>\n",
       "      <td>0.049051</td>\n",
       "      <td>-0.061755</td>\n",
       "      <td>-0.041201</td>\n",
       "      <td>-0.225043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412757</td>\n",
       "      <td>0.063004</td>\n",
       "      <td>-0.307110</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>-0.092939</td>\n",
       "      <td>0.064727</td>\n",
       "      <td>0.277682</td>\n",
       "      <td>0.251598</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.161331</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.071007</td>\n",
       "      <td>-0.051830</td>\n",
       "      <td>0.176151</td>\n",
       "      <td>0.113389</td>\n",
       "      <td>-0.095297</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.049841</td>\n",
       "      <td>0.287859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169983</td>\n",
       "      <td>-0.252843</td>\n",
       "      <td>0.301526</td>\n",
       "      <td>-0.165273</td>\n",
       "      <td>0.160485</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.092747</td>\n",
       "      <td>-0.050598</td>\n",
       "      <td>-0.083087</td>\n",
       "      <td>-0.031058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.344843</td>\n",
       "      <td>-0.306362</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>-0.154014</td>\n",
       "      <td>-0.049146</td>\n",
       "      <td>-0.047699</td>\n",
       "      <td>-0.040165</td>\n",
       "      <td>-0.270961</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.173218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104003</td>\n",
       "      <td>-0.003634</td>\n",
       "      <td>-0.289010</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>0.040285</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.265299</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.178632</td>\n",
       "      <td>0.094738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0.137551</td>\n",
       "      <td>0.079759</td>\n",
       "      <td>-0.045217</td>\n",
       "      <td>0.071614</td>\n",
       "      <td>-0.170071</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>0.070231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075576</td>\n",
       "      <td>0.202567</td>\n",
       "      <td>-0.051388</td>\n",
       "      <td>-0.052495</td>\n",
       "      <td>-0.119300</td>\n",
       "      <td>0.062312</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>0.050451</td>\n",
       "      <td>-0.295505</td>\n",
       "      <td>0.265794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.249911</td>\n",
       "      <td>-0.061425</td>\n",
       "      <td>-0.116440</td>\n",
       "      <td>-0.082400</td>\n",
       "      <td>0.121043</td>\n",
       "      <td>0.106544</td>\n",
       "      <td>0.019271</td>\n",
       "      <td>-0.189427</td>\n",
       "      <td>0.091987</td>\n",
       "      <td>-0.251845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070930</td>\n",
       "      <td>0.069810</td>\n",
       "      <td>-0.089222</td>\n",
       "      <td>0.050080</td>\n",
       "      <td>-0.217516</td>\n",
       "      <td>-0.170910</td>\n",
       "      <td>0.107314</td>\n",
       "      <td>-0.019778</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>-0.011437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.075994</td>\n",
       "      <td>0.055961</td>\n",
       "      <td>0.341774</td>\n",
       "      <td>-0.388812</td>\n",
       "      <td>-0.272047</td>\n",
       "      <td>0.076942</td>\n",
       "      <td>0.033929</td>\n",
       "      <td>0.059169</td>\n",
       "      <td>-0.040977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192189</td>\n",
       "      <td>-0.265258</td>\n",
       "      <td>-0.066365</td>\n",
       "      <td>-0.170803</td>\n",
       "      <td>-0.133528</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.146686</td>\n",
       "      <td>-0.190778</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>0.059303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.085794</td>\n",
       "      <td>-0.120376</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.353345</td>\n",
       "      <td>-0.162531</td>\n",
       "      <td>-0.092106</td>\n",
       "      <td>-0.031237</td>\n",
       "      <td>-0.170996</td>\n",
       "      <td>0.049940</td>\n",
       "      <td>-0.280733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150295</td>\n",
       "      <td>-0.126311</td>\n",
       "      <td>-0.662272</td>\n",
       "      <td>-0.316178</td>\n",
       "      <td>-0.344477</td>\n",
       "      <td>-0.159237</td>\n",
       "      <td>-0.467633</td>\n",
       "      <td>-0.158273</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.278863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.042136  0.170209 -0.116848  0.156102 -0.123191 -0.286598  0.143633   \n",
       "1    0.197366 -0.073016 -0.100333  0.165717 -0.231966 -0.012948  0.182127   \n",
       "2    0.242387 -0.124068 -0.096091 -0.155801 -0.267663 -0.040077 -0.037240   \n",
       "3    0.159948  0.118588  0.104487  0.141161 -0.139110 -0.257860  0.049051   \n",
       "4    0.161331  0.006886  0.071007 -0.051830  0.176151  0.113389 -0.095297   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123  0.344843 -0.306362  0.301486 -0.154014 -0.049146 -0.047699 -0.040165   \n",
       "124  0.149952  0.031278  0.137551  0.079759 -0.045217  0.071614 -0.170071   \n",
       "125  0.249911 -0.061425 -0.116440 -0.082400  0.121043  0.106544  0.019271   \n",
       "126  0.003903 -0.075994  0.055961  0.341774 -0.388812 -0.272047  0.076942   \n",
       "127 -0.085794 -0.120376  0.158318  0.353345 -0.162531 -0.092106 -0.031237   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0   -0.131374  0.091925 -0.084053  ...  0.272565  0.036772 -0.011831   \n",
       "1   -0.189466 -0.094413  0.100834  ...  0.086531 -0.129929 -0.005105   \n",
       "2   -0.410337  0.326404  0.022879  ...  0.200626  0.035721 -0.077504   \n",
       "3   -0.061755 -0.041201 -0.225043  ...  0.412757  0.063004 -0.307110   \n",
       "4   -0.028318 -0.049841  0.287859  ... -0.169983 -0.252843  0.301526   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -0.270961  0.010893  0.173218  ...  0.104003 -0.003634 -0.289010   \n",
       "124  0.126761  0.019442  0.070231  ... -0.075576  0.202567 -0.051388   \n",
       "125 -0.189427  0.091987 -0.251845  ... -0.070930  0.069810 -0.089222   \n",
       "126  0.033929  0.059169 -0.040977  ...  0.192189 -0.265258 -0.066365   \n",
       "127 -0.170996  0.049940 -0.280733  ...  0.150295 -0.126311 -0.662272   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.243795  0.048231  0.183284 -0.025144 -0.001557  0.178474 -0.085103  \n",
       "1   -0.149544  0.421449  0.048981 -0.285559 -0.119820 -0.304313 -0.103129  \n",
       "2   -0.203179  0.162364  0.052292  0.112479 -0.014697 -0.009259  0.090702  \n",
       "3    0.014686  0.005654 -0.092939  0.064727  0.277682  0.251598 -0.012653  \n",
       "4   -0.165273  0.160485 -0.017550 -0.092747 -0.050598 -0.083087 -0.031058  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.039310  0.040285 -0.007376 -0.265299 -0.126506 -0.178632  0.094738  \n",
       "124 -0.052495 -0.119300  0.062312 -0.045166  0.050451 -0.295505  0.265794  \n",
       "125  0.050080 -0.217516 -0.170910  0.107314 -0.019778  0.059598 -0.011437  \n",
       "126 -0.170803 -0.133528  0.008439  0.146686 -0.190778  0.121876  0.059303  \n",
       "127 -0.316178 -0.344477 -0.159237 -0.467633 -0.158273  0.006735  0.278863  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/backward_lstm_2/recurrent_kernel:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/backward_lstm_2/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113365</td>\n",
       "      <td>-0.066477</td>\n",
       "      <td>-0.077145</td>\n",
       "      <td>-0.201650</td>\n",
       "      <td>0.082834</td>\n",
       "      <td>-0.166311</td>\n",
       "      <td>-0.068215</td>\n",
       "      <td>-0.294091</td>\n",
       "      <td>0.119829</td>\n",
       "      <td>0.070533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049885</td>\n",
       "      <td>-0.075069</td>\n",
       "      <td>-0.019896</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>0.075204</td>\n",
       "      <td>0.091049</td>\n",
       "      <td>-0.034732</td>\n",
       "      <td>-0.140702</td>\n",
       "      <td>-0.342115</td>\n",
       "      <td>-0.094959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.380196</td>\n",
       "      <td>0.357901</td>\n",
       "      <td>-0.055198</td>\n",
       "      <td>0.166668</td>\n",
       "      <td>-0.005226</td>\n",
       "      <td>0.134892</td>\n",
       "      <td>-0.043455</td>\n",
       "      <td>0.067565</td>\n",
       "      <td>0.315098</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212590</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>-0.011966</td>\n",
       "      <td>0.012336</td>\n",
       "      <td>-0.009826</td>\n",
       "      <td>-0.071702</td>\n",
       "      <td>-0.323143</td>\n",
       "      <td>-0.312501</td>\n",
       "      <td>0.254333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.311098</td>\n",
       "      <td>0.063064</td>\n",
       "      <td>-0.069232</td>\n",
       "      <td>0.143166</td>\n",
       "      <td>0.015862</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>-0.047470</td>\n",
       "      <td>-0.146724</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246168</td>\n",
       "      <td>0.017631</td>\n",
       "      <td>-0.375594</td>\n",
       "      <td>-0.133066</td>\n",
       "      <td>0.060530</td>\n",
       "      <td>-0.139548</td>\n",
       "      <td>0.096349</td>\n",
       "      <td>-0.047665</td>\n",
       "      <td>-0.161537</td>\n",
       "      <td>-0.306266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.202799</td>\n",
       "      <td>-0.030664</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>-0.157339</td>\n",
       "      <td>0.073748</td>\n",
       "      <td>-0.230914</td>\n",
       "      <td>-0.251336</td>\n",
       "      <td>-0.094574</td>\n",
       "      <td>0.327084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158103</td>\n",
       "      <td>-0.161209</td>\n",
       "      <td>-0.189187</td>\n",
       "      <td>-0.104654</td>\n",
       "      <td>-0.249175</td>\n",
       "      <td>-0.131730</td>\n",
       "      <td>-0.080941</td>\n",
       "      <td>0.013315</td>\n",
       "      <td>-0.201577</td>\n",
       "      <td>-0.342337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.060401</td>\n",
       "      <td>-0.005650</td>\n",
       "      <td>0.136464</td>\n",
       "      <td>-0.091503</td>\n",
       "      <td>-0.108035</td>\n",
       "      <td>-0.068974</td>\n",
       "      <td>-0.022420</td>\n",
       "      <td>0.026119</td>\n",
       "      <td>-0.010982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119232</td>\n",
       "      <td>0.120790</td>\n",
       "      <td>-0.112949</td>\n",
       "      <td>-0.205411</td>\n",
       "      <td>-0.077799</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.240174</td>\n",
       "      <td>0.036912</td>\n",
       "      <td>0.104630</td>\n",
       "      <td>-0.251774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.116511</td>\n",
       "      <td>0.183788</td>\n",
       "      <td>0.197387</td>\n",
       "      <td>-0.045091</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.052487</td>\n",
       "      <td>0.215623</td>\n",
       "      <td>-0.069669</td>\n",
       "      <td>-0.047859</td>\n",
       "      <td>-0.085281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089983</td>\n",
       "      <td>0.106364</td>\n",
       "      <td>0.092656</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.081209</td>\n",
       "      <td>-0.126505</td>\n",
       "      <td>-0.295141</td>\n",
       "      <td>0.046855</td>\n",
       "      <td>-0.043349</td>\n",
       "      <td>0.036824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.227623</td>\n",
       "      <td>-0.288061</td>\n",
       "      <td>-0.093988</td>\n",
       "      <td>0.126217</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.317748</td>\n",
       "      <td>-0.144208</td>\n",
       "      <td>0.147894</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.131196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322843</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>-0.265233</td>\n",
       "      <td>0.108485</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>-0.100095</td>\n",
       "      <td>-0.015699</td>\n",
       "      <td>0.307324</td>\n",
       "      <td>0.092920</td>\n",
       "      <td>-0.028453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.222266</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.085729</td>\n",
       "      <td>0.095647</td>\n",
       "      <td>-0.100594</td>\n",
       "      <td>0.124958</td>\n",
       "      <td>-0.262960</td>\n",
       "      <td>0.115596</td>\n",
       "      <td>0.014947</td>\n",
       "      <td>0.054508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210888</td>\n",
       "      <td>-0.023973</td>\n",
       "      <td>0.156254</td>\n",
       "      <td>-0.154288</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>-0.107917</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>-0.055083</td>\n",
       "      <td>-0.069153</td>\n",
       "      <td>-0.084099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.025605</td>\n",
       "      <td>-0.070984</td>\n",
       "      <td>-0.089823</td>\n",
       "      <td>-0.035468</td>\n",
       "      <td>0.113487</td>\n",
       "      <td>0.200780</td>\n",
       "      <td>-0.047188</td>\n",
       "      <td>0.063011</td>\n",
       "      <td>-0.065633</td>\n",
       "      <td>0.036337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>-0.048554</td>\n",
       "      <td>0.190152</td>\n",
       "      <td>-0.140256</td>\n",
       "      <td>0.034951</td>\n",
       "      <td>0.058343</td>\n",
       "      <td>-0.120864</td>\n",
       "      <td>0.114127</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>-0.030780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.303891</td>\n",
       "      <td>-0.028169</td>\n",
       "      <td>0.212194</td>\n",
       "      <td>-0.024174</td>\n",
       "      <td>-0.117004</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>0.029816</td>\n",
       "      <td>-0.107642</td>\n",
       "      <td>0.121814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025665</td>\n",
       "      <td>-0.186698</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>-0.055104</td>\n",
       "      <td>0.047514</td>\n",
       "      <td>-0.157095</td>\n",
       "      <td>-0.060073</td>\n",
       "      <td>-0.352946</td>\n",
       "      <td>-0.012165</td>\n",
       "      <td>-0.057239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.113365 -0.066477 -0.077145 -0.201650  0.082834 -0.166311 -0.068215   \n",
       "1  -0.380196  0.357901 -0.055198  0.166668 -0.005226  0.134892 -0.043455   \n",
       "2   0.311098  0.063064 -0.069232  0.143166  0.015862  0.037318 -0.047470   \n",
       "3  -0.202799 -0.030664  0.022341  0.041104 -0.157339  0.073748 -0.230914   \n",
       "4   0.024239  0.060401 -0.005650  0.136464 -0.091503 -0.108035 -0.068974   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -0.116511  0.183788  0.197387 -0.045091  0.013585  0.052487  0.215623   \n",
       "60  0.227623 -0.288061 -0.093988  0.126217 -0.024017 -0.317748 -0.144208   \n",
       "61 -0.222266  0.054390  0.085729  0.095647 -0.100594  0.124958 -0.262960   \n",
       "62 -0.025605 -0.070984 -0.089823 -0.035468  0.113487  0.200780 -0.047188   \n",
       "63 -0.303891 -0.028169  0.212194 -0.024174 -0.117004  0.022600  0.157690   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0  -0.294091  0.119829  0.070533  ...  0.049885 -0.075069 -0.019896  0.088682   \n",
       "1   0.067565  0.315098  0.005050  ... -0.212590  0.074597  0.018715 -0.011966   \n",
       "2  -0.146724  0.012923  0.030762  ...  0.246168  0.017631 -0.375594 -0.133066   \n",
       "3  -0.251336 -0.094574  0.327084  ... -0.158103 -0.161209 -0.189187 -0.104654   \n",
       "4  -0.022420  0.026119 -0.010982  ...  0.119232  0.120790 -0.112949 -0.205411   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.069669 -0.047859 -0.085281  ... -0.089983  0.106364  0.092656  0.012881   \n",
       "60  0.147894  0.013761  0.131196  ...  0.322843  0.033061 -0.265233  0.108485   \n",
       "61  0.115596  0.014947  0.054508  ... -0.210888 -0.023973  0.156254 -0.154288   \n",
       "62  0.063011 -0.065633  0.036337  ...  0.006529 -0.048554  0.190152 -0.140256   \n",
       "63  0.029816 -0.107642  0.121814  ... -0.025665 -0.186698  0.010132 -0.055104   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.075204  0.091049 -0.034732 -0.140702 -0.342115 -0.094959  \n",
       "1   0.012336 -0.009826 -0.071702 -0.323143 -0.312501  0.254333  \n",
       "2   0.060530 -0.139548  0.096349 -0.047665 -0.161537 -0.306266  \n",
       "3  -0.249175 -0.131730 -0.080941  0.013315 -0.201577 -0.342337  \n",
       "4  -0.077799  0.025402  0.240174  0.036912  0.104630 -0.251774  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.081209 -0.126505 -0.295141  0.046855 -0.043349  0.036824  \n",
       "60  0.135269 -0.100095 -0.015699  0.307324  0.092920 -0.028453  \n",
       "61  0.063401 -0.107917  0.028588 -0.055083 -0.069153 -0.084099  \n",
       "62  0.034951  0.058343 -0.120864  0.114127 -0.024240 -0.030780  \n",
       "63  0.047514 -0.157095 -0.060073 -0.352946 -0.012165 -0.057239  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/forward_lstm_2\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/forward_lstm_2/bias:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/forward_lstm_2/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.05138684 -0.21712695 -0.11323024 -0.13085037 -0.1863799 ]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/forward_lstm_2/kernel:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/forward_lstm_2/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024975</td>\n",
       "      <td>0.133030</td>\n",
       "      <td>0.113209</td>\n",
       "      <td>-0.194213</td>\n",
       "      <td>-0.047634</td>\n",
       "      <td>-0.239748</td>\n",
       "      <td>-0.007641</td>\n",
       "      <td>-0.053103</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>0.144175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111725</td>\n",
       "      <td>0.188135</td>\n",
       "      <td>0.194893</td>\n",
       "      <td>-0.140648</td>\n",
       "      <td>0.186187</td>\n",
       "      <td>0.070274</td>\n",
       "      <td>0.091296</td>\n",
       "      <td>0.399293</td>\n",
       "      <td>-0.308012</td>\n",
       "      <td>0.113597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.089942</td>\n",
       "      <td>-0.224083</td>\n",
       "      <td>0.130345</td>\n",
       "      <td>0.057904</td>\n",
       "      <td>-0.277020</td>\n",
       "      <td>-0.067282</td>\n",
       "      <td>0.078110</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.349365</td>\n",
       "      <td>-0.049039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016360</td>\n",
       "      <td>0.034429</td>\n",
       "      <td>-0.033821</td>\n",
       "      <td>0.037130</td>\n",
       "      <td>-0.155109</td>\n",
       "      <td>-0.292179</td>\n",
       "      <td>0.146536</td>\n",
       "      <td>0.027929</td>\n",
       "      <td>-0.061192</td>\n",
       "      <td>-0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123757</td>\n",
       "      <td>0.037760</td>\n",
       "      <td>0.346197</td>\n",
       "      <td>-0.132530</td>\n",
       "      <td>-0.048081</td>\n",
       "      <td>0.230548</td>\n",
       "      <td>-0.042153</td>\n",
       "      <td>0.108513</td>\n",
       "      <td>-0.048922</td>\n",
       "      <td>-0.262515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160121</td>\n",
       "      <td>-0.061259</td>\n",
       "      <td>0.028277</td>\n",
       "      <td>0.115331</td>\n",
       "      <td>0.226023</td>\n",
       "      <td>0.120201</td>\n",
       "      <td>0.093484</td>\n",
       "      <td>-0.131236</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>-0.109746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133024</td>\n",
       "      <td>0.147480</td>\n",
       "      <td>0.318444</td>\n",
       "      <td>-0.151254</td>\n",
       "      <td>-0.334475</td>\n",
       "      <td>0.399589</td>\n",
       "      <td>-0.070508</td>\n",
       "      <td>0.210566</td>\n",
       "      <td>-0.087078</td>\n",
       "      <td>-0.206806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016023</td>\n",
       "      <td>0.026735</td>\n",
       "      <td>0.093130</td>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.246428</td>\n",
       "      <td>-0.048671</td>\n",
       "      <td>-0.102705</td>\n",
       "      <td>0.209298</td>\n",
       "      <td>0.065581</td>\n",
       "      <td>0.153263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038764</td>\n",
       "      <td>0.080318</td>\n",
       "      <td>-0.143045</td>\n",
       "      <td>0.108486</td>\n",
       "      <td>-0.108183</td>\n",
       "      <td>-0.311339</td>\n",
       "      <td>0.075991</td>\n",
       "      <td>0.199499</td>\n",
       "      <td>-0.141539</td>\n",
       "      <td>-0.294527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070001</td>\n",
       "      <td>-0.132924</td>\n",
       "      <td>-0.071182</td>\n",
       "      <td>0.226420</td>\n",
       "      <td>-0.188880</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>0.225173</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>-0.137421</td>\n",
       "      <td>-0.377093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.126851</td>\n",
       "      <td>0.037547</td>\n",
       "      <td>0.206855</td>\n",
       "      <td>-0.086044</td>\n",
       "      <td>-0.155662</td>\n",
       "      <td>-0.265007</td>\n",
       "      <td>-0.251630</td>\n",
       "      <td>0.140152</td>\n",
       "      <td>-0.129491</td>\n",
       "      <td>0.066471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122662</td>\n",
       "      <td>-0.157811</td>\n",
       "      <td>0.107495</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>-0.112148</td>\n",
       "      <td>0.079546</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>-0.060452</td>\n",
       "      <td>0.040891</td>\n",
       "      <td>-0.171390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.078542</td>\n",
       "      <td>-0.181011</td>\n",
       "      <td>-0.061873</td>\n",
       "      <td>-0.171350</td>\n",
       "      <td>0.147861</td>\n",
       "      <td>-0.036882</td>\n",
       "      <td>0.088434</td>\n",
       "      <td>0.036849</td>\n",
       "      <td>0.067365</td>\n",
       "      <td>-0.225023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050469</td>\n",
       "      <td>-0.207932</td>\n",
       "      <td>0.052953</td>\n",
       "      <td>-0.102155</td>\n",
       "      <td>0.073901</td>\n",
       "      <td>-0.139324</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>-0.106482</td>\n",
       "      <td>0.129440</td>\n",
       "      <td>-0.191408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.109108</td>\n",
       "      <td>0.125546</td>\n",
       "      <td>-0.060830</td>\n",
       "      <td>-0.199946</td>\n",
       "      <td>0.260895</td>\n",
       "      <td>-0.054074</td>\n",
       "      <td>0.038943</td>\n",
       "      <td>-0.101845</td>\n",
       "      <td>0.117243</td>\n",
       "      <td>0.128794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397628</td>\n",
       "      <td>-0.161954</td>\n",
       "      <td>0.221571</td>\n",
       "      <td>-0.030549</td>\n",
       "      <td>-0.210122</td>\n",
       "      <td>-0.113363</td>\n",
       "      <td>0.095904</td>\n",
       "      <td>-0.265047</td>\n",
       "      <td>0.236934</td>\n",
       "      <td>-0.127351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.029439</td>\n",
       "      <td>0.230634</td>\n",
       "      <td>0.165357</td>\n",
       "      <td>0.020123</td>\n",
       "      <td>0.304789</td>\n",
       "      <td>-0.059882</td>\n",
       "      <td>0.103611</td>\n",
       "      <td>-0.158523</td>\n",
       "      <td>-0.154560</td>\n",
       "      <td>-0.330574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109944</td>\n",
       "      <td>0.237369</td>\n",
       "      <td>-0.196033</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.077119</td>\n",
       "      <td>-0.069263</td>\n",
       "      <td>-0.253682</td>\n",
       "      <td>-0.111009</td>\n",
       "      <td>0.205747</td>\n",
       "      <td>-0.103828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.133331</td>\n",
       "      <td>-0.148391</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.354640</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.493651</td>\n",
       "      <td>-0.321624</td>\n",
       "      <td>0.100349</td>\n",
       "      <td>-0.252114</td>\n",
       "      <td>0.033315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167212</td>\n",
       "      <td>0.275680</td>\n",
       "      <td>0.208272</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>0.172499</td>\n",
       "      <td>0.214241</td>\n",
       "      <td>0.343431</td>\n",
       "      <td>-0.230038</td>\n",
       "      <td>0.435395</td>\n",
       "      <td>0.079481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.024975  0.133030  0.113209 -0.194213 -0.047634 -0.239748 -0.007641   \n",
       "1   -0.089942 -0.224083  0.130345  0.057904 -0.277020 -0.067282  0.078110   \n",
       "2    0.123757  0.037760  0.346197 -0.132530 -0.048081  0.230548 -0.042153   \n",
       "3    0.133024  0.147480  0.318444 -0.151254 -0.334475  0.399589 -0.070508   \n",
       "4    0.038764  0.080318 -0.143045  0.108486 -0.108183 -0.311339  0.075991   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -0.126851  0.037547  0.206855 -0.086044 -0.155662 -0.265007 -0.251630   \n",
       "124 -0.078542 -0.181011 -0.061873 -0.171350  0.147861 -0.036882  0.088434   \n",
       "125  0.109108  0.125546 -0.060830 -0.199946  0.260895 -0.054074  0.038943   \n",
       "126  0.029439  0.230634  0.165357  0.020123  0.304789 -0.059882  0.103611   \n",
       "127 -0.133331 -0.148391  0.146693 -0.354640  0.155328  0.493651 -0.321624   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0   -0.053103 -0.108658  0.144175  ... -0.111725  0.188135  0.194893   \n",
       "1    0.089301  0.349365 -0.049039  ... -0.016360  0.034429 -0.033821   \n",
       "2    0.108513 -0.048922 -0.262515  ... -0.160121 -0.061259  0.028277   \n",
       "3    0.210566 -0.087078 -0.206806  ... -0.016023  0.026735  0.093130   \n",
       "4    0.199499 -0.141539 -0.294527  ...  0.070001 -0.132924 -0.071182   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123  0.140152 -0.129491  0.066471  ... -0.122662 -0.157811  0.107495   \n",
       "124  0.036849  0.067365 -0.225023  ...  0.050469 -0.207932  0.052953   \n",
       "125 -0.101845  0.117243  0.128794  ... -0.397628 -0.161954  0.221571   \n",
       "126 -0.158523 -0.154560 -0.330574  ...  0.109944  0.237369 -0.196033   \n",
       "127  0.100349 -0.252114  0.033315  ... -0.167212  0.275680  0.208272   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.140648  0.186187  0.070274  0.091296  0.399293 -0.308012  0.113597  \n",
       "1    0.037130 -0.155109 -0.292179  0.146536  0.027929 -0.061192 -0.002483  \n",
       "2    0.115331  0.226023  0.120201  0.093484 -0.131236  0.014667 -0.109746  \n",
       "3    0.100343  0.246428 -0.048671 -0.102705  0.209298  0.065581  0.153263  \n",
       "4    0.226420 -0.188880  0.037362  0.225173  0.003970 -0.137421 -0.377093  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123  0.159139 -0.112148  0.079546 -0.066267 -0.060452  0.040891 -0.171390  \n",
       "124 -0.102155  0.073901 -0.139324  0.010033 -0.106482  0.129440 -0.191408  \n",
       "125 -0.030549 -0.210122 -0.113363  0.095904 -0.265047  0.236934 -0.127351  \n",
       "126  0.026450  0.077119 -0.069263 -0.253682 -0.111009  0.205747 -0.103828  \n",
       "127  0.316162  0.172499  0.214241  0.343431 -0.230038  0.435395  0.079481  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_2/bidirectional_2/forward_lstm_2/recurrent_kernel:0\n",
      "  Dataset: bidirectional_2/bidirectional_2/forward_lstm_2/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.035099</td>\n",
       "      <td>-0.031110</td>\n",
       "      <td>0.233514</td>\n",
       "      <td>0.012410</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.225749</td>\n",
       "      <td>0.274569</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>0.089656</td>\n",
       "      <td>0.146937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041841</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>-0.045826</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>-0.099319</td>\n",
       "      <td>-0.031226</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>0.100272</td>\n",
       "      <td>-0.082715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138887</td>\n",
       "      <td>-0.036116</td>\n",
       "      <td>0.061169</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>-0.114118</td>\n",
       "      <td>0.282045</td>\n",
       "      <td>-0.222125</td>\n",
       "      <td>0.103095</td>\n",
       "      <td>0.045059</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015645</td>\n",
       "      <td>0.185352</td>\n",
       "      <td>0.083744</td>\n",
       "      <td>0.140816</td>\n",
       "      <td>-0.036618</td>\n",
       "      <td>-0.017877</td>\n",
       "      <td>-0.214359</td>\n",
       "      <td>0.102728</td>\n",
       "      <td>-0.075208</td>\n",
       "      <td>0.098886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153738</td>\n",
       "      <td>-0.010465</td>\n",
       "      <td>0.216932</td>\n",
       "      <td>-0.143207</td>\n",
       "      <td>-0.093715</td>\n",
       "      <td>-0.148458</td>\n",
       "      <td>-0.227183</td>\n",
       "      <td>-0.187591</td>\n",
       "      <td>0.211575</td>\n",
       "      <td>-0.140586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122733</td>\n",
       "      <td>0.335388</td>\n",
       "      <td>0.348609</td>\n",
       "      <td>-0.285239</td>\n",
       "      <td>-0.159639</td>\n",
       "      <td>-0.305693</td>\n",
       "      <td>0.051954</td>\n",
       "      <td>-0.279135</td>\n",
       "      <td>0.073390</td>\n",
       "      <td>0.209716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.088322</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.319937</td>\n",
       "      <td>-0.054196</td>\n",
       "      <td>0.203983</td>\n",
       "      <td>-0.103856</td>\n",
       "      <td>0.366363</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.139801</td>\n",
       "      <td>-0.049613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048004</td>\n",
       "      <td>-0.030582</td>\n",
       "      <td>-0.037724</td>\n",
       "      <td>0.107289</td>\n",
       "      <td>-0.030478</td>\n",
       "      <td>-0.187677</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>0.033856</td>\n",
       "      <td>0.098626</td>\n",
       "      <td>-0.002794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024955</td>\n",
       "      <td>-0.088151</td>\n",
       "      <td>0.093214</td>\n",
       "      <td>-0.020299</td>\n",
       "      <td>0.188381</td>\n",
       "      <td>0.226231</td>\n",
       "      <td>-0.093904</td>\n",
       "      <td>0.042097</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>-0.268230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045924</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.050789</td>\n",
       "      <td>-0.246851</td>\n",
       "      <td>0.068079</td>\n",
       "      <td>-0.153635</td>\n",
       "      <td>-0.215963</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>-0.070634</td>\n",
       "      <td>0.320079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.117650</td>\n",
       "      <td>-0.119894</td>\n",
       "      <td>0.255848</td>\n",
       "      <td>-0.071213</td>\n",
       "      <td>-0.076178</td>\n",
       "      <td>-0.049558</td>\n",
       "      <td>0.011881</td>\n",
       "      <td>-0.037552</td>\n",
       "      <td>0.160164</td>\n",
       "      <td>-0.051470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244787</td>\n",
       "      <td>0.161697</td>\n",
       "      <td>-0.043779</td>\n",
       "      <td>-0.221532</td>\n",
       "      <td>0.083445</td>\n",
       "      <td>0.115779</td>\n",
       "      <td>0.121235</td>\n",
       "      <td>0.089916</td>\n",
       "      <td>0.208512</td>\n",
       "      <td>0.167114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.103318</td>\n",
       "      <td>0.040635</td>\n",
       "      <td>-0.117987</td>\n",
       "      <td>-0.086772</td>\n",
       "      <td>-0.158204</td>\n",
       "      <td>-0.202309</td>\n",
       "      <td>-0.217190</td>\n",
       "      <td>0.028223</td>\n",
       "      <td>0.036497</td>\n",
       "      <td>-0.344153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135648</td>\n",
       "      <td>-0.152389</td>\n",
       "      <td>-0.007621</td>\n",
       "      <td>-0.274540</td>\n",
       "      <td>-0.108596</td>\n",
       "      <td>-0.046709</td>\n",
       "      <td>0.168988</td>\n",
       "      <td>-0.161154</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>-0.078850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.158170</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.193596</td>\n",
       "      <td>-0.167060</td>\n",
       "      <td>0.039092</td>\n",
       "      <td>0.137082</td>\n",
       "      <td>-0.110320</td>\n",
       "      <td>0.197816</td>\n",
       "      <td>-0.140753</td>\n",
       "      <td>0.313746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139217</td>\n",
       "      <td>0.051657</td>\n",
       "      <td>0.191359</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.204675</td>\n",
       "      <td>-0.151865</td>\n",
       "      <td>0.158307</td>\n",
       "      <td>0.249956</td>\n",
       "      <td>-0.073422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.384893</td>\n",
       "      <td>-0.060396</td>\n",
       "      <td>-0.153622</td>\n",
       "      <td>-0.145076</td>\n",
       "      <td>-0.290340</td>\n",
       "      <td>-0.101263</td>\n",
       "      <td>-0.171086</td>\n",
       "      <td>-0.134486</td>\n",
       "      <td>-0.157486</td>\n",
       "      <td>0.086847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242517</td>\n",
       "      <td>-0.010827</td>\n",
       "      <td>0.058645</td>\n",
       "      <td>-0.151910</td>\n",
       "      <td>-0.032179</td>\n",
       "      <td>0.086268</td>\n",
       "      <td>0.067337</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.226609</td>\n",
       "      <td>-0.126163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.028008</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>-0.140050</td>\n",
       "      <td>-0.036541</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>-0.286725</td>\n",
       "      <td>-0.226001</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.151617</td>\n",
       "      <td>-0.191340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.099649</td>\n",
       "      <td>-0.142113</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.249123</td>\n",
       "      <td>-0.150529</td>\n",
       "      <td>-0.024975</td>\n",
       "      <td>0.024737</td>\n",
       "      <td>0.113092</td>\n",
       "      <td>0.095681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.035099 -0.031110  0.233514  0.012410  0.014747  0.225749  0.274569   \n",
       "1   0.138887 -0.036116  0.061169  0.011559 -0.114118  0.282045 -0.222125   \n",
       "2   0.153738 -0.010465  0.216932 -0.143207 -0.093715 -0.148458 -0.227183   \n",
       "3  -0.088322  0.011526  0.319937 -0.054196  0.203983 -0.103856  0.366363   \n",
       "4   0.024955 -0.088151  0.093214 -0.020299  0.188381  0.226231 -0.093904   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -0.117650 -0.119894  0.255848 -0.071213 -0.076178 -0.049558  0.011881   \n",
       "60 -0.103318  0.040635 -0.117987 -0.086772 -0.158204 -0.202309 -0.217190   \n",
       "61  0.158170  0.094727 -0.193596 -0.167060  0.039092  0.137082 -0.110320   \n",
       "62  0.384893 -0.060396 -0.153622 -0.145076 -0.290340 -0.101263 -0.171086   \n",
       "63  0.028008  0.003550 -0.140050 -0.036541 -0.252597 -0.286725 -0.226001   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0   0.010367  0.089656  0.146937  ... -0.041841  0.179103 -0.045826  0.011702   \n",
       "1   0.103095  0.045059 -0.001000  ...  0.015645  0.185352  0.083744  0.140816   \n",
       "2  -0.187591  0.211575 -0.140586  ... -0.122733  0.335388  0.348609 -0.285239   \n",
       "3   0.085705  0.139801 -0.049613  ... -0.048004 -0.030582 -0.037724  0.107289   \n",
       "4   0.042097 -0.014755 -0.268230  ... -0.045924  0.028950  0.050789 -0.246851   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.037552  0.160164 -0.051470  ... -0.244787  0.161697 -0.043779 -0.221532   \n",
       "60  0.028223  0.036497 -0.344153  ...  0.135648 -0.152389 -0.007621 -0.274540   \n",
       "61  0.197816 -0.140753  0.313746  ...  0.139217  0.051657  0.191359  0.003266   \n",
       "62 -0.134486 -0.157486  0.086847  ... -0.242517 -0.010827  0.058645 -0.151910   \n",
       "63  0.001734  0.151617 -0.191340  ...  0.006398  0.099649 -0.142113  0.020667   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0  -0.099319 -0.031226  0.068555  0.011068  0.100272 -0.082715  \n",
       "1  -0.036618 -0.017877 -0.214359  0.102728 -0.075208  0.098886  \n",
       "2  -0.159639 -0.305693  0.051954 -0.279135  0.073390  0.209716  \n",
       "3  -0.030478 -0.187677  0.099850  0.033856  0.098626 -0.002794  \n",
       "4   0.068079 -0.153635 -0.215963  0.006824 -0.070634  0.320079  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.083445  0.115779  0.121235  0.089916  0.208512  0.167114  \n",
       "60 -0.108596 -0.046709  0.168988 -0.161154  0.090731 -0.078850  \n",
       "61  0.026262  0.204675 -0.151865  0.158307  0.249956 -0.073422  \n",
       "62 -0.032179  0.086268  0.067337 -0.134838 -0.226609 -0.126163  \n",
       "63  0.249123 -0.150529 -0.024975  0.024737  0.113092  0.095681  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3\n",
      "  Attribute: weight_names = [b'bidirectional_3/forward_lstm_3/kernel:0'\n",
      " b'bidirectional_3/forward_lstm_3/recurrent_kernel:0'\n",
      " b'bidirectional_3/forward_lstm_3/bias:0'\n",
      " b'bidirectional_3/backward_lstm_3/kernel:0'\n",
      " b'bidirectional_3/backward_lstm_3/recurrent_kernel:0'\n",
      " b'bidirectional_3/backward_lstm_3/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/backward_lstm_3\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/backward_lstm_3/bias:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/backward_lstm_3/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.16036844 -0.00771113 -0.10482415 -0.01813088 -0.06003541]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/backward_lstm_3/kernel:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/backward_lstm_3/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.085135</td>\n",
       "      <td>-0.046681</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>-0.107529</td>\n",
       "      <td>0.246897</td>\n",
       "      <td>-0.007822</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.281779</td>\n",
       "      <td>-0.068468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083511</td>\n",
       "      <td>0.019486</td>\n",
       "      <td>-0.078209</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>-0.016593</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>-0.133486</td>\n",
       "      <td>0.142125</td>\n",
       "      <td>0.246471</td>\n",
       "      <td>0.154655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.097178</td>\n",
       "      <td>-0.081473</td>\n",
       "      <td>-0.032737</td>\n",
       "      <td>0.050362</td>\n",
       "      <td>-0.095948</td>\n",
       "      <td>0.049626</td>\n",
       "      <td>0.086362</td>\n",
       "      <td>0.049144</td>\n",
       "      <td>0.120538</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137971</td>\n",
       "      <td>-0.254602</td>\n",
       "      <td>-0.190211</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>-0.078193</td>\n",
       "      <td>0.207068</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.112660</td>\n",
       "      <td>0.083373</td>\n",
       "      <td>0.188165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023709</td>\n",
       "      <td>-0.138543</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.075448</td>\n",
       "      <td>-0.249370</td>\n",
       "      <td>-0.061267</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.122334</td>\n",
       "      <td>0.372493</td>\n",
       "      <td>-0.296311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>0.029766</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.083563</td>\n",
       "      <td>0.089157</td>\n",
       "      <td>0.092012</td>\n",
       "      <td>0.044467</td>\n",
       "      <td>0.432378</td>\n",
       "      <td>-0.239435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.045272</td>\n",
       "      <td>0.175539</td>\n",
       "      <td>0.114607</td>\n",
       "      <td>-0.297293</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>-0.002276</td>\n",
       "      <td>-0.025042</td>\n",
       "      <td>0.166556</td>\n",
       "      <td>-0.140345</td>\n",
       "      <td>-0.281906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031503</td>\n",
       "      <td>-0.172643</td>\n",
       "      <td>-0.055652</td>\n",
       "      <td>-0.276435</td>\n",
       "      <td>0.033947</td>\n",
       "      <td>-0.076171</td>\n",
       "      <td>-0.021434</td>\n",
       "      <td>-0.070815</td>\n",
       "      <td>0.090679</td>\n",
       "      <td>0.192598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.077733</td>\n",
       "      <td>0.125576</td>\n",
       "      <td>0.036743</td>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.247904</td>\n",
       "      <td>0.071868</td>\n",
       "      <td>0.116057</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.082458</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050070</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>0.329024</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.045595</td>\n",
       "      <td>0.244723</td>\n",
       "      <td>0.161891</td>\n",
       "      <td>-0.122566</td>\n",
       "      <td>-0.044876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.244055</td>\n",
       "      <td>0.089101</td>\n",
       "      <td>-0.111181</td>\n",
       "      <td>-0.052986</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>-0.172990</td>\n",
       "      <td>0.076697</td>\n",
       "      <td>-0.189708</td>\n",
       "      <td>0.342362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106269</td>\n",
       "      <td>-0.023045</td>\n",
       "      <td>0.027870</td>\n",
       "      <td>0.150987</td>\n",
       "      <td>-0.027376</td>\n",
       "      <td>0.081487</td>\n",
       "      <td>-0.138986</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>-0.118574</td>\n",
       "      <td>0.023703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.143310</td>\n",
       "      <td>0.105628</td>\n",
       "      <td>0.192258</td>\n",
       "      <td>0.094901</td>\n",
       "      <td>-0.352204</td>\n",
       "      <td>-0.028759</td>\n",
       "      <td>0.093649</td>\n",
       "      <td>-0.059757</td>\n",
       "      <td>0.536735</td>\n",
       "      <td>-0.206404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063055</td>\n",
       "      <td>-0.362057</td>\n",
       "      <td>-0.091900</td>\n",
       "      <td>0.482602</td>\n",
       "      <td>0.118941</td>\n",
       "      <td>0.091612</td>\n",
       "      <td>-0.071655</td>\n",
       "      <td>-0.040932</td>\n",
       "      <td>0.082973</td>\n",
       "      <td>-0.183021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.068732</td>\n",
       "      <td>0.145018</td>\n",
       "      <td>0.024637</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.097263</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.116119</td>\n",
       "      <td>0.272619</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>0.178083</td>\n",
       "      <td>0.029807</td>\n",
       "      <td>0.209280</td>\n",
       "      <td>-0.025047</td>\n",
       "      <td>0.086981</td>\n",
       "      <td>-0.078994</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.291751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.006978</td>\n",
       "      <td>-0.140405</td>\n",
       "      <td>-0.225446</td>\n",
       "      <td>0.218067</td>\n",
       "      <td>0.251498</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>-0.018683</td>\n",
       "      <td>0.134754</td>\n",
       "      <td>-0.022180</td>\n",
       "      <td>-0.210034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124344</td>\n",
       "      <td>0.158025</td>\n",
       "      <td>-0.264539</td>\n",
       "      <td>-0.316990</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.208773</td>\n",
       "      <td>-0.168344</td>\n",
       "      <td>-0.379787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.172633</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>-0.262217</td>\n",
       "      <td>-0.208467</td>\n",
       "      <td>-0.050554</td>\n",
       "      <td>-0.026628</td>\n",
       "      <td>-0.164763</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>-0.165692</td>\n",
       "      <td>0.203819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314221</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>0.049746</td>\n",
       "      <td>-0.085171</td>\n",
       "      <td>-0.151213</td>\n",
       "      <td>-0.138606</td>\n",
       "      <td>0.251096</td>\n",
       "      <td>-0.077186</td>\n",
       "      <td>-0.190383</td>\n",
       "      <td>0.122063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.085135 -0.046681  0.136070 -0.107529  0.246897 -0.007822  0.210818   \n",
       "1   -0.097178 -0.081473 -0.032737  0.050362 -0.095948  0.049626  0.086362   \n",
       "2   -0.023709 -0.138543  0.032962  0.075448 -0.249370 -0.061267  0.006738   \n",
       "3   -0.045272  0.175539  0.114607 -0.297293  0.241758 -0.002276 -0.025042   \n",
       "4   -0.077733  0.125576  0.036743  0.042284  0.247904  0.071868  0.116057   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123  0.002170  0.244055  0.089101 -0.111181 -0.052986  0.013929 -0.172990   \n",
       "124 -0.143310  0.105628  0.192258  0.094901 -0.352204 -0.028759  0.093649   \n",
       "125 -0.068732  0.145018  0.024637  0.003131  0.000819  0.097263 -0.002478   \n",
       "126 -0.006978 -0.140405 -0.225446  0.218067  0.251498 -0.016989 -0.018683   \n",
       "127  0.172633 -0.009154 -0.262217 -0.208467 -0.050554 -0.026628 -0.164763   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.116179  0.281779 -0.068468  ...  0.083511  0.019486 -0.078209   \n",
       "1    0.049144  0.120538  0.343200  ...  0.137971 -0.254602 -0.190211   \n",
       "2    0.122334  0.372493 -0.296311  ...  0.063780  0.029766  0.204013   \n",
       "3    0.166556 -0.140345 -0.281906  ...  0.031503 -0.172643 -0.055652   \n",
       "4    0.008947  0.082458  0.007628  ... -0.050070  0.008138  0.034858   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123  0.076697 -0.189708  0.342362  ... -0.106269 -0.023045  0.027870   \n",
       "124 -0.059757  0.536735 -0.206404  ... -0.063055 -0.362057 -0.091900   \n",
       "125  0.116119  0.272619  0.140820  ... -0.037616  0.017397  0.178083   \n",
       "126  0.134754 -0.022180 -0.210034  ... -0.124344  0.158025 -0.264539   \n",
       "127  0.048622 -0.165692  0.203819  ... -0.314221 -0.124004  0.049746   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0    0.089172 -0.016593  0.132571 -0.133486  0.142125  0.246471  0.154655  \n",
       "1    0.676333 -0.078193  0.207068  0.047296  0.112660  0.083373  0.188165  \n",
       "2    0.164948  0.083563  0.089157  0.092012  0.044467  0.432378 -0.239435  \n",
       "3   -0.276435  0.033947 -0.076171 -0.021434 -0.070815  0.090679  0.192598  \n",
       "4    0.329024 -0.000185 -0.045595  0.244723  0.161891 -0.122566 -0.044876  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123  0.150987 -0.027376  0.081487 -0.138986 -0.224712 -0.118574  0.023703  \n",
       "124  0.482602  0.118941  0.091612 -0.071655 -0.040932  0.082973 -0.183021  \n",
       "125  0.029807  0.209280 -0.025047  0.086981 -0.078994  0.012032  0.291751  \n",
       "126 -0.316990  0.013377  0.074495  0.005399  0.208773 -0.168344 -0.379787  \n",
       "127 -0.085171 -0.151213 -0.138606  0.251096 -0.077186 -0.190383  0.122063  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/backward_lstm_3/recurrent_kernel:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/backward_lstm_3/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124308</td>\n",
       "      <td>0.086974</td>\n",
       "      <td>-0.090458</td>\n",
       "      <td>-0.107619</td>\n",
       "      <td>0.314325</td>\n",
       "      <td>0.056245</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.157302</td>\n",
       "      <td>-0.115158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.014577</td>\n",
       "      <td>0.211639</td>\n",
       "      <td>0.137603</td>\n",
       "      <td>-0.147003</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>0.029841</td>\n",
       "      <td>-0.148997</td>\n",
       "      <td>-0.003128</td>\n",
       "      <td>0.172552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038880</td>\n",
       "      <td>-0.140941</td>\n",
       "      <td>0.108899</td>\n",
       "      <td>-0.172624</td>\n",
       "      <td>0.031595</td>\n",
       "      <td>-0.116251</td>\n",
       "      <td>0.095807</td>\n",
       "      <td>-0.382272</td>\n",
       "      <td>0.154341</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185773</td>\n",
       "      <td>0.069572</td>\n",
       "      <td>0.047909</td>\n",
       "      <td>-0.074402</td>\n",
       "      <td>-0.138934</td>\n",
       "      <td>-0.100985</td>\n",
       "      <td>0.034491</td>\n",
       "      <td>0.242313</td>\n",
       "      <td>-0.136853</td>\n",
       "      <td>0.016532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.194996</td>\n",
       "      <td>0.237348</td>\n",
       "      <td>-0.220876</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.090279</td>\n",
       "      <td>-0.076313</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>-0.238289</td>\n",
       "      <td>0.324416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160033</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.173149</td>\n",
       "      <td>-0.267288</td>\n",
       "      <td>-0.154040</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.188055</td>\n",
       "      <td>-0.062823</td>\n",
       "      <td>0.045332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.222119</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>0.069670</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>-0.108831</td>\n",
       "      <td>0.180199</td>\n",
       "      <td>-0.028605</td>\n",
       "      <td>-0.230777</td>\n",
       "      <td>-0.315343</td>\n",
       "      <td>-0.326131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160055</td>\n",
       "      <td>0.041025</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.198408</td>\n",
       "      <td>0.299450</td>\n",
       "      <td>-0.137862</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>-0.077254</td>\n",
       "      <td>0.041601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130358</td>\n",
       "      <td>-0.141394</td>\n",
       "      <td>-0.224914</td>\n",
       "      <td>-0.047923</td>\n",
       "      <td>-0.204998</td>\n",
       "      <td>-0.097924</td>\n",
       "      <td>0.007577</td>\n",
       "      <td>-0.010690</td>\n",
       "      <td>0.058244</td>\n",
       "      <td>0.229999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>-0.062364</td>\n",
       "      <td>-0.033435</td>\n",
       "      <td>-0.020849</td>\n",
       "      <td>-0.177659</td>\n",
       "      <td>-0.091116</td>\n",
       "      <td>-0.073101</td>\n",
       "      <td>-0.017473</td>\n",
       "      <td>0.071115</td>\n",
       "      <td>-0.091266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.105572</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>-0.044693</td>\n",
       "      <td>0.126916</td>\n",
       "      <td>0.029424</td>\n",
       "      <td>-0.080149</td>\n",
       "      <td>-0.079714</td>\n",
       "      <td>-0.158894</td>\n",
       "      <td>-0.292383</td>\n",
       "      <td>0.045989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217761</td>\n",
       "      <td>0.150273</td>\n",
       "      <td>-0.248242</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.141228</td>\n",
       "      <td>-0.101578</td>\n",
       "      <td>-0.002910</td>\n",
       "      <td>-0.181335</td>\n",
       "      <td>-0.091119</td>\n",
       "      <td>-0.179449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.193924</td>\n",
       "      <td>-0.101751</td>\n",
       "      <td>-0.117092</td>\n",
       "      <td>-0.103470</td>\n",
       "      <td>-0.007358</td>\n",
       "      <td>-0.008026</td>\n",
       "      <td>0.184764</td>\n",
       "      <td>-0.014131</td>\n",
       "      <td>-0.148573</td>\n",
       "      <td>-0.053540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078289</td>\n",
       "      <td>-0.069213</td>\n",
       "      <td>-0.029987</td>\n",
       "      <td>-0.212693</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>-0.074173</td>\n",
       "      <td>-0.142656</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.143729</td>\n",
       "      <td>-0.092268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.149296</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>-0.241708</td>\n",
       "      <td>-0.082159</td>\n",
       "      <td>-0.186520</td>\n",
       "      <td>0.044674</td>\n",
       "      <td>-0.024491</td>\n",
       "      <td>-0.231633</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>-0.005067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184763</td>\n",
       "      <td>-0.127048</td>\n",
       "      <td>0.095312</td>\n",
       "      <td>-0.153005</td>\n",
       "      <td>-0.085545</td>\n",
       "      <td>-0.102025</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>-0.029303</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>-0.133526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.287758</td>\n",
       "      <td>-0.108305</td>\n",
       "      <td>-0.055451</td>\n",
       "      <td>-0.053209</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.013135</td>\n",
       "      <td>-0.006156</td>\n",
       "      <td>-0.051577</td>\n",
       "      <td>0.051742</td>\n",
       "      <td>-0.233224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033363</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>-0.103323</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>-0.094813</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>-0.240478</td>\n",
       "      <td>0.128867</td>\n",
       "      <td>0.062753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.241041</td>\n",
       "      <td>-0.017330</td>\n",
       "      <td>-0.021863</td>\n",
       "      <td>0.096630</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>-0.124145</td>\n",
       "      <td>-0.099347</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>0.247207</td>\n",
       "      <td>-0.339497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029414</td>\n",
       "      <td>-0.071231</td>\n",
       "      <td>-0.139869</td>\n",
       "      <td>0.236249</td>\n",
       "      <td>-0.126511</td>\n",
       "      <td>-0.103734</td>\n",
       "      <td>-0.095334</td>\n",
       "      <td>-0.377598</td>\n",
       "      <td>0.221249</td>\n",
       "      <td>0.084855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.124308  0.086974 -0.090458 -0.107619  0.314325  0.056245 -0.144048   \n",
       "1   0.038880 -0.140941  0.108899 -0.172624  0.031595 -0.116251  0.095807   \n",
       "2   0.194996  0.237348 -0.220876 -0.044215 -0.039439  0.090279 -0.076313   \n",
       "3  -0.222119 -0.067699  0.069670 -0.004807 -0.108831  0.180199 -0.028605   \n",
       "4   0.130358 -0.141394 -0.224914 -0.047923 -0.204998 -0.097924  0.007577   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59  0.105572 -0.159613 -0.044693  0.126916  0.029424 -0.080149 -0.079714   \n",
       "60  0.193924 -0.101751 -0.117092 -0.103470 -0.007358 -0.008026  0.184764   \n",
       "61  0.149296  0.091667 -0.241708 -0.082159 -0.186520  0.044674 -0.024491   \n",
       "62 -0.287758 -0.108305 -0.055451 -0.053209  0.011879 -0.013135 -0.006156   \n",
       "63 -0.241041 -0.017330 -0.021863  0.096630  0.293641 -0.124145 -0.099347   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0   0.007950  0.157302 -0.115158  ...  0.017226  0.014577  0.211639  0.137603   \n",
       "1  -0.382272  0.154341  0.003431  ...  0.185773  0.069572  0.047909 -0.074402   \n",
       "2   0.026470 -0.238289  0.324416  ...  0.160033  0.003587  0.173149 -0.267288   \n",
       "3  -0.230777 -0.315343 -0.326131  ... -0.160055  0.041025  0.001069 -0.198408   \n",
       "4  -0.010690  0.058244  0.229999  ...  0.005151 -0.062364 -0.033435 -0.020849   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.158894 -0.292383  0.045989  ... -0.217761  0.150273 -0.248242  0.079820   \n",
       "60 -0.014131 -0.148573 -0.053540  ... -0.078289 -0.069213 -0.029987 -0.212693   \n",
       "61 -0.231633  0.020410 -0.005067  ...  0.184763 -0.127048  0.095312 -0.153005   \n",
       "62 -0.051577  0.051742 -0.233224  ... -0.033363  0.079616 -0.103323  0.084326   \n",
       "63  0.059537  0.247207 -0.339497  ... -0.029414 -0.071231 -0.139869  0.236249   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0  -0.147003  0.047510  0.029841 -0.148997 -0.003128  0.172552  \n",
       "1  -0.138934 -0.100985  0.034491  0.242313 -0.136853  0.016532  \n",
       "2  -0.154040  0.103433  0.006557  0.188055 -0.062823  0.045332  \n",
       "3   0.299450 -0.137862 -0.022557  0.022256 -0.077254  0.041601  \n",
       "4  -0.177659 -0.091116 -0.073101 -0.017473  0.071115 -0.091266  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.141228 -0.101578 -0.002910 -0.181335 -0.091119 -0.179449  \n",
       "60  0.055335 -0.074173 -0.142656  0.005489  0.143729 -0.092268  \n",
       "61 -0.085545 -0.102025  0.015987 -0.029303  0.003732 -0.133526  \n",
       "62  0.099864 -0.094813  0.017762 -0.240478  0.128867  0.062753  \n",
       "63 -0.126511 -0.103734 -0.095334 -0.377598  0.221249  0.084855  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/forward_lstm_3\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/forward_lstm_3/bias:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/forward_lstm_3/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.15477538 -0.09759965 -0.16769296 -0.1305205  -0.07051392]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/forward_lstm_3/kernel:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/forward_lstm_3/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.134612</td>\n",
       "      <td>0.056407</td>\n",
       "      <td>-0.082898</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>-0.051608</td>\n",
       "      <td>0.099017</td>\n",
       "      <td>-0.274379</td>\n",
       "      <td>0.243285</td>\n",
       "      <td>0.130072</td>\n",
       "      <td>0.026076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202603</td>\n",
       "      <td>0.022797</td>\n",
       "      <td>-0.112155</td>\n",
       "      <td>-0.062935</td>\n",
       "      <td>-0.151319</td>\n",
       "      <td>0.065939</td>\n",
       "      <td>0.073030</td>\n",
       "      <td>-0.070422</td>\n",
       "      <td>-0.029417</td>\n",
       "      <td>0.052458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066414</td>\n",
       "      <td>0.094908</td>\n",
       "      <td>0.136350</td>\n",
       "      <td>0.203971</td>\n",
       "      <td>0.108737</td>\n",
       "      <td>0.066935</td>\n",
       "      <td>0.230353</td>\n",
       "      <td>0.076333</td>\n",
       "      <td>0.108366</td>\n",
       "      <td>0.191218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135184</td>\n",
       "      <td>-0.142503</td>\n",
       "      <td>0.235187</td>\n",
       "      <td>0.037222</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.061193</td>\n",
       "      <td>-0.215222</td>\n",
       "      <td>0.150785</td>\n",
       "      <td>0.117388</td>\n",
       "      <td>0.159299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130551</td>\n",
       "      <td>-0.180827</td>\n",
       "      <td>0.050183</td>\n",
       "      <td>-0.171762</td>\n",
       "      <td>-0.120342</td>\n",
       "      <td>0.057523</td>\n",
       "      <td>0.027171</td>\n",
       "      <td>-0.128033</td>\n",
       "      <td>0.108196</td>\n",
       "      <td>-0.061645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075554</td>\n",
       "      <td>-0.121337</td>\n",
       "      <td>-0.129285</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.250783</td>\n",
       "      <td>0.072747</td>\n",
       "      <td>0.175070</td>\n",
       "      <td>0.246062</td>\n",
       "      <td>-0.030211</td>\n",
       "      <td>0.174731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.064239</td>\n",
       "      <td>-0.177977</td>\n",
       "      <td>-0.226028</td>\n",
       "      <td>-0.355610</td>\n",
       "      <td>0.078369</td>\n",
       "      <td>-0.337150</td>\n",
       "      <td>-0.235491</td>\n",
       "      <td>-0.040418</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>-0.107036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.041720</td>\n",
       "      <td>-0.080199</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>-0.073034</td>\n",
       "      <td>-0.143581</td>\n",
       "      <td>0.092598</td>\n",
       "      <td>-0.269900</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>0.136137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.064399</td>\n",
       "      <td>0.181781</td>\n",
       "      <td>0.250810</td>\n",
       "      <td>0.140925</td>\n",
       "      <td>0.017127</td>\n",
       "      <td>-0.121552</td>\n",
       "      <td>0.090522</td>\n",
       "      <td>0.273506</td>\n",
       "      <td>0.124553</td>\n",
       "      <td>-0.194718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.129853</td>\n",
       "      <td>-0.134559</td>\n",
       "      <td>-0.248831</td>\n",
       "      <td>0.133402</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.069124</td>\n",
       "      <td>-0.124245</td>\n",
       "      <td>-0.227234</td>\n",
       "      <td>0.068826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.043860</td>\n",
       "      <td>0.021313</td>\n",
       "      <td>-0.165159</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>0.136142</td>\n",
       "      <td>-0.135858</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.298980</td>\n",
       "      <td>-0.105327</td>\n",
       "      <td>0.072325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061755</td>\n",
       "      <td>-0.205641</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.023615</td>\n",
       "      <td>0.131070</td>\n",
       "      <td>0.068373</td>\n",
       "      <td>-0.310271</td>\n",
       "      <td>0.057275</td>\n",
       "      <td>0.290508</td>\n",
       "      <td>-0.151332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.089316</td>\n",
       "      <td>0.056230</td>\n",
       "      <td>-0.230033</td>\n",
       "      <td>-0.127358</td>\n",
       "      <td>-0.018183</td>\n",
       "      <td>-0.096293</td>\n",
       "      <td>0.146451</td>\n",
       "      <td>0.285101</td>\n",
       "      <td>-0.283959</td>\n",
       "      <td>0.162778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137251</td>\n",
       "      <td>0.157042</td>\n",
       "      <td>-0.229946</td>\n",
       "      <td>-0.119604</td>\n",
       "      <td>-0.052995</td>\n",
       "      <td>-0.098761</td>\n",
       "      <td>0.207926</td>\n",
       "      <td>-0.114760</td>\n",
       "      <td>-0.015044</td>\n",
       "      <td>0.260709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.139910</td>\n",
       "      <td>0.135283</td>\n",
       "      <td>0.192862</td>\n",
       "      <td>0.101883</td>\n",
       "      <td>0.044932</td>\n",
       "      <td>0.282517</td>\n",
       "      <td>0.108221</td>\n",
       "      <td>-0.082533</td>\n",
       "      <td>-0.048013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141728</td>\n",
       "      <td>-0.313387</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.021075</td>\n",
       "      <td>-0.022068</td>\n",
       "      <td>-0.103322</td>\n",
       "      <td>0.103809</td>\n",
       "      <td>0.109021</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.073609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.082476</td>\n",
       "      <td>0.201343</td>\n",
       "      <td>0.328106</td>\n",
       "      <td>0.218037</td>\n",
       "      <td>-0.128013</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.232063</td>\n",
       "      <td>0.331759</td>\n",
       "      <td>0.190049</td>\n",
       "      <td>-0.012664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040582</td>\n",
       "      <td>0.191327</td>\n",
       "      <td>0.079531</td>\n",
       "      <td>-0.122830</td>\n",
       "      <td>-0.076857</td>\n",
       "      <td>-0.337964</td>\n",
       "      <td>0.237197</td>\n",
       "      <td>-0.077586</td>\n",
       "      <td>-0.220245</td>\n",
       "      <td>-0.228504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.150459</td>\n",
       "      <td>-0.252190</td>\n",
       "      <td>-0.086064</td>\n",
       "      <td>0.175125</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.285259</td>\n",
       "      <td>-0.035191</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>-0.122438</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117411</td>\n",
       "      <td>-0.220096</td>\n",
       "      <td>0.346604</td>\n",
       "      <td>-0.210281</td>\n",
       "      <td>-0.150661</td>\n",
       "      <td>-0.339688</td>\n",
       "      <td>-0.140629</td>\n",
       "      <td>0.047864</td>\n",
       "      <td>0.040435</td>\n",
       "      <td>-0.148952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.134612  0.056407 -0.082898  0.009715 -0.051608  0.099017 -0.274379   \n",
       "1    0.066414  0.094908  0.136350  0.203971  0.108737  0.066935  0.230353   \n",
       "2    0.130551 -0.180827  0.050183 -0.171762 -0.120342  0.057523  0.027171   \n",
       "3   -0.064239 -0.177977 -0.226028 -0.355610  0.078369 -0.337150 -0.235491   \n",
       "4   -0.064399  0.181781  0.250810  0.140925  0.017127 -0.121552  0.090522   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -0.043860  0.021313 -0.165159  0.015754  0.136142 -0.135858 -0.000041   \n",
       "124  0.089316  0.056230 -0.230033 -0.127358 -0.018183 -0.096293  0.146451   \n",
       "125  0.089021  0.139910  0.135283  0.192862  0.101883  0.044932  0.282517   \n",
       "126 -0.082476  0.201343  0.328106  0.218037 -0.128013  0.003395  0.232063   \n",
       "127 -0.150459 -0.252190 -0.086064  0.175125  0.013103  0.285259 -0.035191   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.243285  0.130072  0.026076  ...  0.202603  0.022797 -0.112155   \n",
       "1    0.076333  0.108366  0.191218  ...  0.135184 -0.142503  0.235187   \n",
       "2   -0.128033  0.108196 -0.061645  ...  0.075554 -0.121337 -0.129285   \n",
       "3   -0.040418  0.100642 -0.107036  ...  0.019744  0.041720 -0.080199   \n",
       "4    0.273506  0.124553 -0.194718  ...  0.013867  0.129853 -0.134559   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -0.298980 -0.105327  0.072325  ... -0.061755 -0.205641  0.012595   \n",
       "124  0.285101 -0.283959  0.162778  ...  0.137251  0.157042 -0.229946   \n",
       "125  0.108221 -0.082533 -0.048013  ... -0.141728 -0.313387  0.010660   \n",
       "126  0.331759  0.190049 -0.012664  ... -0.040582  0.191327  0.079531   \n",
       "127 -0.000441 -0.122438  0.015395  ... -0.117411 -0.220096  0.346604   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.062935 -0.151319  0.065939  0.073030 -0.070422 -0.029417  0.052458  \n",
       "1    0.037222  0.016487  0.061193 -0.215222  0.150785  0.117388  0.159299  \n",
       "2    0.005917  0.250783  0.072747  0.175070  0.246062 -0.030211  0.174731  \n",
       "3    0.007911 -0.073034 -0.143581  0.092598 -0.269900  0.037491  0.136137  \n",
       "4   -0.248831  0.133402  0.196279  0.069124 -0.124245 -0.227234  0.068826  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123  0.023615  0.131070  0.068373 -0.310271  0.057275  0.290508 -0.151332  \n",
       "124 -0.119604 -0.052995 -0.098761  0.207926 -0.114760 -0.015044  0.260709  \n",
       "125  0.021075 -0.022068 -0.103322  0.103809  0.109021 -0.000157 -0.073609  \n",
       "126 -0.122830 -0.076857 -0.337964  0.237197 -0.077586 -0.220245 -0.228504  \n",
       "127 -0.210281 -0.150661 -0.339688 -0.140629  0.047864  0.040435 -0.148952  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_3/bidirectional_3/forward_lstm_3/recurrent_kernel:0\n",
      "  Dataset: bidirectional_3/bidirectional_3/forward_lstm_3/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031481</td>\n",
       "      <td>-0.021631</td>\n",
       "      <td>0.100270</td>\n",
       "      <td>0.114965</td>\n",
       "      <td>-0.077648</td>\n",
       "      <td>-0.103820</td>\n",
       "      <td>0.068718</td>\n",
       "      <td>-0.074477</td>\n",
       "      <td>-0.074990</td>\n",
       "      <td>0.007708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080989</td>\n",
       "      <td>0.084078</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.041754</td>\n",
       "      <td>0.302278</td>\n",
       "      <td>0.186655</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>-0.141339</td>\n",
       "      <td>0.141562</td>\n",
       "      <td>0.089630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210107</td>\n",
       "      <td>0.058962</td>\n",
       "      <td>0.097892</td>\n",
       "      <td>0.089209</td>\n",
       "      <td>0.157710</td>\n",
       "      <td>-0.170486</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>-0.227578</td>\n",
       "      <td>0.118850</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137297</td>\n",
       "      <td>0.118718</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.182603</td>\n",
       "      <td>0.022258</td>\n",
       "      <td>0.246268</td>\n",
       "      <td>0.199229</td>\n",
       "      <td>0.092940</td>\n",
       "      <td>-0.021476</td>\n",
       "      <td>-0.014463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.067638</td>\n",
       "      <td>0.211772</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.083941</td>\n",
       "      <td>-0.088633</td>\n",
       "      <td>-0.006231</td>\n",
       "      <td>-0.206879</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>-0.026170</td>\n",
       "      <td>-0.072990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.026022</td>\n",
       "      <td>0.137032</td>\n",
       "      <td>-0.286792</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>-0.151407</td>\n",
       "      <td>-0.075582</td>\n",
       "      <td>-0.091366</td>\n",
       "      <td>-0.225424</td>\n",
       "      <td>-0.072036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.180632</td>\n",
       "      <td>-0.066030</td>\n",
       "      <td>-0.290731</td>\n",
       "      <td>-0.112521</td>\n",
       "      <td>-0.344814</td>\n",
       "      <td>0.253105</td>\n",
       "      <td>-0.020736</td>\n",
       "      <td>0.259084</td>\n",
       "      <td>-0.114516</td>\n",
       "      <td>-0.213762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.022756</td>\n",
       "      <td>-0.179425</td>\n",
       "      <td>-0.212190</td>\n",
       "      <td>-0.098093</td>\n",
       "      <td>-0.079063</td>\n",
       "      <td>0.174933</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>-0.019662</td>\n",
       "      <td>-0.089217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126187</td>\n",
       "      <td>-0.218417</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>-0.033126</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>-0.114291</td>\n",
       "      <td>-0.107455</td>\n",
       "      <td>-0.309055</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112074</td>\n",
       "      <td>0.040487</td>\n",
       "      <td>0.194875</td>\n",
       "      <td>0.267601</td>\n",
       "      <td>-0.133905</td>\n",
       "      <td>-0.186346</td>\n",
       "      <td>-0.183532</td>\n",
       "      <td>0.166995</td>\n",
       "      <td>-0.194556</td>\n",
       "      <td>-0.047969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.022708</td>\n",
       "      <td>-0.053251</td>\n",
       "      <td>0.144750</td>\n",
       "      <td>-0.069154</td>\n",
       "      <td>-0.093301</td>\n",
       "      <td>-0.124777</td>\n",
       "      <td>0.018288</td>\n",
       "      <td>-0.038925</td>\n",
       "      <td>-0.247402</td>\n",
       "      <td>0.014861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252316</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.036414</td>\n",
       "      <td>-0.068891</td>\n",
       "      <td>0.266699</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>-0.021033</td>\n",
       "      <td>-0.266447</td>\n",
       "      <td>-0.219047</td>\n",
       "      <td>0.173565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.211578</td>\n",
       "      <td>-0.131974</td>\n",
       "      <td>0.124445</td>\n",
       "      <td>0.046472</td>\n",
       "      <td>0.044176</td>\n",
       "      <td>0.211107</td>\n",
       "      <td>-0.034266</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>0.053535</td>\n",
       "      <td>0.077949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243877</td>\n",
       "      <td>-0.054713</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.029032</td>\n",
       "      <td>-0.181769</td>\n",
       "      <td>0.176591</td>\n",
       "      <td>0.037921</td>\n",
       "      <td>-0.098080</td>\n",
       "      <td>0.005278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.165055</td>\n",
       "      <td>-0.252595</td>\n",
       "      <td>0.041697</td>\n",
       "      <td>-0.658540</td>\n",
       "      <td>-0.091195</td>\n",
       "      <td>-0.087570</td>\n",
       "      <td>-0.032003</td>\n",
       "      <td>-0.160294</td>\n",
       "      <td>0.246209</td>\n",
       "      <td>-0.006043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081809</td>\n",
       "      <td>0.138331</td>\n",
       "      <td>-0.157933</td>\n",
       "      <td>0.318879</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.129769</td>\n",
       "      <td>-0.027524</td>\n",
       "      <td>-0.010344</td>\n",
       "      <td>-0.279912</td>\n",
       "      <td>0.139181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.010477</td>\n",
       "      <td>-0.117148</td>\n",
       "      <td>-0.163990</td>\n",
       "      <td>-0.221459</td>\n",
       "      <td>-0.166841</td>\n",
       "      <td>-0.157097</td>\n",
       "      <td>-0.149035</td>\n",
       "      <td>-0.025241</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.104927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.087750</td>\n",
       "      <td>-0.175800</td>\n",
       "      <td>-0.022967</td>\n",
       "      <td>0.158706</td>\n",
       "      <td>0.039364</td>\n",
       "      <td>-0.046582</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.145294</td>\n",
       "      <td>0.091767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.096633</td>\n",
       "      <td>0.283532</td>\n",
       "      <td>-0.142188</td>\n",
       "      <td>0.180519</td>\n",
       "      <td>0.181840</td>\n",
       "      <td>0.199783</td>\n",
       "      <td>-0.142790</td>\n",
       "      <td>0.265275</td>\n",
       "      <td>-0.301043</td>\n",
       "      <td>-0.098898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222347</td>\n",
       "      <td>-0.063410</td>\n",
       "      <td>-0.230083</td>\n",
       "      <td>-0.147409</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.135451</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>-0.316431</td>\n",
       "      <td>0.200046</td>\n",
       "      <td>-0.080280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.031481 -0.021631  0.100270  0.114965 -0.077648 -0.103820  0.068718   \n",
       "1   0.210107  0.058962  0.097892  0.089209  0.157710 -0.170486  0.022020   \n",
       "2  -0.067638  0.211772  0.014580  0.083941 -0.088633 -0.006231 -0.206879   \n",
       "3  -0.180632 -0.066030 -0.290731 -0.112521 -0.344814  0.253105 -0.020736   \n",
       "4   0.126187 -0.218417  0.114517 -0.033126  0.073786 -0.114291 -0.107455   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59  0.022708 -0.053251  0.144750 -0.069154 -0.093301 -0.124777  0.018288   \n",
       "60  0.211578 -0.131974  0.124445  0.046472  0.044176  0.211107 -0.034266   \n",
       "61 -0.165055 -0.252595  0.041697 -0.658540 -0.091195 -0.087570 -0.032003   \n",
       "62 -0.010477 -0.117148 -0.163990 -0.221459 -0.166841 -0.157097 -0.149035   \n",
       "63 -0.096633  0.283532 -0.142188  0.180519  0.181840  0.199783 -0.142790   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0  -0.074477 -0.074990  0.007708  ...  0.080989  0.084078  0.045007  0.041754   \n",
       "1  -0.227578  0.118850  0.227374  ... -0.137297  0.118718  0.035703  0.182603   \n",
       "2   0.136614 -0.026170 -0.072990  ... -0.009949  0.026022  0.137032 -0.286792   \n",
       "3   0.259084 -0.114516 -0.213762  ...  0.052982  0.022756 -0.179425 -0.212190   \n",
       "4  -0.309055  0.040890 -0.010489  ... -0.112074  0.040487  0.194875  0.267601   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.038925 -0.247402  0.014861  ...  0.252316  0.149058  0.036414 -0.068891   \n",
       "60  0.022061  0.053535  0.077949  ...  0.243877 -0.054713 -0.000256  0.117904   \n",
       "61 -0.160294  0.246209 -0.006043  ... -0.081809  0.138331 -0.157933  0.318879   \n",
       "62 -0.025241  0.016824  0.104927  ...  0.003028  0.087750 -0.175800 -0.022967   \n",
       "63  0.265275 -0.301043 -0.098898  ...  0.222347 -0.063410 -0.230083 -0.147409   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.302278  0.186655  0.036700 -0.141339  0.141562  0.089630  \n",
       "1   0.022258  0.246268  0.199229  0.092940 -0.021476 -0.014463  \n",
       "2   0.031857 -0.151407 -0.075582 -0.091366 -0.225424 -0.072036  \n",
       "3  -0.098093 -0.079063  0.174933  0.006908 -0.019662 -0.089217  \n",
       "4  -0.133905 -0.186346 -0.183532  0.166995 -0.194556 -0.047969  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.266699 -0.012770 -0.021033 -0.266447 -0.219047  0.173565  \n",
       "60  0.029032 -0.181769  0.176591  0.037921 -0.098080  0.005278  \n",
       "61  0.083131  0.129769 -0.027524 -0.010344 -0.279912  0.139181  \n",
       "62  0.158706  0.039364 -0.046582  0.051092  0.145294  0.091767  \n",
       "63  0.040330  0.135451  0.004627 -0.316431  0.200046 -0.080280  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4\n",
      "  Attribute: weight_names = [b'bidirectional_4/forward_lstm_4/kernel:0'\n",
      " b'bidirectional_4/forward_lstm_4/recurrent_kernel:0'\n",
      " b'bidirectional_4/forward_lstm_4/bias:0'\n",
      " b'bidirectional_4/backward_lstm_4/kernel:0'\n",
      " b'bidirectional_4/backward_lstm_4/recurrent_kernel:0'\n",
      " b'bidirectional_4/backward_lstm_4/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/backward_lstm_4\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/backward_lstm_4/bias:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/backward_lstm_4/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[ 0.01465087 -0.02014678 -0.0900076  -0.06453342  0.01354897]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/backward_lstm_4/kernel:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/backward_lstm_4/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.047644</td>\n",
       "      <td>-0.125982</td>\n",
       "      <td>0.132228</td>\n",
       "      <td>0.098934</td>\n",
       "      <td>0.517208</td>\n",
       "      <td>-0.115175</td>\n",
       "      <td>0.033162</td>\n",
       "      <td>0.342592</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>0.090376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.165437</td>\n",
       "      <td>0.089094</td>\n",
       "      <td>-0.023723</td>\n",
       "      <td>-0.044668</td>\n",
       "      <td>0.171876</td>\n",
       "      <td>-0.093167</td>\n",
       "      <td>-0.031630</td>\n",
       "      <td>0.053813</td>\n",
       "      <td>0.054715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122294</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>-0.049558</td>\n",
       "      <td>0.109938</td>\n",
       "      <td>-0.240678</td>\n",
       "      <td>0.140721</td>\n",
       "      <td>0.086801</td>\n",
       "      <td>-0.100558</td>\n",
       "      <td>-0.067576</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070380</td>\n",
       "      <td>-0.081313</td>\n",
       "      <td>0.062711</td>\n",
       "      <td>0.122146</td>\n",
       "      <td>0.076759</td>\n",
       "      <td>-0.140905</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>0.093474</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.034722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.099994</td>\n",
       "      <td>-0.107546</td>\n",
       "      <td>0.073991</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.081432</td>\n",
       "      <td>-0.052890</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>0.078905</td>\n",
       "      <td>-0.055442</td>\n",
       "      <td>-0.113498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>0.004109</td>\n",
       "      <td>-0.046574</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>0.061568</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.029106</td>\n",
       "      <td>-0.156661</td>\n",
       "      <td>0.116470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.113428</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.081122</td>\n",
       "      <td>0.083528</td>\n",
       "      <td>0.014534</td>\n",
       "      <td>-0.028083</td>\n",
       "      <td>0.054139</td>\n",
       "      <td>-0.015547</td>\n",
       "      <td>-0.009178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113684</td>\n",
       "      <td>-0.051941</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>-0.010702</td>\n",
       "      <td>-0.098933</td>\n",
       "      <td>0.149399</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>0.160042</td>\n",
       "      <td>-0.217373</td>\n",
       "      <td>-0.031073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134939</td>\n",
       "      <td>0.280770</td>\n",
       "      <td>-0.366106</td>\n",
       "      <td>0.068151</td>\n",
       "      <td>-0.404717</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>-0.025724</td>\n",
       "      <td>-0.091989</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>-0.008695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>-0.037279</td>\n",
       "      <td>-0.087297</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>0.134308</td>\n",
       "      <td>-0.214404</td>\n",
       "      <td>0.052955</td>\n",
       "      <td>-0.091578</td>\n",
       "      <td>-0.182329</td>\n",
       "      <td>0.203806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.016081</td>\n",
       "      <td>-0.051410</td>\n",
       "      <td>-0.153160</td>\n",
       "      <td>-0.037569</td>\n",
       "      <td>0.054691</td>\n",
       "      <td>-0.031119</td>\n",
       "      <td>-0.127892</td>\n",
       "      <td>-0.193450</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>-0.071157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016168</td>\n",
       "      <td>-0.104944</td>\n",
       "      <td>-0.105720</td>\n",
       "      <td>-0.151277</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>-0.127658</td>\n",
       "      <td>0.035449</td>\n",
       "      <td>0.068151</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>-0.054712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.081808</td>\n",
       "      <td>0.039032</td>\n",
       "      <td>-0.131297</td>\n",
       "      <td>-0.057932</td>\n",
       "      <td>-0.169240</td>\n",
       "      <td>-0.204971</td>\n",
       "      <td>-0.080842</td>\n",
       "      <td>-0.029824</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>-0.176379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>0.144006</td>\n",
       "      <td>-0.035194</td>\n",
       "      <td>0.183910</td>\n",
       "      <td>0.074845</td>\n",
       "      <td>0.147389</td>\n",
       "      <td>0.065612</td>\n",
       "      <td>-0.125831</td>\n",
       "      <td>0.061305</td>\n",
       "      <td>0.004907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.020185</td>\n",
       "      <td>-0.041423</td>\n",
       "      <td>-0.078312</td>\n",
       "      <td>0.184718</td>\n",
       "      <td>-0.308170</td>\n",
       "      <td>-0.137117</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>-0.244528</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>0.079544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193331</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.131922</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>-0.048619</td>\n",
       "      <td>0.184136</td>\n",
       "      <td>-0.051470</td>\n",
       "      <td>-0.087394</td>\n",
       "      <td>0.052052</td>\n",
       "      <td>-0.081841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.173540</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>-0.305066</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>-0.270864</td>\n",
       "      <td>-0.165664</td>\n",
       "      <td>-0.144225</td>\n",
       "      <td>-0.145411</td>\n",
       "      <td>-0.213226</td>\n",
       "      <td>-0.138347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.075950</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.014915</td>\n",
       "      <td>0.125868</td>\n",
       "      <td>-0.083455</td>\n",
       "      <td>0.034524</td>\n",
       "      <td>0.103110</td>\n",
       "      <td>-0.068261</td>\n",
       "      <td>0.254833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.093716</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.051538</td>\n",
       "      <td>-0.200720</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>-0.206069</td>\n",
       "      <td>-0.065394</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.035970</td>\n",
       "      <td>-0.036648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.011554</td>\n",
       "      <td>-0.009518</td>\n",
       "      <td>-0.013494</td>\n",
       "      <td>-0.055687</td>\n",
       "      <td>0.145838</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>-0.106271</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>-0.044024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.047644 -0.125982  0.132228  0.098934  0.517208 -0.115175  0.033162   \n",
       "1    0.122294  0.250096 -0.049558  0.109938 -0.240678  0.140721  0.086801   \n",
       "2   -0.099994 -0.107546  0.073991  0.006980  0.081432 -0.052890  0.010103   \n",
       "3    0.113428 -0.054895  0.031829  0.081122  0.083528  0.014534 -0.028083   \n",
       "4    0.134939  0.280770 -0.366106  0.068151 -0.404717  0.206655 -0.025724   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123  0.016081 -0.051410 -0.153160 -0.037569  0.054691 -0.031119 -0.127892   \n",
       "124 -0.081808  0.039032 -0.131297 -0.057932 -0.169240 -0.204971 -0.080842   \n",
       "125  0.020185 -0.041423 -0.078312  0.184718 -0.308170 -0.137117  0.020480   \n",
       "126 -0.173540  0.007699 -0.305066 -0.023169 -0.270864 -0.165664 -0.144225   \n",
       "127  0.093716  0.099900  0.051538 -0.200720  0.032242 -0.206069 -0.065394   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.342592  0.043476  0.090376  ...  0.023191  0.165437  0.089094   \n",
       "1   -0.100558 -0.067576  0.014522  ... -0.070380 -0.081313  0.062711   \n",
       "2    0.078905 -0.055442 -0.113498  ... -0.023047  0.004109 -0.046574   \n",
       "3    0.054139 -0.015547 -0.009178  ...  0.113684 -0.051941  0.014607   \n",
       "4   -0.091989  0.035900 -0.008695  ...  0.068622 -0.037279 -0.087297   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -0.193450  0.016678 -0.071157  ... -0.016168 -0.104944 -0.105720   \n",
       "124 -0.029824  0.003298 -0.176379  ... -0.019119  0.144006 -0.035194   \n",
       "125 -0.244528  0.050755  0.079544  ... -0.193331  0.003323  0.131922   \n",
       "126 -0.145411 -0.213226 -0.138347  ...  0.004042  0.075950  0.051402   \n",
       "127  0.004370 -0.035970 -0.036648  ... -0.065430 -0.011554 -0.009518   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.023723 -0.044668  0.171876 -0.093167 -0.031630  0.053813  0.054715  \n",
       "1    0.122146  0.076759 -0.140905  0.041533  0.093474  0.005867  0.034722  \n",
       "2    0.041266  0.061568  0.017051  0.042659  0.029106 -0.156661  0.116470  \n",
       "3   -0.010702 -0.098933  0.149399 -0.025511  0.160042 -0.217373 -0.031073  \n",
       "4    0.100883  0.134308 -0.214404  0.052955 -0.091578 -0.182329  0.203806  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.151277 -0.009688 -0.127658  0.035449  0.068151  0.005770 -0.054712  \n",
       "124  0.183910  0.074845  0.147389  0.065612 -0.125831  0.061305  0.004907  \n",
       "125  0.006997 -0.048619  0.184136 -0.051470 -0.087394  0.052052 -0.081841  \n",
       "126  0.014915  0.125868 -0.083455  0.034524  0.103110 -0.068261  0.254833  \n",
       "127 -0.013494 -0.055687  0.145838  0.143683 -0.106271  0.002114 -0.044024  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/backward_lstm_4/recurrent_kernel:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/backward_lstm_4/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.142622</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.131680</td>\n",
       "      <td>0.202945</td>\n",
       "      <td>0.137113</td>\n",
       "      <td>-0.030785</td>\n",
       "      <td>0.082234</td>\n",
       "      <td>-0.044816</td>\n",
       "      <td>0.195509</td>\n",
       "      <td>0.198822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153796</td>\n",
       "      <td>0.029279</td>\n",
       "      <td>0.176938</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>0.322822</td>\n",
       "      <td>-0.050977</td>\n",
       "      <td>0.169948</td>\n",
       "      <td>0.145483</td>\n",
       "      <td>0.173731</td>\n",
       "      <td>0.119881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038779</td>\n",
       "      <td>-0.134798</td>\n",
       "      <td>0.055877</td>\n",
       "      <td>0.083277</td>\n",
       "      <td>0.072690</td>\n",
       "      <td>0.043544</td>\n",
       "      <td>-0.117449</td>\n",
       "      <td>0.153602</td>\n",
       "      <td>-0.092133</td>\n",
       "      <td>-0.065356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094458</td>\n",
       "      <td>-0.064888</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.105584</td>\n",
       "      <td>0.007912</td>\n",
       "      <td>-0.017583</td>\n",
       "      <td>0.150103</td>\n",
       "      <td>0.187494</td>\n",
       "      <td>0.094717</td>\n",
       "      <td>0.196928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.122544</td>\n",
       "      <td>0.035201</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>-0.040650</td>\n",
       "      <td>-0.120290</td>\n",
       "      <td>0.073694</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>-0.174601</td>\n",
       "      <td>-0.059580</td>\n",
       "      <td>0.098836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022457</td>\n",
       "      <td>-0.118144</td>\n",
       "      <td>-0.161508</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>0.056309</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.063662</td>\n",
       "      <td>-0.011297</td>\n",
       "      <td>-0.018910</td>\n",
       "      <td>0.024338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088914</td>\n",
       "      <td>0.074625</td>\n",
       "      <td>0.088841</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>-0.172115</td>\n",
       "      <td>0.135606</td>\n",
       "      <td>-0.110560</td>\n",
       "      <td>0.079651</td>\n",
       "      <td>0.027482</td>\n",
       "      <td>0.029898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106184</td>\n",
       "      <td>-0.024410</td>\n",
       "      <td>-0.101828</td>\n",
       "      <td>0.050533</td>\n",
       "      <td>0.027974</td>\n",
       "      <td>0.140551</td>\n",
       "      <td>0.107339</td>\n",
       "      <td>0.114450</td>\n",
       "      <td>0.205837</td>\n",
       "      <td>-0.056669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>-0.099020</td>\n",
       "      <td>-0.134275</td>\n",
       "      <td>-0.031521</td>\n",
       "      <td>0.016226</td>\n",
       "      <td>-0.061788</td>\n",
       "      <td>0.048977</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>0.118581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060931</td>\n",
       "      <td>0.168927</td>\n",
       "      <td>-0.038273</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>0.076148</td>\n",
       "      <td>0.023317</td>\n",
       "      <td>0.053585</td>\n",
       "      <td>0.071423</td>\n",
       "      <td>0.079626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.152843</td>\n",
       "      <td>-0.093828</td>\n",
       "      <td>-0.120368</td>\n",
       "      <td>-0.273912</td>\n",
       "      <td>-0.115347</td>\n",
       "      <td>-0.086795</td>\n",
       "      <td>-0.169636</td>\n",
       "      <td>-0.203056</td>\n",
       "      <td>-0.077584</td>\n",
       "      <td>-0.154658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107423</td>\n",
       "      <td>-0.201025</td>\n",
       "      <td>-0.066646</td>\n",
       "      <td>-0.256360</td>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.240509</td>\n",
       "      <td>-0.137851</td>\n",
       "      <td>0.027515</td>\n",
       "      <td>-0.068888</td>\n",
       "      <td>0.068423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.077403</td>\n",
       "      <td>0.065672</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>-0.101490</td>\n",
       "      <td>0.041033</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>-0.078649</td>\n",
       "      <td>-0.026414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098520</td>\n",
       "      <td>0.114273</td>\n",
       "      <td>0.184877</td>\n",
       "      <td>-0.130020</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.088117</td>\n",
       "      <td>0.114250</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.109470</td>\n",
       "      <td>0.008935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.104031</td>\n",
       "      <td>-0.023641</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>-0.232301</td>\n",
       "      <td>-0.180605</td>\n",
       "      <td>-0.121414</td>\n",
       "      <td>-0.018657</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>0.067115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169163</td>\n",
       "      <td>0.174867</td>\n",
       "      <td>-0.005872</td>\n",
       "      <td>0.099331</td>\n",
       "      <td>0.147962</td>\n",
       "      <td>0.087592</td>\n",
       "      <td>0.244892</td>\n",
       "      <td>-0.113337</td>\n",
       "      <td>-0.015527</td>\n",
       "      <td>0.062783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.005697</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>-0.314845</td>\n",
       "      <td>-0.015241</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>-0.098080</td>\n",
       "      <td>-0.077902</td>\n",
       "      <td>-0.014975</td>\n",
       "      <td>-0.109561</td>\n",
       "      <td>-0.173558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134116</td>\n",
       "      <td>-0.024434</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>-0.030362</td>\n",
       "      <td>-0.087109</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>-0.130337</td>\n",
       "      <td>-0.099962</td>\n",
       "      <td>-0.070054</td>\n",
       "      <td>-0.195602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.032125</td>\n",
       "      <td>-0.030648</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>-0.105420</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>-0.162370</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>-0.142516</td>\n",
       "      <td>-0.049654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.086613</td>\n",
       "      <td>-0.060831</td>\n",
       "      <td>-0.006459</td>\n",
       "      <td>-0.044634</td>\n",
       "      <td>0.028834</td>\n",
       "      <td>-0.215964</td>\n",
       "      <td>-0.109679</td>\n",
       "      <td>-0.069613</td>\n",
       "      <td>0.080234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.142622  0.005489  0.131680  0.202945  0.137113 -0.030785  0.082234   \n",
       "1  -0.038779 -0.134798  0.055877  0.083277  0.072690  0.043544 -0.117449   \n",
       "2   0.122544  0.035201  0.150328 -0.040650 -0.120290  0.073694  0.056425   \n",
       "3   0.088914  0.074625  0.088841  0.022142 -0.172115  0.135606 -0.110560   \n",
       "4   0.003873  0.039865 -0.099020 -0.134275 -0.031521  0.016226 -0.061788   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -0.152843 -0.093828 -0.120368 -0.273912 -0.115347 -0.086795 -0.169636   \n",
       "60  0.077403  0.065672 -0.007690 -0.101490  0.041033  0.003652  0.166125   \n",
       "61 -0.104031 -0.023641  0.073810 -0.232301 -0.180605 -0.121414 -0.018657   \n",
       "62 -0.005697  0.026345 -0.314845 -0.015241  0.020062 -0.098080 -0.077902   \n",
       "63 -0.032125 -0.030648  0.005570 -0.105420  0.078756 -0.162370 -0.014397   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0  -0.044816  0.195509  0.198822  ...  0.153796  0.029279  0.176938 -0.025496   \n",
       "1   0.153602 -0.092133 -0.065356  ...  0.094458 -0.064888  0.056519  0.105584   \n",
       "2  -0.174601 -0.059580  0.098836  ...  0.022457 -0.118144 -0.161508  0.126900   \n",
       "3   0.079651  0.027482  0.029898  ...  0.106184 -0.024410 -0.101828  0.050533   \n",
       "4   0.048977  0.042855  0.118581  ...  0.060931  0.168927 -0.038273  0.004506   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59 -0.203056 -0.077584 -0.154658  ... -0.107423 -0.201025 -0.066646 -0.256360   \n",
       "60  0.001487 -0.078649 -0.026414  ...  0.098520  0.114273  0.184877 -0.130020   \n",
       "61  0.083326  0.020668  0.067115  ...  0.169163  0.174867 -0.005872  0.099331   \n",
       "62 -0.014975 -0.109561 -0.173558  ... -0.134116 -0.024434  0.052100 -0.030362   \n",
       "63  0.036731 -0.142516 -0.049654  ...  0.005073  0.086613 -0.060831 -0.006459   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.322822 -0.050977  0.169948  0.145483  0.173731  0.119881  \n",
       "1   0.007912 -0.017583  0.150103  0.187494  0.094717  0.196928  \n",
       "2   0.056309  0.028116  0.063662 -0.011297 -0.018910  0.024338  \n",
       "3   0.027974  0.140551  0.107339  0.114450  0.205837 -0.056669  \n",
       "4  -0.000119  0.076148  0.023317  0.053585  0.071423  0.079626  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59 -0.009757 -0.240509 -0.137851  0.027515 -0.068888  0.068423  \n",
       "60  0.002713  0.088117  0.114250  0.000907  0.109470  0.008935  \n",
       "61  0.147962  0.087592  0.244892 -0.113337 -0.015527  0.062783  \n",
       "62 -0.087109  0.004719 -0.130337 -0.099962 -0.070054 -0.195602  \n",
       "63 -0.044634  0.028834 -0.215964 -0.109679 -0.069613  0.080234  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/forward_lstm_4\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/forward_lstm_4/bias:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/forward_lstm_4/bias:0\n",
      "    Shape: (256,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.03309577 -0.15174292 -0.12376173 -0.15004267  0.01883818]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/forward_lstm_4/kernel:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/forward_lstm_4/kernel:0\n",
      "    Shape: (128, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039854</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>0.279487</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>0.106008</td>\n",
       "      <td>0.382421</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.080694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090990</td>\n",
       "      <td>0.079089</td>\n",
       "      <td>0.035011</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>0.067453</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>-0.125295</td>\n",
       "      <td>-0.072662</td>\n",
       "      <td>-0.011904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015984</td>\n",
       "      <td>-0.060795</td>\n",
       "      <td>0.148113</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>0.201932</td>\n",
       "      <td>0.038565</td>\n",
       "      <td>0.152215</td>\n",
       "      <td>0.174304</td>\n",
       "      <td>0.136421</td>\n",
       "      <td>-0.116924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>0.056162</td>\n",
       "      <td>0.046566</td>\n",
       "      <td>-0.049303</td>\n",
       "      <td>-0.011361</td>\n",
       "      <td>-0.054555</td>\n",
       "      <td>-0.003828</td>\n",
       "      <td>-0.021726</td>\n",
       "      <td>0.049651</td>\n",
       "      <td>-0.076605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.057855</td>\n",
       "      <td>-0.158905</td>\n",
       "      <td>0.092555</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>0.064708</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>-0.029701</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>-0.022324</td>\n",
       "      <td>-0.118542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049019</td>\n",
       "      <td>-0.032868</td>\n",
       "      <td>0.033404</td>\n",
       "      <td>-0.060470</td>\n",
       "      <td>-0.032112</td>\n",
       "      <td>0.156392</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.105971</td>\n",
       "      <td>0.127703</td>\n",
       "      <td>-0.003257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.093554</td>\n",
       "      <td>0.128370</td>\n",
       "      <td>-0.028194</td>\n",
       "      <td>-0.018356</td>\n",
       "      <td>-0.053395</td>\n",
       "      <td>-0.225168</td>\n",
       "      <td>-0.011497</td>\n",
       "      <td>-0.305022</td>\n",
       "      <td>0.117323</td>\n",
       "      <td>-0.025285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118483</td>\n",
       "      <td>-0.042447</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>-0.057516</td>\n",
       "      <td>-0.158376</td>\n",
       "      <td>0.102997</td>\n",
       "      <td>0.139313</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.080842</td>\n",
       "      <td>0.006264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.140438</td>\n",
       "      <td>0.081195</td>\n",
       "      <td>-0.047703</td>\n",
       "      <td>0.130174</td>\n",
       "      <td>0.056607</td>\n",
       "      <td>0.207251</td>\n",
       "      <td>0.070721</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.153874</td>\n",
       "      <td>0.070782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067396</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.141574</td>\n",
       "      <td>-0.230385</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.048312</td>\n",
       "      <td>0.086723</td>\n",
       "      <td>-0.050822</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>0.251917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.289409</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>-0.253958</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>-0.076804</td>\n",
       "      <td>-0.180595</td>\n",
       "      <td>-0.035166</td>\n",
       "      <td>-0.145462</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>-0.184930</td>\n",
       "      <td>-0.100268</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.242584</td>\n",
       "      <td>-0.188970</td>\n",
       "      <td>-0.185496</td>\n",
       "      <td>-0.090638</td>\n",
       "      <td>-0.065336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.116327</td>\n",
       "      <td>0.151799</td>\n",
       "      <td>-0.028764</td>\n",
       "      <td>-0.106600</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>-0.031400</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.083942</td>\n",
       "      <td>0.086584</td>\n",
       "      <td>-0.035632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053366</td>\n",
       "      <td>-0.043573</td>\n",
       "      <td>-0.029702</td>\n",
       "      <td>-0.057871</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>-0.084420</td>\n",
       "      <td>-0.014334</td>\n",
       "      <td>-0.075682</td>\n",
       "      <td>-0.035531</td>\n",
       "      <td>-0.213721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.068207</td>\n",
       "      <td>0.128392</td>\n",
       "      <td>-0.151553</td>\n",
       "      <td>-0.055404</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.051655</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.343014</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.174997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.032842</td>\n",
       "      <td>0.025645</td>\n",
       "      <td>-0.241301</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.215912</td>\n",
       "      <td>0.115223</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>0.124583</td>\n",
       "      <td>-0.046661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.182741</td>\n",
       "      <td>-0.365308</td>\n",
       "      <td>-0.290506</td>\n",
       "      <td>-0.184267</td>\n",
       "      <td>-0.074892</td>\n",
       "      <td>0.102709</td>\n",
       "      <td>-0.154247</td>\n",
       "      <td>-0.110662</td>\n",
       "      <td>-0.030901</td>\n",
       "      <td>-0.098289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071445</td>\n",
       "      <td>0.114158</td>\n",
       "      <td>0.124041</td>\n",
       "      <td>-0.026818</td>\n",
       "      <td>-0.104572</td>\n",
       "      <td>-0.186069</td>\n",
       "      <td>0.030783</td>\n",
       "      <td>-0.099956</td>\n",
       "      <td>-0.068018</td>\n",
       "      <td>0.209464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.189778</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>-0.120371</td>\n",
       "      <td>-0.018678</td>\n",
       "      <td>-0.081769</td>\n",
       "      <td>0.072061</td>\n",
       "      <td>-0.016983</td>\n",
       "      <td>-0.044562</td>\n",
       "      <td>-0.019461</td>\n",
       "      <td>0.067517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060864</td>\n",
       "      <td>0.117330</td>\n",
       "      <td>-0.018234</td>\n",
       "      <td>-0.053165</td>\n",
       "      <td>-0.027099</td>\n",
       "      <td>0.051672</td>\n",
       "      <td>-0.037847</td>\n",
       "      <td>0.116063</td>\n",
       "      <td>0.103119</td>\n",
       "      <td>0.125154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.039854  0.077193  0.279487  0.076518  0.016184  0.106008  0.382421   \n",
       "1    0.015984 -0.060795  0.148113  0.089952  0.201932  0.038565  0.152215   \n",
       "2   -0.057855 -0.158905  0.092555 -0.001988  0.064708 -0.011258 -0.029701   \n",
       "3   -0.093554  0.128370 -0.028194 -0.018356 -0.053395 -0.225168 -0.011497   \n",
       "4   -0.140438  0.081195 -0.047703  0.130174  0.056607  0.207251  0.070721   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123  0.289409 -0.046361 -0.253958  0.030570  0.101677 -0.076804 -0.180595   \n",
       "124 -0.116327  0.151799 -0.028764 -0.106600  0.050047 -0.031400  0.011641   \n",
       "125  0.068207  0.128392 -0.151553 -0.055404  0.001867  0.051655  0.036914   \n",
       "126 -0.182741 -0.365308 -0.290506 -0.184267 -0.074892  0.102709 -0.154247   \n",
       "127  0.189778  0.051186 -0.120371 -0.018678 -0.081769  0.072061 -0.016983   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.041995  0.120926  0.080694  ...  0.090990  0.079089  0.035011   \n",
       "1    0.174304  0.136421 -0.116924  ...  0.017210  0.056162  0.046566   \n",
       "2   -0.014987 -0.022324 -0.118542  ...  0.049019 -0.032868  0.033404   \n",
       "3   -0.305022  0.117323 -0.025285  ... -0.118483 -0.042447 -0.004954   \n",
       "4    0.083958  0.153874  0.070782  ... -0.067396  0.022529  0.141574   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -0.035166 -0.145462  0.014344  ...  0.017268  0.079038 -0.184930   \n",
       "124  0.083942  0.086584 -0.035632  ... -0.053366 -0.043573 -0.029702   \n",
       "125  0.343014  0.007300  0.174997  ...  0.034483 -0.032842  0.025645   \n",
       "126 -0.110662 -0.030901 -0.098289  ... -0.071445  0.114158  0.124041   \n",
       "127 -0.044562 -0.019461  0.067517  ... -0.060864  0.117330 -0.018234   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0    0.029171 -0.001345  0.067453  0.002025 -0.125295 -0.072662 -0.011904  \n",
       "1   -0.049303 -0.011361 -0.054555 -0.003828 -0.021726  0.049651 -0.076605  \n",
       "2   -0.060470 -0.032112  0.156392  0.026340  0.105971  0.127703 -0.003257  \n",
       "3   -0.057516 -0.158376  0.102997  0.139313  0.036914  0.080842  0.006264  \n",
       "4   -0.230385 -0.000100 -0.048312  0.086723 -0.050822  0.068883  0.251917  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.100268  0.001748  0.242584 -0.188970 -0.185496 -0.090638 -0.065336  \n",
       "124 -0.057871  0.014545 -0.084420 -0.014334 -0.075682 -0.035531 -0.213721  \n",
       "125 -0.241301  0.017702 -0.215912  0.115223 -0.069543  0.124583 -0.046661  \n",
       "126 -0.026818 -0.104572 -0.186069  0.030783 -0.099956 -0.068018  0.209464  \n",
       "127 -0.053165 -0.027099  0.051672 -0.037847  0.116063  0.103119  0.125154  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: bidirectional_4/bidirectional_4/forward_lstm_4/recurrent_kernel:0\n",
      "  Dataset: bidirectional_4/bidirectional_4/forward_lstm_4/recurrent_kernel:0\n",
      "    Shape: (64, 256)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143747</td>\n",
       "      <td>-0.137689</td>\n",
       "      <td>-0.252136</td>\n",
       "      <td>-0.125380</td>\n",
       "      <td>0.066511</td>\n",
       "      <td>-0.065125</td>\n",
       "      <td>-0.088099</td>\n",
       "      <td>-0.206855</td>\n",
       "      <td>-0.023826</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136309</td>\n",
       "      <td>-0.092718</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>-0.030242</td>\n",
       "      <td>-0.046756</td>\n",
       "      <td>-0.034638</td>\n",
       "      <td>-0.155904</td>\n",
       "      <td>0.043066</td>\n",
       "      <td>-0.088068</td>\n",
       "      <td>-0.111838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007907</td>\n",
       "      <td>-0.123546</td>\n",
       "      <td>0.068042</td>\n",
       "      <td>0.084445</td>\n",
       "      <td>0.112795</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>-0.100774</td>\n",
       "      <td>-0.021858</td>\n",
       "      <td>-0.043605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074289</td>\n",
       "      <td>-0.047453</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>-0.012196</td>\n",
       "      <td>-0.160222</td>\n",
       "      <td>-0.028098</td>\n",
       "      <td>-0.070709</td>\n",
       "      <td>-0.081557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.270143</td>\n",
       "      <td>0.213489</td>\n",
       "      <td>-0.106780</td>\n",
       "      <td>0.144884</td>\n",
       "      <td>0.116329</td>\n",
       "      <td>0.103691</td>\n",
       "      <td>-0.074102</td>\n",
       "      <td>0.208190</td>\n",
       "      <td>0.104987</td>\n",
       "      <td>0.032802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053738</td>\n",
       "      <td>0.211554</td>\n",
       "      <td>0.175841</td>\n",
       "      <td>-0.092264</td>\n",
       "      <td>0.244220</td>\n",
       "      <td>0.145667</td>\n",
       "      <td>0.072427</td>\n",
       "      <td>0.020641</td>\n",
       "      <td>0.130391</td>\n",
       "      <td>-0.009470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108608</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>0.266443</td>\n",
       "      <td>-0.137620</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.149294</td>\n",
       "      <td>0.140633</td>\n",
       "      <td>-0.014618</td>\n",
       "      <td>0.222954</td>\n",
       "      <td>0.087559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150081</td>\n",
       "      <td>0.177432</td>\n",
       "      <td>0.119409</td>\n",
       "      <td>0.094289</td>\n",
       "      <td>-0.032329</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>-0.044770</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>-0.077616</td>\n",
       "      <td>-0.120568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.271360</td>\n",
       "      <td>-0.124135</td>\n",
       "      <td>-0.120583</td>\n",
       "      <td>-0.206317</td>\n",
       "      <td>0.037004</td>\n",
       "      <td>0.111691</td>\n",
       "      <td>-0.018763</td>\n",
       "      <td>-0.217252</td>\n",
       "      <td>-0.098997</td>\n",
       "      <td>-0.127931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031259</td>\n",
       "      <td>-0.036240</td>\n",
       "      <td>-0.209187</td>\n",
       "      <td>-0.118442</td>\n",
       "      <td>-0.240993</td>\n",
       "      <td>-0.024193</td>\n",
       "      <td>-0.026636</td>\n",
       "      <td>0.056544</td>\n",
       "      <td>-0.032042</td>\n",
       "      <td>-0.260374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.049071</td>\n",
       "      <td>0.150946</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.039621</td>\n",
       "      <td>0.072603</td>\n",
       "      <td>0.124875</td>\n",
       "      <td>0.086348</td>\n",
       "      <td>0.074716</td>\n",
       "      <td>-0.045483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166198</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.093318</td>\n",
       "      <td>0.229695</td>\n",
       "      <td>0.046894</td>\n",
       "      <td>0.010081</td>\n",
       "      <td>0.090798</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.178152</td>\n",
       "      <td>0.155199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.071350</td>\n",
       "      <td>0.109691</td>\n",
       "      <td>0.005223</td>\n",
       "      <td>0.131909</td>\n",
       "      <td>0.062101</td>\n",
       "      <td>-0.061091</td>\n",
       "      <td>0.155047</td>\n",
       "      <td>0.091853</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>-0.086261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067902</td>\n",
       "      <td>-0.140040</td>\n",
       "      <td>-0.067790</td>\n",
       "      <td>-0.093881</td>\n",
       "      <td>0.072878</td>\n",
       "      <td>0.110551</td>\n",
       "      <td>-0.021690</td>\n",
       "      <td>0.080094</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>-0.075879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.129325</td>\n",
       "      <td>-0.190723</td>\n",
       "      <td>-0.050517</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.054185</td>\n",
       "      <td>0.182933</td>\n",
       "      <td>-0.257974</td>\n",
       "      <td>0.165790</td>\n",
       "      <td>-0.014406</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158676</td>\n",
       "      <td>-0.086535</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>-0.140567</td>\n",
       "      <td>-0.135106</td>\n",
       "      <td>-0.165339</td>\n",
       "      <td>-0.190837</td>\n",
       "      <td>-0.274605</td>\n",
       "      <td>-0.132063</td>\n",
       "      <td>-0.058827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.101928</td>\n",
       "      <td>0.120039</td>\n",
       "      <td>0.042421</td>\n",
       "      <td>-0.016486</td>\n",
       "      <td>-0.014630</td>\n",
       "      <td>-0.007357</td>\n",
       "      <td>0.097199</td>\n",
       "      <td>0.064765</td>\n",
       "      <td>0.057629</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104759</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>0.170482</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>-0.125394</td>\n",
       "      <td>0.130785</td>\n",
       "      <td>-0.204970</td>\n",
       "      <td>-0.136850</td>\n",
       "      <td>-0.275249</td>\n",
       "      <td>-0.038005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.015722</td>\n",
       "      <td>0.030328</td>\n",
       "      <td>-0.017238</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>-0.034679</td>\n",
       "      <td>0.156099</td>\n",
       "      <td>-0.016773</td>\n",
       "      <td>0.154260</td>\n",
       "      <td>-0.020655</td>\n",
       "      <td>-0.017130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153528</td>\n",
       "      <td>0.014389</td>\n",
       "      <td>-0.125142</td>\n",
       "      <td>-0.020328</td>\n",
       "      <td>0.031876</td>\n",
       "      <td>0.035666</td>\n",
       "      <td>0.064575</td>\n",
       "      <td>-0.034657</td>\n",
       "      <td>-0.037926</td>\n",
       "      <td>0.076263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.143747 -0.137689 -0.252136 -0.125380  0.066511 -0.065125 -0.088099   \n",
       "1   0.007907 -0.123546  0.068042  0.084445  0.112795  0.030034  0.099175   \n",
       "2   0.270143  0.213489 -0.106780  0.144884  0.116329  0.103691 -0.074102   \n",
       "3   0.108608  0.078435  0.266443 -0.137620  0.011441  0.149294  0.140633   \n",
       "4  -0.271360 -0.124135 -0.120583 -0.206317  0.037004  0.111691 -0.018763   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59  0.021944  0.049071  0.150946  0.037100  0.039621  0.072603  0.124875   \n",
       "60  0.071350  0.109691  0.005223  0.131909  0.062101 -0.061091  0.155047   \n",
       "61 -0.129325 -0.190723 -0.050517 -0.076437 -0.054185  0.182933 -0.257974   \n",
       "62 -0.101928  0.120039  0.042421 -0.016486 -0.014630 -0.007357  0.097199   \n",
       "63 -0.015722  0.030328 -0.017238  0.100456 -0.034679  0.156099 -0.016773   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0  -0.206855 -0.023826  0.055363  ... -0.136309 -0.092718  0.003517 -0.030242   \n",
       "1  -0.100774 -0.021858 -0.043605  ...  0.074289 -0.047453  0.074287 -0.035171   \n",
       "2   0.208190  0.104987  0.032802  ...  0.053738  0.211554  0.175841 -0.092264   \n",
       "3  -0.014618  0.222954  0.087559  ...  0.150081  0.177432  0.119409  0.094289   \n",
       "4  -0.217252 -0.098997 -0.127931  ... -0.031259 -0.036240 -0.209187 -0.118442   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59  0.086348  0.074716 -0.045483  ...  0.166198  0.081093  0.093318  0.229695   \n",
       "60  0.091853  0.003013 -0.086261  ...  0.067902 -0.140040 -0.067790 -0.093881   \n",
       "61  0.165790 -0.014406 -0.002187  ...  0.158676 -0.086535  0.041876 -0.140567   \n",
       "62  0.064765  0.057629  0.008556  ...  0.104759  0.118533  0.170482  0.084928   \n",
       "63  0.154260 -0.020655 -0.017130  ...  0.153528  0.014389 -0.125142 -0.020328   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0  -0.046756 -0.034638 -0.155904  0.043066 -0.088068 -0.111838  \n",
       "1   0.056995 -0.012196 -0.160222 -0.028098 -0.070709 -0.081557  \n",
       "2   0.244220  0.145667  0.072427  0.020641  0.130391 -0.009470  \n",
       "3  -0.032329  0.008353 -0.044770  0.026993 -0.077616 -0.120568  \n",
       "4  -0.240993 -0.024193 -0.026636  0.056544 -0.032042 -0.260374  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.046894  0.010081  0.090798  0.215900  0.178152  0.155199  \n",
       "60  0.072878  0.110551 -0.021690  0.080094 -0.002546 -0.075879  \n",
       "61 -0.135106 -0.165339 -0.190837 -0.274605 -0.132063 -0.058827  \n",
       "62 -0.125394  0.130785 -0.204970 -0.136850 -0.275249 -0.038005  \n",
       "63  0.031876  0.035666  0.064575 -0.034657 -0.037926  0.076263  \n",
       "\n",
       "[64 rows x 256 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Path: input_1\n",
      "  Attribute: weight_names = []\n",
      "--------------------------------------------------------------------------------\n",
      "Path: time_distributed_1\n",
      "  Attribute: weight_names = [b'time_distributed_1/kernel:0' b'time_distributed_1/bias:0']\n",
      "--------------------------------------------------------------------------------\n",
      "Path: time_distributed_1/time_distributed_1\n",
      "--------------------------------------------------------------------------------\n",
      "Path: time_distributed_1/time_distributed_1/bias:0\n",
      "  Dataset: time_distributed_1/time_distributed_1/bias:0\n",
      "    Shape: (5,)\n",
      "    Data type: float32\n",
      "    Data preview (first 5 elements):\n",
      "[-0.17893921  0.39645055 -0.12922056 -0.12762186 -0.21325165]\n",
      "--------------------------------------------------------------------------------\n",
      "Path: time_distributed_1/time_distributed_1/kernel:0\n",
      "  Dataset: time_distributed_1/time_distributed_1/kernel:0\n",
      "    Shape: (128, 5)\n",
      "    Data type: float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176741</td>\n",
       "      <td>0.149159</td>\n",
       "      <td>0.140591</td>\n",
       "      <td>-0.357064</td>\n",
       "      <td>0.152898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103523</td>\n",
       "      <td>0.059694</td>\n",
       "      <td>0.067929</td>\n",
       "      <td>-0.155039</td>\n",
       "      <td>-0.154260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067650</td>\n",
       "      <td>-0.129953</td>\n",
       "      <td>-0.124112</td>\n",
       "      <td>-0.131719</td>\n",
       "      <td>0.113393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.125927</td>\n",
       "      <td>0.111058</td>\n",
       "      <td>-0.132814</td>\n",
       "      <td>-0.173868</td>\n",
       "      <td>-0.169111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.111131</td>\n",
       "      <td>0.123191</td>\n",
       "      <td>0.147116</td>\n",
       "      <td>-0.171284</td>\n",
       "      <td>0.156122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.151881</td>\n",
       "      <td>-0.004667</td>\n",
       "      <td>0.063316</td>\n",
       "      <td>0.239943</td>\n",
       "      <td>-0.028391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.149984</td>\n",
       "      <td>-0.109656</td>\n",
       "      <td>-0.106166</td>\n",
       "      <td>0.341478</td>\n",
       "      <td>-0.135460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.132180</td>\n",
       "      <td>-0.175056</td>\n",
       "      <td>-0.156898</td>\n",
       "      <td>-0.183913</td>\n",
       "      <td>0.221081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.303516</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.048241</td>\n",
       "      <td>-0.142379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.184436</td>\n",
       "      <td>0.169432</td>\n",
       "      <td>-0.220823</td>\n",
       "      <td>0.144417</td>\n",
       "      <td>0.107330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4\n",
       "0    0.176741  0.149159  0.140591 -0.357064  0.152898\n",
       "1   -0.103523  0.059694  0.067929 -0.155039 -0.154260\n",
       "2    0.067650 -0.129953 -0.124112 -0.131719  0.113393\n",
       "3   -0.125927  0.111058 -0.132814 -0.173868 -0.169111\n",
       "4   -0.111131  0.123191  0.147116 -0.171284  0.156122\n",
       "..        ...       ...       ...       ...       ...\n",
       "123  0.151881 -0.004667  0.063316  0.239943 -0.028391\n",
       "124 -0.149984 -0.109656 -0.106166  0.341478 -0.135460\n",
       "125 -0.132180 -0.175056 -0.156898 -0.183913  0.221081\n",
       "126 -0.303516  0.018084  0.050997  0.048241 -0.142379\n",
       "127  0.184436  0.169432 -0.220823  0.144417  0.107330\n",
       "\n",
       "[128 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def print_h5_file_structure(file_name):\n",
    "    \"\"\"\n",
    "    Prints the structure of an HDF5 file, including groups, datasets, \n",
    "    and attributes. Datasets are printed as DataFrames if they are 2D.\n",
    "    \n",
    "    Parameters:\n",
    "    file_name (str): The name or path of the HDF5 file to print.\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_structure(name, obj):\n",
    "        \"\"\"\n",
    "        A helper function to recursively print the structure of the HDF5 file.\n",
    "        \n",
    "        Parameters:\n",
    "        name (str): The name of the object in the HDF5 file.\n",
    "        obj (h5py.Group or h5py.Dataset): The object itself (Group or Dataset).\n",
    "        \"\"\"\n",
    "        print(f\"Path: {name}\")\n",
    "        \n",
    "        # Print attributes of the group or dataset\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(f\"  Attribute: {key} = {val}\")\n",
    "        \n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  Dataset: {name}\")\n",
    "            print(f\"    Shape: {obj.shape}\")\n",
    "            print(f\"    Data type: {obj.dtype}\")\n",
    "            # If the dataset is 2D, display it as a DataFrame\n",
    "            if len(obj.shape) == 2:\n",
    "                df = pd.DataFrame(obj[:])\n",
    "                display(df)\n",
    "            else:\n",
    "                # Print dataset contents (avoid large data dumps)\n",
    "                print(\"    Data preview (first 5 elements):\")\n",
    "                print(obj[:5])  # Modify as needed to avoid too large outputs\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Open the file and print the structure\n",
    "    with h5py.File(file_name, 'r') as hdf:\n",
    "        print(f\"File: {file_name}\")\n",
    "        hdf.visititems(print_structure)\n",
    "\n",
    "# Usage\n",
    "filename = '../lstm_weights.h5'  # Replace with your actual file name\n",
    "print_h5_file_structure(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to lstm_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "def h5_to_pth(h5_filename, pth_filename):\n",
    "    \"\"\"\n",
    "    Converts an HDF5 (.h5) file to a PyTorch (.pth) file.\n",
    "\n",
    "    Parameters:\n",
    "    h5_filename (str): The path to the input .h5 file.\n",
    "    pth_filename (str): The path to save the output .pth file.\n",
    "    \"\"\"\n",
    "    # Dictionary to store data from the HDF5 file\n",
    "    data_dict = {}\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(h5_filename, 'r') as hdf:\n",
    "        def recursively_load_group(group, dict_to_save):\n",
    "            \"\"\"\n",
    "            Recursively loads data from HDF5 groups and datasets into a dictionary.\n",
    "            \"\"\"\n",
    "            for key in group.keys():\n",
    "                item = group[key]\n",
    "                if isinstance(item, h5py.Dataset):\n",
    "                    dict_to_save[key] = torch.tensor(item[:])\n",
    "                elif isinstance(item, h5py.Group):\n",
    "                    dict_to_save[key] = {}\n",
    "                    recursively_load_group(item, dict_to_save[key])\n",
    "        \n",
    "        # Start loading data from the root group\n",
    "        recursively_load_group(hdf, data_dict)\n",
    "\n",
    "    # Save the dictionary as a .pth file\n",
    "    torch.save(data_dict, pth_filename)\n",
    "    print(f\"Data saved to {pth_filename}\")\n",
    "\n",
    "# Usage\n",
    "h5_filename = '../lstm_weights.h5'  # Replace with your actual .h5 file path\n",
    "pth_filename = 'lstm_weights.pth'  # Replace with desired .pth file path\n",
    "h5_to_pth(h5_filename, pth_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/sgn_zcm52nl7hs_clb0tdbph0000gn/T/ipykernel_74486/665818149.py:36: UserWarning: Tight layout not applied. tight_layout cannot make Axes height small enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data from the .mat file\n",
    "filepath = '../data/data.mat'\n",
    "mat_file = sio.loadmat(filepath)\n",
    "n_examples = np.shape(mat_file['sig1'])[0]\n",
    "x = np.zeros((4, n_examples, 3000, 1))\n",
    "x[0] = mat_file['sig1']\n",
    "x[1] = mat_file['sig2']\n",
    "x[2] = mat_file['sig3']\n",
    "x[3] = mat_file['sig4']\n",
    "\n",
    "\n",
    "# load labels\n",
    "labels_file = sio.loadmat('../data/labels.mat')\n",
    "labels = labels_file['labels'][0].flatten() - 1\n",
    "\n",
    "sleep_stages = {4: 'Wake', 3: 'REM', 2: 'N1', 1: 'N2', 0: 'N3'}\n",
    "\n",
    "# convert labels to sleep stages\n",
    "labels = [sleep_stages[label] for label in labels]\n",
    "\n",
    "\n",
    "\n",
    "# plot the data from the 10 examples with a for loop and adjust spacing between subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(10, 20):\n",
    "    plt.subplot(20, 2, i+1)\n",
    "    plt.plot(x[0][i])\n",
    "    plt.title(f\"Example {i+1}: {labels[i]}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 1), (3000, 1), (3000, 1), (3000, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][2].shape, x[1][2].shape, x[2][2].shape, x[3][2].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
