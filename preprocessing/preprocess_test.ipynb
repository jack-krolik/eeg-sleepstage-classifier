{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_file_name = 'preprocessed_NaN.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_file_name = 'preprocessed_data.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import mne\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "\n",
    "# Load the EDF file\n",
    "edf_path = '../data/201 N1.edf'\n",
    "raw = mne.io.read_raw_edf(edf_path, preload=True)\n",
    "\n",
    "# Convert to microvolts\n",
    "raw._data *= 1e6  # Convert from volts to microvolts\n",
    "print(f\"After conversion to Î¼V - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Bandpass filter from 0.5 to 50 Hz\n",
    "raw.filter(0.5, 50, fir_design='firwin')\n",
    "print(f\"After filtering - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Downsample to 100 Hz\n",
    "raw.resample(100)\n",
    "print(f\"After resampling - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Print out the channel names in the raw EDF data\n",
    "print(\"Channel names:\", raw.ch_names)\n",
    "\n",
    "# Create bipolar montage\n",
    "bipolar_montage = mne.set_bipolar_reference(\n",
    "    raw,\n",
    "    anode=['F3-M2', 'C3-M2', 'F4-M1', 'C4-M1'],\n",
    "    cathode=['C3-M2', 'O1-M2', 'C4-M1', 'O2-M1'],\n",
    "    ch_name=['F3-C3', 'C3-O1', 'F4-C4', 'C4-O2']\n",
    ")\n",
    "\n",
    "# Pick only the bipolar channels\n",
    "raw = bipolar_montage.pick_channels(['F3-C3', 'C3-O1', 'F4-C4', 'C4-O2'])\n",
    "print(f\"After bipolar montage - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Segment into 30-second epochs\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=True)\n",
    "print(f\"After epoching - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "\n",
    "# Precompute shared value\n",
    "sfreq = raw.info['sfreq']\n",
    "\n",
    "# Optimized artifact detection function with shape correction\n",
    "def detect_artifacts(data, threshold=500, flat_threshold=1e-6, flat_duration=5, sfreq=100):\n",
    "    # Amplitude mask (same length as the data)\n",
    "    amplitude_mask = np.abs(data) > threshold\n",
    "    \n",
    "    # Flat signal detection (diff reduces length by 1)\n",
    "    diff = np.abs(np.diff(data))\n",
    "    flat_mask = np.zeros_like(data, dtype=bool)  # Initialize with the same length as data\n",
    "    flat_mask[:-1] = ndimage.uniform_filter1d(diff < flat_threshold, size=int(flat_duration * sfreq)) > 0.99\n",
    "    \n",
    "    return amplitude_mask, flat_mask\n",
    "\n",
    "# Optimized artifact handling\n",
    "def handle_artifacts(data, artifact_mask):\n",
    "    data[artifact_mask] = 0  # You could also use np.nan depending on the requirements\n",
    "    return data\n",
    "\n",
    "# Optimized preprocessing function for sleep stages\n",
    "def preprocess_sleep_stages(file_path, epochs_to_remove, keep_unscored=False):\n",
    "    stage_mapping = {7: 4, 5: 3, 1: 2, 2: 1, 3: 0, 0: -1}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[2:]  # Skip the first two lines\n",
    "\n",
    "    sleep_stages = np.array([int(line.split()[0]) for line in lines])\n",
    "\n",
    "    # Efficiently find start and end of awake states (7)\n",
    "    start_index = np.argmax(sleep_stages == 7)\n",
    "    end_index = len(sleep_stages) - np.argmax(sleep_stages[::-1] == 7)\n",
    "\n",
    "    # Ensure 50 awake states at start and end\n",
    "    sleep_stages[max(0, start_index - 50):start_index] = 7\n",
    "    sleep_stages[end_index:min(len(sleep_stages), end_index + 50)] = 7\n",
    "\n",
    "    processed_stages = sleep_stages if keep_unscored else sleep_stages[start_index:end_index]\n",
    "\n",
    "    # Renumber stages and remove unwanted epochs\n",
    "    renumbered_stages = np.vectorize(stage_mapping.get)(processed_stages)\n",
    "    np_stages = np.delete(renumbered_stages, list(epochs_to_remove), axis=0)\n",
    "\n",
    "    return np_stages.reshape(1, -1)\n",
    "\n",
    "# Initialize arrays to track artifacts\n",
    "num_epochs = epochs._data.shape[0]\n",
    "amplitude_artifact_counts = np.zeros(num_epochs)\n",
    "flat_artifact_counts = np.zeros(num_epochs)\n",
    "epochs_to_remove = []  # Change this to a list to preserve order\n",
    "\n",
    "# Apply artifact detection and handling\n",
    "for i in range(num_epochs):  # Loop through epochs\n",
    "    for j in range(epochs._data.shape[1]):  # Loop through channels\n",
    "        data_epoch = epochs._data[i, j, :]\n",
    "        amplitude_mask, flat_mask = detect_artifacts(data_epoch, sfreq=sfreq)\n",
    "        artifact_mask = amplitude_mask | flat_mask\n",
    "        \n",
    "        # Count the number of artifacts for each epoch\n",
    "        amplitude_artifact_counts[i] += np.sum(amplitude_mask)\n",
    "        flat_artifact_counts[i] += np.sum(flat_mask)\n",
    "        \n",
    "        # Remove epochs with long consecutive artifacts (e.g., 5 seconds)\n",
    "        if (np.sum(amplitude_mask) >= 5 * sfreq) or (np.sum(flat_mask) >= 5 * sfreq):\n",
    "            epochs_to_remove.append(i)\n",
    "            break\n",
    "        \n",
    "        # Handle artifacts in one pass\n",
    "        epochs._data[i, j, :] = handle_artifacts(data_epoch, artifact_mask)\n",
    "\n",
    "\n",
    "\n",
    "# Separate the artifact counts for kept and removed epochs\n",
    "kept_amplitude_counts = np.delete(amplitude_artifact_counts, list(epochs_to_remove))\n",
    "kept_flat_counts = np.delete(flat_artifact_counts, list(epochs_to_remove))\n",
    "# Separate the artifact counts for kept and removed epochs\n",
    "removed_amplitude_counts = amplitude_artifact_counts[epochs_to_remove]\n",
    "removed_flat_counts = flat_artifact_counts[epochs_to_remove]\n",
    "\n",
    "# Remove epochs marked for removal\n",
    "epochs.drop(epochs_to_remove)\n",
    "\n",
    "# Print total artifact counts and epoch removal information\n",
    "print(f\"Total interpolated amplitude artifacts in kept epochs: {int(np.sum(kept_amplitude_counts))}\")\n",
    "print(f\"Total interpolated flat signal artifacts in kept epochs: {int(np.sum(kept_flat_counts))}\")\n",
    "print(f\"Total amplitude artifacts in removed epochs: {int(np.sum(removed_amplitude_counts))}\")\n",
    "print(f\"Total flat signal artifacts in removed epochs: {int(np.sum(removed_flat_counts))}\")\n",
    "print(f\"Number of epochs removed due to long artifacts: {len(epochs_to_remove)}\")\n",
    "\n",
    "# Print data range after artifact handling\n",
    "print(f\"After artifact handling - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")\n",
    "\n",
    "# Your target medians and IQRs for each channel\n",
    "iqr_target = torch.tensor([7.90, 11.37, 7.92, 11.56])\n",
    "med_target = torch.tensor([0.0257, 0.0942, 0.02157, 0.1055])\n",
    "\n",
    "# Efficiently rescale each channel\n",
    "for i, ch in enumerate(raw.ch_names):\n",
    "    data = epochs.get_data(picks=ch).reshape(-1)\n",
    "    median = np.median(data)\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    \n",
    "    # Scale the data to match target median and IQR\n",
    "    scaled_data = (data - median) / iqr * iqr_target[i].item() + med_target[i].item()\n",
    "    epochs._data[:, epochs.ch_names.index(ch), :] = scaled_data.reshape(epochs._data.shape[0], -1)\n",
    "\n",
    "# Print data range after scaling\n",
    "print(f\"After scaling - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")\n",
    "\n",
    "# Extract preprocessed data (without the need for transposition)\n",
    "preprocessed_data = epochs.get_data()\n",
    "\n",
    "file_path = '../data/Sleep Stages 201_N1.txt'  # Replace with the actual file path\n",
    "\n",
    "\n",
    "# Save the preprocessed data\n",
    "mat_dict = {\n",
    "    'sig1': preprocessed_data[:, 0, :],  \n",
    "    'sig2': preprocessed_data[:, 1, :],  \n",
    "    'sig3': preprocessed_data[:, 2, :],  \n",
    "    'sig4': preprocessed_data[:, 3, :],  \n",
    "    'Fs': sfreq,\n",
    "    'ch_names': raw.ch_names,\n",
    "    'labels': preprocess_sleep_stages(file_path, epochs_to_remove, keep_unscored=True)\n",
    "}\n",
    "\n",
    "# Save as .mat file\n",
    "preprocessed_file_name = 'preprocessed_data.mat'\n",
    "try:\n",
    "    scipy.io.savemat(preprocessed_file_name, mat_dict)\n",
    "    print(\"Data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "# Plot function\n",
    "def plot_channel(data, artifact_mask, channel_name, fs):\n",
    "    time = np.arange(len(data)) / fs\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(time, data, label=\"Signal\", color='blue')\n",
    "    artifact_indices = np.where(artifact_mask)[0]\n",
    "    if artifact_indices.size > 0:\n",
    "        plt.scatter(time[artifact_indices], data[artifact_indices], color='red', label=\"Artifact\", marker='.', s=10)\n",
    "    plt.title(f'{channel_name} from 201 N1.edf (Preprocessed with Artifacts Highlighted)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude (Î¼V)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot each channel and highlight artifacts\n",
    "for i, ch_name in enumerate(raw.ch_names):\n",
    "    channel_data = preprocessed_data[:, i, :].flatten()\n",
    "    amplitude_mask, flat_mask = detect_artifacts(channel_data, sfreq=sfreq)\n",
    "    plot_channel(channel_data, amplitude_mask | flat_mask, ch_name, sfreq)\n",
    "\n",
    "# Check data quality\n",
    "def check_data_quality(data):\n",
    "    if np.isnan(data).any():\n",
    "        print(\"Warning: NaN values detected in preprocessed data.\")\n",
    "    if np.isinf(data).any():\n",
    "        print(\"Warning: Infinite values detected in preprocessed data.\")\n",
    "    print(f\"Final data range: {data.min():.2f} to {data.max():.2f} Î¼V\")\n",
    "\n",
    "check_data_quality(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(data, title='Histogram of EEG Amplitudes After Preprocessing', xlabel='Amplitude (Î¼V)', ylabel='Frequency', bins=100, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Function to plot a histogram for the given data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to plot the histogram for.\n",
    "    - title: The title of the plot.\n",
    "    - xlabel: Label for the x-axis.\n",
    "    - ylabel: Label for the y-axis.\n",
    "    - bins: Number of bins for the histogram.\n",
    "    - figsize: Size of the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.hist(data.flatten(), bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_artifact_counts(ax, title, counts, color, xlabel, ylabel, epoch_label, log_scale=True):\n",
    "    \"\"\"\n",
    "    Plots the artifact counts for a specific set of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    - ax: The axis object to plot on.\n",
    "    - title: The title of the plot.\n",
    "    - counts: The array of artifact counts to plot.\n",
    "    - color: The color of the bars.\n",
    "    - xlabel: The label for the x-axis.\n",
    "    - ylabel: The label for the y-axis.\n",
    "    - epoch_label: A string to describe the type of epoch (e.g., \"Kept\" or \"Removed\").\n",
    "    - log_scale: Whether to apply a symlog scale to the y-axis.\n",
    "    \"\"\"\n",
    "    ax.bar(np.arange(len(counts)), counts, color=color, alpha=0.7)\n",
    "    ax.set_title(f'{title} (Total: {int(np.sum(counts))})')\n",
    "    ax.set_xlabel(f'{epoch_label} Epoch Index')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(0, max(1, len(counts)))\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    if log_scale:\n",
    "        ax.set_yscale('symlog')\n",
    "\n",
    "\n",
    "\n",
    "def select_ticks(epoch_numbers, max_ticks=10):\n",
    "    if len(epoch_numbers) <= max_ticks:\n",
    "        return epoch_numbers\n",
    "    \n",
    "    step = max(1, len(epoch_numbers) // max_ticks)\n",
    "    return epoch_numbers[::step]\n",
    "\n",
    "def plot_combined_artifacts(ax, amp_counts, flat_counts, epoch_numbers, xlabel, ylabel):\n",
    "    ax.bar(epoch_numbers, amp_counts, color='blue', alpha=0.7, label='Amplitude')\n",
    "    ax.bar(epoch_numbers, flat_counts, color='red', alpha=0.7, label='Flat Signal')\n",
    "    \n",
    "    num_removed_epochs = len(epoch_numbers)\n",
    "    ax.set_title(f'Artifacts in Removed Epochs (Total Removed: {num_removed_epochs})')\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Use the select_ticks function to choose which ticks to display\n",
    "    selected_ticks = select_ticks(epoch_numbers)\n",
    "    ax.set_xticks(selected_ticks)\n",
    "    ax.set_xticklabels(selected_ticks, rotation=45, ha='right')\n",
    "    \n",
    "    # Ensure all epoch numbers are included in the x-axis range\n",
    "    ax.set_xlim(min(epoch_numbers) - 1, max(epoch_numbers) + 1)\n",
    "\n",
    "def plot_removed_epochs_no_data(ax):\n",
    "    \"\"\"\n",
    "    Displays a message when no epochs were removed due to artifacts.\n",
    "    \n",
    "    Parameters:\n",
    "    - ax: The axis object to display the message on.\n",
    "    \"\"\"\n",
    "    ax.text(0.5, 0.5, 'No epochs were removed', ha='center', va='center', fontsize=12)\n",
    "    ax.set_title('Artifacts in Removed Epochs')\n",
    "    ax.axis('off')\n",
    "\n",
    "def plot_artifact_summary(kept_amplitude_counts, kept_flat_counts, removed_amplitude_counts, removed_flat_counts):\n",
    "    \"\"\"\n",
    "    Creates subplots for artifact counts across kept and removed epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    - kept_amplitude_counts: Array of amplitude artifact counts for kept epochs.\n",
    "    - kept_flat_counts: Array of flat signal artifact counts for kept epochs.\n",
    "    - removed_amplitude_counts: Array of amplitude artifact counts for removed epochs.\n",
    "    - removed_flat_counts: Array of flat signal artifact counts for removed epochs.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "    # Plot for kept epochs - Amplitude artifacts\n",
    "    plot_artifact_counts(ax[0], 'Interpolated Amplitude Artifacts in Kept Epochs', kept_amplitude_counts, \n",
    "                         color='blue', xlabel='Kept Epoch Index', ylabel='Number of Artifacts', epoch_label='Kept')\n",
    "\n",
    "    # Plot for kept epochs - Flat signal artifacts\n",
    "    plot_artifact_counts(ax[1], 'Interpolated Flat Signal Artifacts in Kept Epochs', kept_flat_counts, \n",
    "                         color='red', xlabel='Kept Epoch Index', ylabel='Number of Artifacts', epoch_label='Kept')\n",
    "\n",
    "    \n",
    "    # In your main code, modify the loop as follows:\n",
    "    if len(removed_amplitude_counts) > 0:\n",
    "        plot_combined_artifacts(ax[2], removed_amplitude_counts, removed_flat_counts, \n",
    "                                epoch_numbers=epochs_to_remove,  # Use the actual epoch numbers\n",
    "                                xlabel='Removed Epoch Number', \n",
    "                                ylabel='Number of Artifacts')\n",
    "    else:\n",
    "        plot_removed_epochs_no_data(ax[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(preprocessed_data, title='Histogram of EEG Amplitudes After Preprocessing')\n",
    "# Call the plotting function with the relevant data\n",
    "plot_artifact_summary(kept_amplitude_counts, kept_flat_counts, removed_amplitude_counts, removed_flat_counts)\n",
    "\n",
    "\n",
    "# Print number of sleep stages and epochs\n",
    "print(f\"Number of sleep stages: {mat_dict['labels'].shape[1]}\")\n",
    "print(f\"Number of epochs: {preprocessed_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.signal import welch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import optuna\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import\n",
    "from sleepdetector_new import ImprovedSleepdetector\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def load_data(filepath, add_dim=False):\n",
    "    try:\n",
    "        # Load the data from the .mat file\n",
    "        mat_file = sio.loadmat(filepath)\n",
    "        \n",
    "        # Stack the signals into x\n",
    "        x = np.stack((mat_file['sig1'], mat_file['sig2'], mat_file['sig3'], mat_file['sig4']), axis=1)\n",
    "        x = torch.from_numpy(x).float()  # Convert to PyTorch tensor\n",
    "        \n",
    "        # Load the labels\n",
    "        y = torch.from_numpy(mat_file['labels'].flatten()).long()\n",
    "        \n",
    "        # Remove epochs where y is -1 (if any)\n",
    "        valid_indices = y != -1\n",
    "        x = x[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "        \n",
    "        # x is already in shape [number of epochs, 4, 3000], so no need to permute\n",
    "        \n",
    "        if add_dim:\n",
    "            x = x.unsqueeze(1)  # Add an extra dimension if required\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_spectral_features(x):\n",
    "    features = []\n",
    "    for channel in x:\n",
    "        f, psd = welch(channel.squeeze().numpy(), fs=100, nperseg=1000)\n",
    "        delta = np.sum(psd[(f >= 0.5) & (f <= 4)])\n",
    "        theta = np.sum(psd[(f > 4) & (f <= 8)])\n",
    "        alpha = np.sum(psd[(f > 8) & (f <= 13)])\n",
    "        beta = np.sum(psd[(f > 13) & (f <= 30)])\n",
    "        features.extend([delta, theta, alpha, beta])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def prepare_data(x, y, test_size=0.2, split=True):\n",
    "    \"\"\"\n",
    "    Prepare data for training or testing.\n",
    "    \n",
    "    :param x: Input data tensor\n",
    "    :param y: Labels tensor\n",
    "    :param test_size: Proportion of the dataset to include in the test split\n",
    "    :param split: If True, split the data into train and test sets. If False, process all data without splitting.\n",
    "    :return: Processed data tensors\n",
    "    \"\"\"\n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=test_size, stratify=y, random_state=42)\n",
    "        \n",
    "        X_train_spectral = np.array([extract_spectral_features(torch.from_numpy(x)) for x in X_train])\n",
    "        X_test_spectral = np.array([extract_spectral_features(torch.from_numpy(x)) for x in X_test])\n",
    "        \n",
    "        X_train_combined = np.concatenate([X_train.reshape(X_train.shape[0], -1), X_train_spectral], axis=1)\n",
    "        X_test_combined = np.concatenate([X_test.reshape(X_test.shape[0], -1), X_test_spectral], axis=1)\n",
    "        \n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
    "        \n",
    "        original_shape = list(X_train.shape)\n",
    "        original_shape[0] = X_train_resampled.shape[0]\n",
    "        spectral_shape = (X_train_resampled.shape[0], X_train_spectral.shape[1])\n",
    "        \n",
    "        X_train_final = X_train_resampled[:, :-X_train_spectral.shape[1]].reshape(original_shape)\n",
    "        X_train_spectral_final = X_train_resampled[:, -X_train_spectral.shape[1]:].reshape(spectral_shape)\n",
    "        \n",
    "        return (torch.from_numpy(X_train_final).float(),\n",
    "                torch.from_numpy(X_train_spectral_final).float(),\n",
    "                torch.from_numpy(y_train_resampled).long(),\n",
    "                torch.from_numpy(X_test).float(),\n",
    "                torch.from_numpy(X_test_spectral).float(),\n",
    "                torch.from_numpy(y_test).long())\n",
    "    else:\n",
    "        X_spectral = np.array([extract_spectral_features(x_i) for x_i in x])\n",
    "        \n",
    "        return (x.float(),\n",
    "                torch.from_numpy(X_spectral).float(),\n",
    "                y.long())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([ImprovedSleepdetector(**model_params) for _ in range(n_models)])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100):\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        for batch_x, batch_x_spectral, batch_y in train_loader:\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_spectral)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        accuracy = evaluate_model(model, val_data, device)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "        \n",
    "        scheduler.step(accuracy)\n",
    "    \n",
    "    return best_model_state, best_accuracy\n",
    "\n",
    "\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()\n",
    "    try:\n",
    "        X, X_spectral, y = data\n",
    "        logging.info(f\"Data shapes - X: {X.shape}, X_spectral: {X_spectral.shape}, y: {y.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X.to(device), X_spectral.to(device))\n",
    "            logging.info(f\"Model output shape: {outputs.shape}\")\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            logging.info(f\"Predicted shape: {predicted.shape}\")\n",
    "            \n",
    "            accuracy = accuracy_score(y.cpu().numpy(), predicted.cpu().numpy())\n",
    "            kappa = cohen_kappa_score(y.cpu().numpy(), predicted.cpu().numpy())\n",
    "            \n",
    "        return accuracy, kappa, predicted.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in evaluate_model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter optimization\n",
    "def objective(trial, X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test, device):\n",
    "    model_params = {\n",
    "        'n_filters': trial.suggest_categorical('n_filters', [[32, 64, 128], [64, 128, 256]]),\n",
    "        'lstm_hidden': trial.suggest_int('lstm_hidden', 64, 512),\n",
    "        'lstm_layers': trial.suggest_int('lstm_layers', 1, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    }\n",
    "    \n",
    "    train_params = {\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    }\n",
    "    \n",
    "    model = ImprovedSleepdetector(**model_params).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "    train_loader = DataLoader(TensorDataset(X_train, X_train_spectral, y_train), batch_size=train_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    _, accuracy = train_model(model, train_loader, (X_test, X_test_spectral, y_test), optimizer, ReduceLROnPlateau(optimizer), nn.CrossEntropyLoss(), device, epochs=10)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Cross-validation\n",
    "def cross_validate(X, X_spectral, y, model_params, train_params, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        X_spectral_train_fold, X_spectral_val_fold = X_spectral[train_idx], X_spectral[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = ImprovedSleepdetector(**model_params).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "        train_loader = DataLoader(TensorDataset(X_train_fold, X_spectral_train_fold, y_train_fold), batch_size=train_params['batch_size'], shuffle=True)\n",
    "        \n",
    "        _, accuracy = train_model(model, train_loader, (X_val_fold, X_spectral_val_fold, y_val_fold), optimizer, ReduceLROnPlateau(optimizer), nn.CrossEntropyLoss(), device, epochs=50)\n",
    "        scores.append(accuracy)\n",
    "        \n",
    "        logging.info(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    logging.info(f\"Average Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    return scores\n",
    "\n",
    "# Confusion matrix plotting\n",
    "def plot_confusion_matrix(y_true, y_pred, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Class names in the correct order (0 to 4)\n",
    "    class_names = ['N3', 'N2', 'N1', 'REM', 'Awake']\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',\n",
    "                cmap=cmap, square=True, xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "x, y = load_data(preprocessed_file_name)\n",
    "X, X_spectral, y = prepare_data(x, y, split=False)\n",
    "print(f\"X shape: {x.shape}, Y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.io\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_eeg_and_sleep_stages(x, y, fs, ch_names, start_epoch=0, num_epochs=None):\n",
    "    \"\"\"\n",
    "    Plot EEG data and corresponding sleep stages.\n",
    "    \n",
    "    :param x: EEG data tensor of shape (epochs, channels, time)\n",
    "    :param y: Sleep stage labels tensor of shape (epochs,)\n",
    "    :param fs: Sampling frequency (Hz)\n",
    "    :param ch_names: List of EEG channel names\n",
    "    :param start_epoch: Starting epoch to plot (default 0)\n",
    "    :param num_epochs: Number of epochs to plot (default is all epochs)\n",
    "    \"\"\"\n",
    "    if num_epochs is None:\n",
    "        num_epochs = x.shape[0]\n",
    "    \n",
    "    start_epoch = max(0, min(start_epoch, x.shape[0] - num_epochs))\n",
    "    \n",
    "    plot_x = x[start_epoch:start_epoch+num_epochs].numpy()\n",
    "    plot_y = y[start_epoch:start_epoch+num_epochs].numpy()\n",
    "    \n",
    "    num_samples_per_epoch = plot_x.shape[2]\n",
    "    total_samples = num_epochs * num_samples_per_epoch\n",
    "    time_vector = np.arange(total_samples) / fs\n",
    "    \n",
    "    # fig, axs = plt.subplots(5, 1, figsize=(15, 12), sharex=True, gridspec_kw={'height_ratios': [3, 3, 3, 3, 1]})\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(15, 12), sharex=False, gridspec_kw={'height_ratios': [3, 3, 3, 3, 1]})\n",
    "    fig.suptitle(f'EEG Data and Sleep Stages (Epochs {start_epoch} to {start_epoch+num_epochs-1})')\n",
    "    \n",
    "    for i in range(4):\n",
    "        flattened_data = plot_x[:, i, :].flatten()\n",
    "        axs[i].plot(time_vector, flattened_data)\n",
    "        axs[i].set_ylabel(ch_names[i])\n",
    "        axs[i].set_xlim(0, total_samples / fs)\n",
    "        axs[i].set_ylim(flattened_data.min(), flattened_data.max())\n",
    "    \n",
    "    stage_colors = ['purple', 'blue', 'green', 'yellow', 'red']  # Colors for N3, N2, N1, REM, Awake\n",
    "    cmap = ListedColormap(stage_colors)\n",
    "    \n",
    "    epoch_duration = num_samples_per_epoch / fs\n",
    "    for i, stage in enumerate(plot_y):\n",
    "        start = i * epoch_duration\n",
    "        end = (i + 1) * epoch_duration\n",
    "        axs[4].axvspan(start, end, facecolor=stage_colors[int(stage)], alpha=0.7)\n",
    "    \n",
    "    axs[4].set_yticks([])\n",
    "    # axs[4].set_xlabel('Time (seconds)')   \n",
    "    axs[4].set_xlim(0, total_samples / fs)\n",
    "\n",
    "    print(\"Total Length of time in hours:\", total_samples / fs / 3600)\n",
    "\n",
    "    for i in range(5):  # Now including the sleep stage plot\n",
    "        axs[i].set_xlabel('Time (seconds)')\n",
    "\n",
    "    # Create legend elements\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=stage_colors[i], alpha=0.7) \n",
    "                       for i in range(5)]\n",
    "    \n",
    "    # Add legend to the right side under the graph\n",
    "    fig.legend(legend_elements, ['N3', 'N2', 'N1', 'REM', 'Awake'], \n",
    "               loc='lower right', bbox_to_anchor=(0.98, 0.02), ncol=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Adjust the layout to make room for the legend\n",
    "    plt.subplots_adjust(bottom=0.1, right=.9)\n",
    "    return fig\n",
    "\n",
    "# Usage remains the same\n",
    "fs = scipy.io.loadmat(preprocessed_file_name)['Fs'].flatten()[0]\n",
    "ch_names = scipy.io.loadmat(preprocessed_file_name)['ch_names'].flatten().tolist()\n",
    "\n",
    "# Plot all epochs\n",
    "plot_eeg_and_sleep_stages(x, y, fs, ch_names, start_epoch=0, num_epochs=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the best parameters from JSON file\n",
    "    with open('../models/best_params_ensemble.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    best_model_params = params['best_model_params']\n",
    "    logging.info(f\"Loaded best model parameters: {best_model_params}\")\n",
    "\n",
    "    # Load the saved model\n",
    "    model_state = torch.load(\"../models/best_ensemble_model.pth\", map_location=device)\n",
    "    \n",
    "    # Recreate the model architecture using the loaded parameters\n",
    "    model = EnsembleModel(best_model_params, n_models=3).to(device)\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(model_state)\n",
    "    logging.info(\"Model loaded successfully\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    logging.info(\"Starting model evaluation\")\n",
    "    accuracy, kappa, predictions = evaluate_model(model, (X, X_spectral, y), device)\n",
    "    \n",
    "    logging.info(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    logging.info(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "    # Plot and save confusion matrices\n",
    "    fig_norm = plot_confusion_matrix(y.numpy(), predictions, normalize=True)\n",
    "    fig_norm.savefig('../images/confusion_matrix_normalized.png')\n",
    "    logging.info(\"Normalized confusion matrix saved to 'images' folder as 'confusion_matrix_normalized.png'\") \n",
    "    \n",
    "\n",
    "    fig_non_norm = plot_confusion_matrix(y.numpy(), predictions, normalize=False)\n",
    "    fig_non_norm.savefig('../images/confusion_matrix_non_normalized.png')\n",
    "    logging.info(\"Non-normalized confusion matrix saved to 'images' folder as 'confusion_matrix_non_normalized.png'\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    logging.error(f\"Error type: {type(e)}\")\n",
    "    logging.error(f\"Error args: {e.args}\")\n",
    "    # Optionally, print the full traceback\n",
    "    import traceback\n",
    "    logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def introduce_artifacts(data, labels, artifact_ratio, max_consecutive_artifacts):\n",
    "#     \"\"\"\n",
    "#     Introduce artifacts (zeros) into the data.\n",
    "    \n",
    "#     :param data: EEG data of shape (epochs, channels, time)\n",
    "#     :param labels: Corresponding labels\n",
    "#     :param artifact_ratio: Ratio of epochs to introduce artifacts\n",
    "#     :param max_consecutive_artifacts: Maximum number of consecutive artifacts\n",
    "#     :return: Data with artifacts, modified labels\n",
    "#     \"\"\"\n",
    "#     num_epochs, num_channels, time_steps = data.shape\n",
    "#     num_artifacts = int(num_epochs * artifact_ratio)\n",
    "    \n",
    "#     artifact_indices = np.random.choice(num_epochs, num_artifacts, replace=False)\n",
    "    \n",
    "#     for idx in artifact_indices:\n",
    "#         consecutive_artifacts = np.random.randint(1, max_consecutive_artifacts + 1)\n",
    "#         end_idx = min(idx + consecutive_artifacts, num_epochs)\n",
    "#         data[idx:end_idx] = 0\n",
    "#         labels[idx:end_idx] = -1  # Mark as artifact\n",
    "    \n",
    "#     return data, labels\n",
    "\n",
    "# def test_artifact_impact(model, X, X_spectral, y, device, max_ratio=0.5, steps=10, test_size=0.2):\n",
    "#     \"\"\"\n",
    "#     Test the impact of artifacts on model performance.\n",
    "    \n",
    "#     :param model: Trained model\n",
    "#     :param X: EEG data\n",
    "#     :param X_spectral: Spectral features\n",
    "#     :param y: Labels\n",
    "#     :param device: Computation device (CPU/GPU)\n",
    "#     :param max_ratio: Maximum ratio of artifacts to introduce\n",
    "#     :param steps: Number of steps to test\n",
    "#     :param test_size: Ratio of data to use for testing\n",
    "#     :return: Lists of artifact ratios, accuracies, and kappas\n",
    "#     \"\"\"\n",
    "#     X_train, X_test, X_spectral_train, X_spectral_test, y_train, y_test = train_test_split(\n",
    "#         X, X_spectral, y, test_size=test_size, stratify=y, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     artifact_ratios = np.linspace(0, max_ratio, steps)\n",
    "#     accuracies = []\n",
    "#     kappas = []\n",
    "    \n",
    "#     for ratio in artifact_ratios:\n",
    "#         X_test_artifact, y_test_artifact = introduce_artifacts(\n",
    "#             X_test.clone(), y_test.clone(), ratio, max_consecutive_artifacts=5\n",
    "#         )\n",
    "        \n",
    "#         # Remove epochs marked as artifacts\n",
    "#         valid_indices = y_test_artifact != -1\n",
    "#         X_test_valid = X_test_artifact[valid_indices]\n",
    "#         X_spectral_test_valid = X_spectral_test[valid_indices]\n",
    "#         y_test_valid = y_test_artifact[valid_indices]\n",
    "        \n",
    "#         accuracy, kappa, _ = evaluate_model(model, (X_test_valid, X_spectral_test_valid, y_test_valid), device)\n",
    "#         accuracies.append(accuracy)\n",
    "#         kappas.append(kappa)\n",
    "    \n",
    "#     return artifact_ratios, accuracies, kappas\n",
    "\n",
    "# def plot_artifact_impact(artifact_ratios, accuracies, kappas):\n",
    "#     \"\"\"\n",
    "#     Plot the impact of artifacts on model performance.\n",
    "    \n",
    "#     :param artifact_ratios: List of artifact ratios\n",
    "#     :param accuracies: List of accuracies\n",
    "#     :param kappas: List of kappa scores\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(artifact_ratios, accuracies, label='Accuracy', marker='o')\n",
    "#     plt.plot(artifact_ratios, kappas, label='Cohen\\'s Kappa', marker='s')\n",
    "#     plt.xlabel('Artifact Ratio')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title('Impact of Artifacts on Model Performance')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig('../images/artifact_impact.png')\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage\n",
    "# artifact_ratios, accuracies, kappas = test_artifact_impact(model, X, X_spectral, y, device)\n",
    "# plot_artifact_impact(artifact_ratios, accuracies, kappas)\n",
    "\n",
    "# # Print the results\n",
    "# for ratio, acc, kappa in zip(artifact_ratios, accuracies, kappas):\n",
    "#     print(f\"Artifact Ratio: {ratio:.2f}, Accuracy: {acc:.4f}, Kappa: {kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import torch\n",
    "\n",
    "def introduce_artifacts(data, artifact_ratio, max_consecutive_artifacts):\n",
    "    \"\"\"\n",
    "    Introduce artifacts (zeros) into the data without changing labels.\n",
    "    \n",
    "    :param data: EEG data of shape (epochs, channels, time)\n",
    "    :param artifact_ratio: Ratio of epochs to introduce artifacts\n",
    "    :param max_consecutive_artifacts: Maximum number of consecutive artifacts\n",
    "    :return: Data with artifacts\n",
    "    \"\"\"\n",
    "    num_epochs, num_channels, time_steps = data.shape\n",
    "    num_artifacts = int(num_epochs * artifact_ratio)\n",
    "    \n",
    "    artifact_indices = np.random.choice(num_epochs, num_artifacts, replace=False)\n",
    "    \n",
    "    data_with_artifacts = data.clone()\n",
    "    \n",
    "    for idx in artifact_indices:\n",
    "        consecutive_artifacts = np.random.randint(1, max_consecutive_artifacts + 1)\n",
    "        end_idx = min(idx + consecutive_artifacts, num_epochs)\n",
    "        data_with_artifacts[idx:end_idx] = 0\n",
    "    \n",
    "    return data_with_artifacts\n",
    "\n",
    "def evaluate_model_with_artifacts(model, X, X_spectral, y, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on data with artifacts.\n",
    "    \n",
    "    :param model: Trained model\n",
    "    :param X: EEG data\n",
    "    :param X_spectral: Spectral features\n",
    "    :param y: Labels\n",
    "    :param device: Computation device (CPU/GPU)\n",
    "    :return: Accuracy and Cohen's Kappa\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X.to(device), X_spectral.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true = y.cpu().numpy()\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        \n",
    "    return acc, kappa\n",
    "\n",
    "def test_artifact_impact(model, X, X_spectral, y, device, max_ratio=0.5, steps=10, max_consecutive_range=(1, 10)):\n",
    "    \"\"\"\n",
    "    Test the impact of artifacts on model performance.\n",
    "    \n",
    "    :param model: Trained model\n",
    "    :param X: EEG data\n",
    "    :param X_spectral: Spectral features\n",
    "    :param y: Labels\n",
    "    :param device: Computation device (CPU/GPU)\n",
    "    :param max_ratio: Maximum ratio of artifacts to introduce\n",
    "    :param steps: Number of steps to test for artifact ratio\n",
    "    :param max_consecutive_range: Range of maximum consecutive artifacts to test\n",
    "    :return: 2D lists of artifact ratios, max consecutive artifacts, accuracies, and kappas\n",
    "    \"\"\"\n",
    "    artifact_ratios = np.linspace(0, max_ratio, steps)\n",
    "    max_consecutive_artifacts = range(max_consecutive_range[0], max_consecutive_range[1] + 1)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for max_consec in max_consecutive_artifacts:\n",
    "        accuracies = []\n",
    "        kappas = []\n",
    "        for ratio in artifact_ratios:\n",
    "            X_artifact = introduce_artifacts(X, ratio, max_consec)\n",
    "            \n",
    "            # Recalculate spectral features for data with artifacts\n",
    "            X_spectral_artifact = torch.tensor(np.array([extract_spectral_features(x) for x in X_artifact]))\n",
    "            \n",
    "            acc, kappa = evaluate_model_with_artifacts(model, X_artifact, X_spectral_artifact, y, device)\n",
    "            accuracies.append(acc)\n",
    "            kappas.append(kappa)\n",
    "        \n",
    "        results.append((max_consec, artifact_ratios.tolist(), accuracies, kappas))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_artifact_impact(results):\n",
    "    \"\"\"\n",
    "    Plot the impact of artifacts on model performance.\n",
    "    \n",
    "    :param results: List of tuples (max_consec, artifact_ratios, accuracies, kappas)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for max_consec, artifact_ratios, accuracies, kappas in results:\n",
    "        plt.plot(artifact_ratios, accuracies, label=f'Accuracy (Max Consec: {max_consec})', marker='o')\n",
    "        plt.plot(artifact_ratios, kappas, label=f'Kappa (Max Consec: {max_consec})', marker='s', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Artifact Ratio')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Impact of Artifacts on Model Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('../images/artifact_impact_with_max_consec.png')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "results = test_artifact_impact(model, X, X_spectral, y, device, max_ratio=0.3, steps=10, max_consecutive_range=(3, 10))\n",
    "plot_artifact_impact(results)\n",
    "\n",
    "# Print the results\n",
    "for max_consec, artifact_ratios, accuracies, kappas in results:\n",
    "    print(f\"\\nMax Consecutive Artifacts: {max_consec}\")\n",
    "    for ratio, acc, kappa in zip(artifact_ratios, accuracies, kappas):\n",
    "        print(f\"  Artifact Ratio: {ratio:.2f}, Accuracy: {acc:.4f}, Kappa: {kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "# def introduce_artifacts(data, labels, artifact_ratio, max_consecutive_artifacts):\n",
    "#     \"\"\"\n",
    "#     Introduce artifacts (zeros) into the data without removing epochs.\n",
    "    \n",
    "#     :param data: EEG data of shape (epochs, channels, time)\n",
    "#     :param labels: Corresponding labels\n",
    "#     :param artifact_ratio: Ratio of epochs to introduce artifacts\n",
    "#     :param max_consecutive_artifacts: Maximum number of consecutive artifacts\n",
    "#     :return: Data with artifacts, modified labels\n",
    "#     \"\"\"\n",
    "#     num_epochs, num_channels, time_steps = data.shape\n",
    "#     num_artifacts = int(num_epochs * artifact_ratio)\n",
    "    \n",
    "#     artifact_indices = np.random.choice(num_epochs, num_artifacts, replace=False)\n",
    "    \n",
    "#     data_with_artifacts = data.clone()\n",
    "#     labels_with_artifacts = labels.clone()\n",
    "    \n",
    "#     for idx in artifact_indices:\n",
    "#         consecutive_artifacts = np.random.randint(1, max_consecutive_artifacts + 1)\n",
    "#         end_idx = min(idx + consecutive_artifacts, num_epochs)\n",
    "#         data_with_artifacts[idx:end_idx] = 0\n",
    "#         labels_with_artifacts[idx:end_idx] = -1  # Mark as artifact, but don't remove\n",
    "    \n",
    "#     return data_with_artifacts, labels_with_artifacts\n",
    "\n",
    "# def evaluate_model_with_artifacts(model, X, X_spectral, y, device):\n",
    "#     \"\"\"\n",
    "#     Evaluate the model, treating artifacts as a separate class.\n",
    "    \n",
    "#     :param model: Trained model\n",
    "#     :param X: EEG data\n",
    "#     :param X_spectral: Spectral features\n",
    "#     :param y: Labels\n",
    "#     :param device: Computation device (CPU/GPU)\n",
    "#     :return: Balanced accuracy and Cohen's Kappa\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(X.to(device), X_spectral.to(device))\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "#         # Treat artifacts (-1) as a separate class\n",
    "#         y_true = y.cpu().numpy()\n",
    "#         y_pred = predicted.cpu().numpy()\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "#         kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        \n",
    "#     return balanced_acc, kappa\n",
    "\n",
    "# def test_artifact_impact(model, X, X_spectral, y, device, max_ratio=0.5, steps=10, test_size=0.2):\n",
    "#     \"\"\"\n",
    "#     Test the impact of artifacts on model performance.\n",
    "    \n",
    "#     :param model: Trained model\n",
    "#     :param X: EEG data\n",
    "#     :param X_spectral: Spectral features\n",
    "#     :param y: Labels\n",
    "#     :param device: Computation device (CPU/GPU)\n",
    "#     :param max_ratio: Maximum ratio of artifacts to introduce\n",
    "#     :param steps: Number of steps to test\n",
    "#     :param test_size: Ratio of data to use for testing\n",
    "#     :return: Lists of artifact ratios, balanced accuracies, and kappas\n",
    "#     \"\"\"\n",
    "#     X_train, X_test, X_spectral_train, X_spectral_test, y_train, y_test = train_test_split(\n",
    "#         X, X_spectral, y, test_size=test_size, stratify=y, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     artifact_ratios = np.linspace(0, max_ratio, steps)\n",
    "#     balanced_accuracies = []\n",
    "#     kappas = []\n",
    "    \n",
    "#     for ratio in artifact_ratios:\n",
    "#         X_test_artifact, y_test_artifact = introduce_artifacts(\n",
    "#             X_test, y_test, ratio, max_consecutive_artifacts=5\n",
    "#         )\n",
    "        \n",
    "#         balanced_acc, kappa = evaluate_model_with_artifacts(model, X_test_artifact, X_spectral_test, y_test_artifact, device)\n",
    "#         balanced_accuracies.append(balanced_acc)\n",
    "#         kappas.append(kappa)\n",
    "    \n",
    "#     return artifact_ratios, balanced_accuracies, kappas\n",
    "\n",
    "# def plot_artifact_impact(artifact_ratios, balanced_accuracies, kappas):\n",
    "#     \"\"\"\n",
    "#     Plot the impact of artifacts on model performance.\n",
    "    \n",
    "#     :param artifact_ratios: List of artifact ratios\n",
    "#     :param balanced_accuracies: List of balanced accuracies\n",
    "#     :param kappas: List of kappa scores\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(artifact_ratios, balanced_accuracies, label='Balanced Accuracy', marker='o')\n",
    "#     plt.plot(artifact_ratios, kappas, label='Cohen\\'s Kappa', marker='s')\n",
    "#     plt.xlabel('Artifact Ratio')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title('Impact of Artifacts on Model Performance')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig('../images/artifact_impact_updated.png')\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage\n",
    "# artifact_ratios, balanced_accuracies, kappas = test_artifact_impact(model, X, X_spectral, y, device)\n",
    "# plot_artifact_impact(artifact_ratios, balanced_accuracies, kappas)\n",
    "\n",
    "# # Print the results\n",
    "# for ratio, acc, kappa in zip(artifact_ratios, balanced_accuracies, kappas):\n",
    "#     print(f\"Artifact Ratio: {ratio:.2f}, Balanced Accuracy: {acc:.4f}, Kappa: {kappa:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
