{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sleepdetector_new import ImprovedSleepdetector\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.signal import welch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import optuna\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_data(filepath, labels_file):\n",
    "    try:\n",
    "        mat_file = sio.loadmat(filepath)\n",
    "        x = np.stack((mat_file['sig1'], mat_file['sig2'], mat_file['sig3'], mat_file['sig4']), axis=0)\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y_true = sio.loadmat(labels_file)['labels'].flatten() - 1\n",
    "        y = torch.from_numpy(y_true).long()\n",
    "        return x.permute(1, 0, 2, 3).squeeze(-1), y\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_spectral_features(x):\n",
    "    features = []\n",
    "    for channel in x:\n",
    "        f, psd = welch(channel.squeeze().numpy(), fs=100, nperseg=1000)\n",
    "        delta = np.sum(psd[(f >= 0.5) & (f <= 4)])\n",
    "        theta = np.sum(psd[(f > 4) & (f <= 8)])\n",
    "        alpha = np.sum(psd[(f > 8) & (f <= 13)])\n",
    "        beta = np.sum(psd[(f > 13) & (f <= 30)])\n",
    "        features.extend([delta, theta, alpha, beta])\n",
    "    return np.array(features)\n",
    "\n",
    "def prepare_data(x, y, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=test_size, stratify=y, random_state=42)\n",
    "    \n",
    "    X_train_spectral = np.array([extract_spectral_features(torch.from_numpy(x)) for x in X_train])\n",
    "    X_test_spectral = np.array([extract_spectral_features(torch.from_numpy(x)) for x in X_test])\n",
    "    \n",
    "    X_train_combined = np.concatenate([X_train.reshape(X_train.shape[0], -1), X_train_spectral], axis=1)\n",
    "    X_test_combined = np.concatenate([X_test.reshape(X_test.shape[0], -1), X_test_spectral], axis=1)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
    "    \n",
    "    original_shape = list(X_train.shape)\n",
    "    original_shape[0] = X_train_resampled.shape[0]\n",
    "    spectral_shape = (X_train_resampled.shape[0], X_train_spectral.shape[1])\n",
    "    \n",
    "    X_train_final = X_train_resampled[:, :-X_train_spectral.shape[1]].reshape(original_shape)\n",
    "    X_train_spectral_final = X_train_resampled[:, -X_train_spectral.shape[1]:].reshape(spectral_shape)\n",
    "    \n",
    "    return (torch.from_numpy(X_train_final).float(),\n",
    "            torch.from_numpy(X_train_spectral_final).float(),\n",
    "            torch.from_numpy(y_train_resampled).long(),\n",
    "            torch.from_numpy(X_test).float(),\n",
    "            torch.from_numpy(X_test_spectral).float(),\n",
    "            torch.from_numpy(y_test).long())\n",
    "\n",
    "# Model definition\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([ImprovedSleepdetector(**model_params) for _ in range(n_models)])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100):\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        for batch_x, batch_x_spectral, batch_y in train_loader:\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_spectral)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        accuracy = evaluate_model(model, val_data, device)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "        \n",
    "        scheduler.step(accuracy)\n",
    "    \n",
    "    return best_model_state, best_accuracy\n",
    "\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()\n",
    "    X, X_spectral, y = data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X.to(device), X_spectral.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return accuracy\n",
    "\n",
    "# Hyperparameter optimization\n",
    "def objective(trial, X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test, device):\n",
    "    model_params = {\n",
    "        'n_filters': trial.suggest_categorical('n_filters', [[32, 64, 128], [64, 128, 256]]),\n",
    "        'lstm_hidden': trial.suggest_int('lstm_hidden', 64, 512),\n",
    "        'lstm_layers': trial.suggest_int('lstm_layers', 1, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    }\n",
    "    \n",
    "    train_params = {\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    }\n",
    "    \n",
    "    model = ImprovedSleepdetector(**model_params).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "    train_loader = DataLoader(TensorDataset(X_train, X_train_spectral, y_train), batch_size=train_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    _, accuracy = train_model(model, train_loader, (X_test, X_test_spectral, y_test), optimizer, ReduceLROnPlateau(optimizer), nn.CrossEntropyLoss(), device, epochs=10)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Cross-validation\n",
    "def cross_validate(X, X_spectral, y, model_params, train_params, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        X_spectral_train_fold, X_spectral_val_fold = X_spectral[train_idx], X_spectral[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = ImprovedSleepdetector(**model_params).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "        train_loader = DataLoader(TensorDataset(X_train_fold, X_spectral_train_fold, y_train_fold), batch_size=train_params['batch_size'], shuffle=True)\n",
    "        \n",
    "        _, accuracy = train_model(model, train_loader, (X_val_fold, X_spectral_val_fold, y_val_fold), optimizer, ReduceLROnPlateau(optimizer), nn.CrossEntropyLoss(), device, epochs=50)\n",
    "        scores.append(accuracy)\n",
    "        \n",
    "        logging.info(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    logging.info(f\"Average Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    return scores\n",
    "\n",
    "# Confusion matrix plotting\n",
    "def plot_confusion_matrix(y_true, y_pred, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Class names in the correct order (0 to 4)\n",
    "    class_names = ['N3', 'N2', 'N1', 'REM', 'Awake']\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',\n",
    "                cmap=cmap, square=True, xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1027, 4, 3000]) torch.Size([1027])\n"
     ]
    }
   ],
   "source": [
    "x, y = load_data('./data/data.mat', './data/labels.mat')\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a list of steps\n",
    "    steps = [\n",
    "        \"Load and prepare data\",\n",
    "        \"Hyperparameter optimization\",\n",
    "        \"Cross-validation\",\n",
    "        \"Train final model\",\n",
    "        \"Save best model\",\n",
    "        \"Final evaluation\"\n",
    "    ]\n",
    "    \n",
    "    # Create the progress bar\n",
    "    pbar = tqdm(total=len(steps), desc=\"Overall Progress\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    x, y = load_data('../data/data.mat', '../data/labels.mat')\n",
    "    X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test = prepare_data(x, y)\n",
    "    logging.info(\"Data loaded and prepared successfully\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Save test data\n",
    "    \n",
    "    # Convert tensors to lists\n",
    "    X_test_list = X_test.tolist()\n",
    "    X_test_spectral_list = X_test_spectral.tolist()\n",
    "    y_test_list = y_test.tolist()\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    data = {\n",
    "        'X_test': X_test_list,\n",
    "        'X_test_spectral': X_test_spectral_list,\n",
    "        'y_test': y_test_list\n",
    "    }\n",
    "\n",
    "    # Save to a JSON file\n",
    "    with open('test_data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    logging.info(\"Test data saved successfully\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test, device), n_trials=100)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_model_params = {k: v for k, v in best_params.items() if k in ['n_filters', 'lstm_hidden', 'lstm_layers', 'dropout']}\n",
    "    best_train_params = {k: v for k, v in best_params.items() if k in ['lr', 'batch_size']}\n",
    "    logging.info(f\"Best hyperparameters: {best_params}\")\n",
    "    \n",
    "    # Save the parameters\n",
    "    params_to_save = {\n",
    "        'best_params': best_params,\n",
    "        'best_model_params': best_model_params,\n",
    "        'best_train_params': best_train_params\n",
    "    }\n",
    "\n",
    "    with open('best_params_ensemble.json', 'w') as f:\n",
    "        json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "    logging.info(\"Best parameters saved to 'best_params.json'\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_validate(X_train, X_train_spectral, y_train, best_model_params, best_train_params)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Train final model (ensemble)\n",
    "    ensemble_model = EnsembleModel(best_model_params, n_models=3).to(device)\n",
    "    train_loader = DataLoader(TensorDataset(X_train, X_train_spectral, y_train), batch_size=best_train_params['batch_size'], shuffle=True)\n",
    "    optimizer = optim.Adam(ensemble_model.parameters(), lr=best_train_params['lr'], weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "    \n",
    "    best_model_state, _ = train_model(ensemble_model, train_loader, (X_test, X_test_spectral, y_test), optimizer, scheduler, nn.CrossEntropyLoss(), device)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Save best model\n",
    "    torch.save(best_model_state, \"best_ensemble_model.pth\")\n",
    "    \n",
    "\n",
    "    logging.info(\"Best ensemble model saved\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "ensemble_model.load_state_dict(best_model_state)\n",
    "final_accuracy = evaluate_model(ensemble_model, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "ensemble_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = ensemble_model(X_test.to(device), X_test_spectral.to(device))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    final_kappa = cohen_kappa_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "logging.info(f\"Ensemble Model - Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "logging.info(f\"Ensemble Model - Final Cohen's Kappa: {final_kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save normalized confusion matrix\n",
    "fig_norm = plot_confusion_matrix(y_test.cpu().numpy(), predicted.cpu().numpy(), \n",
    "                                    normalize=True, \n",
    "                                    title='Normalized Confusion Matrix')\n",
    "# fig_norm.savefig('confusion_matrix_normalized.png')\n",
    "logging.info(\"Normalized confusion matrix plot saved as 'confusion_matrix_normalized.png'\")\n",
    "\n",
    "# Plot and save non-normalized confusion matrix\n",
    "fig_non_norm = plot_confusion_matrix(y_test.cpu().numpy(), predicted.cpu().numpy(), \n",
    "                                        normalize=False, \n",
    "                                        title='Confusion Matrix, without normalization')\n",
    "# fig_non_norm.savefig('confusion_matrix_non_normalized.png')\n",
    "logging.info(\"Non-normalized confusion matrix plot saved as 'confusion_matrix_non_normalized.png'\")\n",
    "\n",
    "# plt.close('all')  # Close all figures to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
