{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sleepdetector_new import ImprovedSleepdetector\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.signal import welch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import optuna\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "from scipy.interpolate import CubicSpline\n",
    "from torch_lr_finder import LRFinder\n",
    "import torch.nn.functional as F\n",
    "# from sleepdetector_newest import ImprovedSleepdetector\n",
    "from sleepdetector_new import ImprovedSleepdetector\n",
    "# from sleepdetector_old import ImprovedSleepdetector\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, LinearLR\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([ImprovedSleepdetector(**model_params) for _ in range(n_models)])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "class DiverseEnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([\n",
    "            ImprovedSleepdetector(**{**model_params, 'dropout': model_params['dropout'] * (i+1)/n_models})\n",
    "            for i in range(n_models)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_params(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    return params['best_model_params']\n",
    "\n",
    "\n",
    "\n",
    "def load_data_and_params(config):\n",
    "    data_dict = torch.load(config['preprocessed_data_path'])\n",
    "    best_params_path = os.path.join(config['previous_model_path'], config['best_params_name'])\n",
    "    best_params = load_best_params(best_params_path)\n",
    "    return data_dict, best_params\n",
    "\n",
    "\n",
    "def print_model_structure(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "\n",
    "def load_data(filepath, add_dim=False):\n",
    "    try:\n",
    "        # Load the data from the .mat file\n",
    "        mat_file = sio.loadmat(filepath)\n",
    "        \n",
    "        # Stack the signals into x\n",
    "        x = np.stack((mat_file['sig1'], mat_file['sig2'], mat_file['sig3'], mat_file['sig4']), axis=1)\n",
    "        x = torch.from_numpy(x).float()  # Convert to PyTorch tensor\n",
    "        \n",
    "        # Load the labels\n",
    "        y = torch.from_numpy(mat_file['labels'].flatten()).long()\n",
    "        \n",
    "        # Remove epochs where y is -1 (if any)\n",
    "        valid_indices = y != -1\n",
    "        x = x[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "        \n",
    "        # Ensure x is in shape [number of epochs, 4, 3000]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        if add_dim:\n",
    "            x = x.unsqueeze(1)  # Add an extra dimension if required\n",
    "        \n",
    "        print(f\"Loaded data shape: {x.shape}, Labels shape: {y.shape}\")\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# def extract_spectral_features(x):\n",
    "#     features = []\n",
    "#     for epoch in x:\n",
    "#         epoch_features = []\n",
    "#         for channel in epoch:\n",
    "#             # Check if channel is a PyTorch tensor, if so convert to numpy array\n",
    "#             if isinstance(channel, torch.Tensor):\n",
    "#                 channel = channel.numpy()\n",
    "#             f, psd = welch(channel, fs=100, nperseg=1000)\n",
    "#             delta = np.sum(psd[(f >= 0.5) & (f <= 4)])\n",
    "#             theta = np.sum(psd[(f > 4) & (f <= 8)])\n",
    "#             alpha = np.sum(psd[(f > 8) & (f <= 13)])\n",
    "#             beta = np.sum(psd[(f > 13) & (f <= 30)])\n",
    "#             epoch_features.extend([delta, theta, alpha, beta])\n",
    "#         features.append(epoch_features)\n",
    "#     return np.array(features)\n",
    "\n",
    "def extract_spectral_features(x):\n",
    "    features = []\n",
    "    for channel in range(x.shape[0]):  # Iterate over channels\n",
    "        # Convert to NumPy array for scipy.signal.welch\n",
    "        channel_data = x[channel].cpu().numpy()\n",
    "        f, psd = welch(channel_data, fs=100, nperseg=min(1000, len(channel_data)))\n",
    "        delta = np.sum(psd[(f >= 0.5) & (f <= 4)])\n",
    "        theta = np.sum(psd[(f > 4) & (f <= 8)])\n",
    "        alpha = np.sum(psd[(f > 8) & (f <= 13)])\n",
    "        beta = np.sum(psd[(f > 13) & (f <= 30)])\n",
    "        features.extend([delta, theta, alpha, beta])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            time_warp = CubicSpline(warp_steps[:, dim], warp_steps[:, dim] * random_warps[i, :, dim])(orig_steps)\n",
    "            scale = (x.shape[1]-1)/time_warp[-1]\n",
    "            ret[i, :, dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:, dim]).T\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def augment_minority_classes(x, x_spectral, y, minority_classes):\n",
    "    augmented_x = []\n",
    "    augmented_x_spectral = []\n",
    "    augmented_y = []\n",
    "    for i in range(len(y)):\n",
    "        augmented_x.append(x[i])\n",
    "        augmented_x_spectral.append(x_spectral[i])\n",
    "        augmented_y.append(y[i])\n",
    "        if y[i] in minority_classes:\n",
    "            # Apply time_warp augmentation\n",
    "            augmented = torch.from_numpy(time_warp(x[i].unsqueeze(0).numpy(), sigma=0.3, knot=5)).squeeze(0)\n",
    "            augmented_x.append(augmented)\n",
    "            augmented_x_spectral.append(x_spectral[i])  # Duplicate spectral features for augmented data\n",
    "            augmented_y.append(y[i])\n",
    "    return torch.stack(augmented_x), torch.stack(augmented_x_spectral), torch.tensor(augmented_y)\n",
    "\n",
    "\n",
    "# def prepare_data(x, y, test_size=0.2, split=True):\n",
    "#     \"\"\"\n",
    "#     Prepare data for training or testing.\n",
    "    \n",
    "#     :param x: Input data tensor\n",
    "#     :param y: Labels tensor\n",
    "#     :param test_size: Proportion of the dataset to include in the test split\n",
    "#     :param split: If True, split the data into train and test sets. If False, process all data without splitting.\n",
    "#     :return: Processed data tensors\n",
    "#     \"\"\"\n",
    "#     if split:\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=test_size, stratify=y, random_state=42)\n",
    "        \n",
    "#         X_train_spectral = extract_spectral_features(torch.from_numpy(X_train))\n",
    "#         X_test_spectral = extract_spectral_features(torch.from_numpy(X_test))\n",
    "        \n",
    "#         X_train_combined = np.concatenate([X_train.reshape(X_train.shape[0], -1), X_train_spectral], axis=1)\n",
    "#         X_test_combined = np.concatenate([X_test.reshape(X_test.shape[0], -1), X_test_spectral], axis=1)\n",
    "        \n",
    "#         smote = SMOTE(random_state=42)\n",
    "#         X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
    "        \n",
    "#         original_shape = list(X_train.shape)\n",
    "#         original_shape[0] = X_train_resampled.shape[0]\n",
    "#         spectral_shape = (X_train_resampled.shape[0], X_train_spectral.shape[1])\n",
    "        \n",
    "#         X_train_final = X_train_resampled[:, :-X_train_spectral.shape[1]].reshape(original_shape)\n",
    "#         X_train_spectral_final = X_train_resampled[:, -X_train_spectral.shape[1]:].reshape(spectral_shape)\n",
    "        \n",
    "#         return (torch.from_numpy(X_train_final).float(),\n",
    "#                 torch.from_numpy(X_train_spectral_final).float(),\n",
    "#                 torch.from_numpy(y_train_resampled).long(),\n",
    "#                 torch.from_numpy(X_test).float(),\n",
    "#                 torch.from_numpy(X_test_spectral).float(),\n",
    "#                 torch.from_numpy(y_test).long())\n",
    "#     else:\n",
    "#         X_spectral = extract_spectral_features(x)\n",
    "        \n",
    "#         return (x.float(),\n",
    "#                 torch.from_numpy(X_spectral).float(),\n",
    "#                 y.long())\n",
    "    \n",
    "def prepare_data(x, y, test_size=0.2):\n",
    "    # Convert PyTorch tensors to NumPy arrays for scikit-learn and SMOTE\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_np, y_np, test_size=test_size, stratify=y_np, random_state=42)\n",
    "    \n",
    "    # Convert back to PyTorch tensors for spectral feature extraction\n",
    "    X_train_torch = torch.from_numpy(X_train).float()\n",
    "    X_test_torch = torch.from_numpy(X_test).float()\n",
    "\n",
    "    # Extract spectral features\n",
    "    X_train_spectral = np.array([extract_spectral_features(x) for x in X_train_torch])\n",
    "    X_test_spectral = np.array([extract_spectral_features(x) for x in X_test_torch])\n",
    "    \n",
    "    print(\"Original train set class distribution:\")\n",
    "    print(Counter(y_train))\n",
    "    \n",
    "    # Reshape the data for SMOTE\n",
    "    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_train_spectral_reshaped = X_train_spectral.reshape(X_train_spectral.shape[0], -1)\n",
    "    X_combined = np.hstack((X_train_reshaped, X_train_spectral_reshaped))\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_combined, y_train)\n",
    "    \n",
    "    print(\"After SMOTE train set class distribution:\")\n",
    "    print(Counter(y_resampled))\n",
    "    \n",
    "    # Reshape the resampled data back to the original shape\n",
    "    X_train_resampled = X_resampled[:, :X_train_reshaped.shape[1]].reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "    X_train_spectral_resampled = X_resampled[:, X_train_reshaped.shape[1]:].reshape(-1, X_train_spectral.shape[1])\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.from_numpy(X_train_resampled).float()\n",
    "    X_train_spectral = torch.from_numpy(X_train_spectral_resampled).float()\n",
    "    y_train = torch.from_numpy(y_resampled).long()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    X_test_spectral = torch.from_numpy(X_test_spectral).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    \n",
    "    return X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, num_warmup_steps, num_training_steps, min_lr=1e-6):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            min_lr,\n",
    "            float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def initialize_model(config, best_model_params, device):\n",
    "    ensemble_model = EnsembleModel(best_model_params, n_models=3).to(device)\n",
    "    \n",
    "    if config['use_pretrained']:\n",
    "        if os.path.exists(config['pretrained_weights_path']):\n",
    "            # Load the state dict\n",
    "            state_dict = torch.load(config['pretrained_weights_path'], map_location=device)\n",
    "            \n",
    "            # Filter out unnecessary keys\n",
    "            model_dict = ensemble_model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "            \n",
    "            # Update the model dict\n",
    "            model_dict.update(pretrained_dict)\n",
    "            \n",
    "            # Load the filtered state dict\n",
    "            ensemble_model.load_state_dict(model_dict, strict=False)\n",
    "            \n",
    "            # Log the loaded and missing keys\n",
    "            loaded_keys = set(pretrained_dict.keys())\n",
    "            all_keys = set(model_dict.keys())\n",
    "            missing_keys = all_keys - loaded_keys\n",
    "            \n",
    "            logging.info(f\"Loaded pre-trained weights from {config['pretrained_weights_path']}\")\n",
    "            logging.info(f\"Number of loaded parameters: {len(loaded_keys)}\")\n",
    "            logging.info(f\"Number of missing parameters: {len(missing_keys)}\")\n",
    "            if missing_keys:\n",
    "                logging.warning(f\"Missing keys: {missing_keys}\")\n",
    "        else:\n",
    "            logging.warning(f\"Pre-trained weights file not found at {config['pretrained_weights_path']}. Initializing with random weights.\")\n",
    "    else:\n",
    "        logging.info(\"Initializing with random weights.\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "def load_params_and_initialize_model(config, device):\n",
    "    params_path = os.path.join(config['new_model_path'], config['best_params_name'])\n",
    "    \n",
    "    try:\n",
    "        with open(params_path, 'r') as f:\n",
    "            loaded_params = json.load(f)\n",
    "        \n",
    "        best_model_params = loaded_params['best_model_params']\n",
    "        best_train_params = loaded_params['best_train_params']\n",
    "        \n",
    "        print(\"Parameters loaded successfully.\")\n",
    "        print(f\"Best model parameters: {best_model_params}\")\n",
    "        print(f\"Best training parameters: {best_train_params}\")\n",
    "        \n",
    "        ensemble_model = initialize_model(config, best_model_params, device)\n",
    "        \n",
    "        return ensemble_model, best_model_params, best_train_params\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {params_path} was not found.\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {params_path} is not a valid JSON file.\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: The key {e} was not found in the loaded parameters.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def find_lr(model, train_loader, optimizer, criterion, device, num_iter=100, start_lr=1e-8, end_lr=1):\n",
    "    logging.info(\"Starting learning rate finder...\")\n",
    "    model.train()\n",
    "    num_samples = len(train_loader.dataset)\n",
    "    update_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "    lr = start_lr\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    running_loss = 0\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    \n",
    "    progress_bar = tqdm(range(num_iter), desc=\"Finding best LR\")\n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            inputs, spectral_features, targets = next(iter(train_loader))\n",
    "        except StopIteration:\n",
    "            train_loader = iter(train_loader)\n",
    "            inputs, spectral_features, targets = next(train_loader)\n",
    "        \n",
    "        inputs, spectral_features, targets = inputs.to(device), spectral_features.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        batch_num += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, spectral_features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Compute the smoothed loss\n",
    "        running_loss = 0.98 * running_loss + 0.02 * loss.item()\n",
    "        smoothed_loss = running_loss / (1 - 0.98**batch_num)\n",
    "        \n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            logging.info(f\"Loss is exploding, stopping early at lr={lr:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{smoothed_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "    plt.xlabel(\"Log Learning Rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Rate vs. Loss\")\n",
    "    plt.savefig('lr_finder_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Find the learning rate with the steepest negative gradient\n",
    "    smoothed_losses = np.array(losses[10:-5])\n",
    "    smoothed_lrs = np.array(log_lrs[10:-5])\n",
    "    gradients = (smoothed_losses[1:] - smoothed_losses[:-1]) / (smoothed_lrs[1:] - smoothed_lrs[:-1])\n",
    "    best_lr = 10 ** smoothed_lrs[np.argmin(gradients)]\n",
    "    \n",
    "    # Adjust the learning rate to be slightly lower than the one with steepest gradient\n",
    "    best_lr *= 0.1\n",
    "    \n",
    "    logging.info(f\"Learning rate finder completed. Suggested Learning Rate: {best_lr:.2e}\")\n",
    "    logging.info(\"Learning rate vs. loss plot saved as 'lr_finder_plot.png'\")\n",
    "    return best_lr\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    class_counts = torch.bincount(y)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100):\n",
    "#     best_accuracy = 0\n",
    "#     best_model_state = None\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for batch_x, batch_x_spectral, batch_y in train_loader:\n",
    "#             batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(batch_x, batch_x_spectral)\n",
    "#             loss = criterion(outputs, batch_y)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         accuracy = evaluate_model(model, val_data, device)\n",
    "        \n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_model_state = model.state_dict()\n",
    "        \n",
    "#         scheduler.step(accuracy)\n",
    "        \n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             logging.info(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     return best_model_state, best_accuracy\n",
    "\n",
    "# def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100):\n",
    "#     best_accuracy = 0\n",
    "#     best_model_state = None\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for batch_x, batch_x_spectral, batch_y in train_loader:\n",
    "#             batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(batch_x, batch_x_spectral)\n",
    "#             loss = criterion(outputs, batch_y)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()  # Step the scheduler after each batch for OneCycleLR\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         # Evaluate every epoch\n",
    "#         accuracy = evaluate_model(model, val_data, device)\n",
    "        \n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_model_state = model.state_dict()\n",
    "#             torch.save(best_model_state, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        \n",
    "#         logging.info(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     return best_model_state, best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100):\n",
    "    scaler = GradScaler()\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_x_spectral, batch_y in train_loader:\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=device.type):\n",
    "                outputs = model(batch_x, batch_x_spectral)\n",
    "                \n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(\"NaNs detected in model outputs!\")\n",
    "                    print(f\"batch_x range: {batch_x.min().item()} to {batch_x.max().item()}\")\n",
    "                    print(f\"batch_x_spectral range: {batch_x_spectral.min().item()} to {batch_x_spectral.max().item()}\")\n",
    "                    print(f\"Model parameters:\")\n",
    "                    for name, param in model.named_parameters():\n",
    "                        print(f\"{name}: {param.data.min().item()} to {param.data.max().item()}\")\n",
    "                    return None, 0\n",
    "                \n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            \n",
    "            if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "                print(f\"Gradient norm is NaN or Inf: {grad_norm}\")\n",
    "                return None, 0\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        accuracy = evaluate_model(model, val_data, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Grad norm: {grad_norm:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "    \n",
    "    return best_model_state, best_accuracy\n",
    "\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()\n",
    "    X, X_spectral, y = data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X.to(device), X_spectral.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return accuracy\n",
    "\n",
    "def distill_knowledge(teacher_model, student_model, train_loader, val_data, device, num_epochs=50, log_interval=5):\n",
    "    optimizer = optim.AdamW(student_model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "    scheduler = get_scheduler(optimizer, num_warmup_steps=len(train_loader) * 5, num_training_steps=len(train_loader) * num_epochs)\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    temperature = 2.0  # Make sure this value is reasonable\n",
    "\n",
    "    teacher_model.eval()\n",
    "    overall_progress = tqdm(total=num_epochs, desc=\"Overall Distillation Progress\", position=0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        epoch_progress = tqdm(train_loader, desc=f\"Distillation Epoch {epoch+1}/{num_epochs}\", position=1, leave=False)\n",
    "        for batch_x, batch_x_spectral, batch_y in epoch_progress:\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "\n",
    "            # Check for NaNs or Infs in input data\n",
    "            if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():\n",
    "                print(\"NaNs or Infs detected in input data!\")\n",
    "            \n",
    "            if torch.isnan(batch_x_spectral).any() or torch.isinf(batch_x_spectral).any():\n",
    "                print(\"NaNs or Infs detected in spectral input data!\")\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(batch_x, batch_x_spectral)\n",
    "            \n",
    "            student_outputs = student_model(batch_x, batch_x_spectral)\n",
    "            \n",
    "\n",
    "            epsilon = 1e-8  # Small constant to prevent log(0)\n",
    "            loss = criterion(\n",
    "                F.log_softmax(student_outputs / temperature + epsilon, dim=1),\n",
    "                F.softmax(teacher_outputs / temperature + epsilon, dim=1)\n",
    "            )\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            epoch_progress.set_postfix({'loss': f'{running_loss/(epoch_progress.n+1):.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
    "        \n",
    "        # Evaluate and log every log_interval epochs\n",
    "        if (epoch + 1) % log_interval == 0 or epoch == num_epochs - 1:\n",
    "            accuracy = evaluate_model(student_model, val_data, device)\n",
    "            logging.info(f\"Distillation Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        overall_progress.update(1)\n",
    "    \n",
    "    overall_progress.close()\n",
    "    return student_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_file_name = './preprocessing/preprocessed_data.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "x, y = load_data(preprocessed_file_name)\n",
    "print(f\"Loaded data shape: {x.shape}, Labels shape: {y.shape}\")\n",
    "\n",
    "# Prepare the data (includes SMOTE)\n",
    "X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test = prepare_data(x, y)\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_train_spectral shape: {X_train_spectral.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Identify minority classes for augmentation\n",
    "class_counts = Counter(y_train.numpy())\n",
    "minority_classes = [cls for cls, count in class_counts.items() if count < len(y_train) / len(class_counts) * 0.5]\n",
    "\n",
    "# Apply augmentation\n",
    "X_train, X_train_spectral, y_train = augment_minority_classes(X_train, X_train_spectral, y_train, minority_classes)\n",
    "\n",
    "print(\"After augmentation:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_train_spectral shape: {X_train_spectral.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(\"Final class distribution:\")\n",
    "print(Counter(y_train.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'previous_model_path': './models/original/',\n",
    "    'new_model_path': './models/new/',\n",
    "    'best_model_name': 'best_ensemble_model.pth',\n",
    "    'best_params_name': 'best_params_ensemble.json',\n",
    "    'test_data_name': 'test_data.json',\n",
    "    'confusion_matrix_norm': 'confusion_matrix_normalized.png',\n",
    "    'confusion_matrix_non_norm': 'confusion_matrix_non_normalized.png',\n",
    "    # 'initial_weights_name': 'best_ensemble_model.pth',\n",
    "    'initial_weights_name': 'best_ensemble_model.pth',\n",
    "    'use_pretrained': False,  # Set to True to use previous weights\n",
    "}\n",
    "\n",
    "# Ensure the model save directory exists\n",
    "\n",
    "os.makedirs(config['new_model_path'], exist_ok=True)\n",
    "config['pretrained_weights_path'] = os.path.join(config['previous_model_path'], config['best_model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model, best_model_params, best_train_params = load_params_and_initialize_model(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_model_structure(model):\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# # Before loading weights\n",
    "# print(\"Current model structure:\")\n",
    "# print_model_structure(ensemble_model)\n",
    "\n",
    "# # After loading weights\n",
    "# print(\"\\nLoaded model structure:\")\n",
    "# state_dict = torch.load(config['pretrained_weights_path'], map_location=device)\n",
    "# for name, param in state_dict.items():\n",
    "#     print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the new model save directory exists\n",
    "# os.makedirs(config['new_model_path'], exist_ok=True)\n",
    "\n",
    "# # Set the full path for the pretrained weights\n",
    "# config['pretrained_weights_path'] = os.path.join(config['previous_model_path'], config['best_model_name'])\n",
    "\n",
    "# # Save test data\n",
    "# test_data = {\n",
    "#     'X_test': X_test.tolist(),\n",
    "#     'X_test_spectral': X_test_spectral.tolist(),\n",
    "#     'y_test': y_test.tolist()\n",
    "# }\n",
    "\n",
    "# with open(os.path.join(config['new_model_path'], config['test_data_name']), 'w') as f:\n",
    "#     json.dump(test_data, f)\n",
    "\n",
    "# logging.info(\"Test data saved successfully\")\n",
    "\n",
    "# # Hyperparameter optimization\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# try:\n",
    "#     study.optimize(lambda trial: objective(trial, X_train, X_train_spectral, y_train, X_test, X_test_spectral, y_test, device), \n",
    "#                n_trials=100, \n",
    "#                callbacks=[OptunaPruneCallback()],\n",
    "#                show_progress_bar=True)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"An error occurred during optimization: {e}\")\n",
    "#     raise\n",
    "\n",
    "# logging.info(f\"Best trial: {study.best_trial.number}\")\n",
    "# logging.info(f\"Best value: {study.best_value:.4f}\")\n",
    "\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_model_params = {k: v for k, v in best_params.items() if k in ['n_filters', 'lstm_hidden', 'lstm_layers', 'dropout']}\n",
    "# best_train_params = {k: v for k, v in best_params.items() if k in ['lr', 'batch_size']}\n",
    "\n",
    "# # Save the best parameters\n",
    "# params_to_save = {\n",
    "#     'best_params': best_params,\n",
    "#     'best_model_params': best_model_params,\n",
    "#     'best_train_params': best_train_params\n",
    "# }\n",
    "\n",
    "# with open(os.path.join(config['new_model_path'], config['best_params_name']), 'w') as f:\n",
    "#     json.dump(params_to_save, f, indent=4)\n",
    "\n",
    "# logging.info(f\"Best parameters saved to {os.path.join(config['new_model_path'], config['best_params_name'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.label_to_indices = {label: np.where(labels == label)[0] for label in set(labels)}\n",
    "        self.used_label_indices_count = {label: 0 for label in set(labels)}\n",
    "        self.count = 0\n",
    "        self.n_classes = len(set(labels))\n",
    "        self.n_samples = len(labels)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_samples:\n",
    "            classes = list(self.label_to_indices.keys())\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                    self.used_label_indices_count[class_]:self.used_label_indices_count[class_] + self.batch_size // self.n_classes\n",
    "                ])\n",
    "                self.used_label_indices_count[class_] += self.batch_size // self.n_classes\n",
    "                if self.used_label_indices_count[class_] + self.batch_size // self.n_classes > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting training process...\")\n",
    "overall_steps = 4  # LR finding, Ensemble training, Diverse Ensemble training, Knowledge Distillation\n",
    "overall_progress = tqdm(total=overall_steps, desc=\"Overall Training Progress\", position=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_sampler = BalancedBatchSampler(y_train.numpy(), batch_size=best_train_params['batch_size'])\n",
    "train_loader = DataLoader(TensorDataset(X_train, X_train_spectral, y_train), batch_sampler=balanced_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss function\n",
    "class_weights = get_class_weights(y_train).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights + 1e-6)\n",
    "\n",
    "# Find best learning rate\n",
    "initial_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_train_params['lr'], weight_decay=1e-5)\n",
    "logging.info(\"Finding best learning rate...\")\n",
    "# best_lr = find_lr(ensemble_model, train_loader, initial_optimizer, criterion, device, num_iter=100, start_lr=best_train_params['lr'], end_lr=1)\n",
    "overall_progress.update(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not torch.isnan(X_train).any(), \"NaN values found in X_train\"\n",
    "assert not torch.isinf(X_train).any(), \"Inf values found in X_train\"\n",
    "assert not torch.isnan(X_train_spectral).any(), \"NaN values found in X_train_spectral\"\n",
    "assert not torch.isinf(X_train_spectral).any(), \"Inf values found in X_train_spectral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # Adjust as needed\n",
    "num_warmup_steps = len(train_loader) * 5  # 5 epochs of warmup\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Set up optimizer and scheduler with best learning rate\n",
    "# best_lr = min(best_lr, best_train_params['lr']) * 0.1  # Reduce the LR slightly\n",
    "best_lr = best_train_params['lr'] * 0.1  # Reduce the LR slightly\n",
    "\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=best_lr, weight_decay=1e-5)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=best_lr, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "# Train model\n",
    "logging.info(\"Training ensemble model...\")\n",
    "best_model_state, best_accuracy = train_model(\n",
    "    ensemble_model, train_loader, (X_test, X_test_spectral, y_test),\n",
    "    optimizer, scheduler, criterion, device, epochs=num_epochs\n",
    ")\n",
    "overall_progress.update(1)\n",
    "\n",
    "# Save best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, os.path.join(config['new_model_path'], config['best_model_name']))\n",
    "    logging.info(f\"Best ensemble model saved. Final accuracy: {best_accuracy:.4f}\")\n",
    "else:\n",
    "    logging.error(\"Training failed due to NaN loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "ensemble_model.load_state_dict(best_model_state)\n",
    "final_accuracy = evaluate_model(ensemble_model, (X_test, X_test_spectral, y_test), device)\n",
    "logging.info(f\"Ensemble Model - Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "overall_progress.update(1)\n",
    "overall_progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diverse Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train diverse ensemble\n",
    "# diverse_ensemble = DiverseEnsembleModel(best_params).to(device)\n",
    "# diverse_optimizer = optim.AdamW(diverse_ensemble.parameters(), lr=best_lr, weight_decay=1e-2)\n",
    "# diverse_scheduler = get_scheduler(diverse_optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "\n",
    "# logging.info(\"Training diverse ensemble model...\")\n",
    "# diverse_best_state, diverse_accuracy = train_model(\n",
    "#     diverse_ensemble, train_loader, (X_test, X_test_spectral, y_test),\n",
    "#     diverse_optimizer, diverse_scheduler, criterion, device, epochs=num_epochs\n",
    "# )\n",
    "# overall_progress.update(1)\n",
    "\n",
    "\n",
    "# torch.save(diverse_best_state, os.path.join(config['new_model_path'], 'best_diverse_ensemble_model.pth'))\n",
    "# logging.info(f\"Best diverse ensemble model saved. Final accuracy: {diverse_accuracy:.4f}\")\n",
    "\n",
    "# # Distill knowledge\n",
    "# single_model = ImprovedSleepdetector(**best_params).to(device)\n",
    "\n",
    "# logging.info(\"Performing knowledge distillation...\")\n",
    "# distilled_model = distill_knowledge(ensemble_model, single_model, train_loader, (X_test, X_test_spectral, y_test), device)\n",
    "# overall_progress.update(1)\n",
    "\n",
    "\n",
    "# torch.save(distilled_model.state_dict(), os.path.join(config['new_model_path'], 'distilled_model.pth'))\n",
    "# overall_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final evaluation\n",
    "# diverse_ensemble.load_state_dict(diverse_best_state)\n",
    "# diverse_final_accuracy = evaluate_model(diverse_ensemble, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "# distilled_accuracy = evaluate_model(distilled_model, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "\n",
    "# logging.info(f\"Training completed. Best accuracy: {best_accuracy:.4f}\")\n",
    "# logging.info(f\"Ensemble Model - Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "# logging.info(f\"Diverse Ensemble Model - Final Test Accuracy: {diverse_final_accuracy:.4f}\")\n",
    "# logging.info(f\"Distilled Model - Final Test Accuracy: {distilled_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
