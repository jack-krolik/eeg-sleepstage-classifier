{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, LinearLR\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sleepdetector_new import ImprovedSleepdetector\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "\n",
    "# Assuming ImprovedSleepdetector is imported from your sleepdetector_new module\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([ImprovedSleepdetector(**model_params) for _ in range(n_models)])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "class DiverseEnsembleModel(nn.Module):\n",
    "    def __init__(self, model_params, n_models=3):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([\n",
    "            ImprovedSleepdetector(**{**model_params, 'dropout': model_params['dropout'] * (i+1)/n_models})\n",
    "            for i in range(n_models)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, spectral_features):\n",
    "        outputs = [model(x.clone(), spectral_features.clone()) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def load_best_params(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    return params['best_model_params']\n",
    "\n",
    "def print_model_structure(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "\n",
    "def load_data_and_params(config):\n",
    "    data_dict = torch.load(config['preprocessed_data_path'])\n",
    "    best_params_path = os.path.join(config['previous_model_path'], config['best_params_name'])\n",
    "    best_params = load_best_params(best_params_path)\n",
    "    return data_dict, best_params\n",
    "\n",
    "# def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "#     return SequentialLR(\n",
    "#         optimizer,\n",
    "#         schedulers=[\n",
    "#             LinearLR(optimizer, start_factor=0.1, total_iters=num_warmup_steps),\n",
    "#             CosineAnnealingLR(optimizer, T_max=num_training_steps - num_warmup_steps)\n",
    "#         ],\n",
    "#         milestones=[num_warmup_steps]\n",
    "#     )\n",
    "\n",
    "def get_scheduler(optimizer, num_warmup_steps, num_training_steps, min_lr=1e-6):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            min_lr,\n",
    "            float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def find_lr(model, train_loader, optimizer, criterion, device, num_iter=100, start_lr=1e-8, end_lr=1):\n",
    "    logging.info(\"Starting learning rate finder...\")\n",
    "    model.train()\n",
    "    num_samples = len(train_loader.dataset)\n",
    "    update_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "    lr = start_lr\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    running_loss = 0\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    \n",
    "    progress_bar = tqdm(range(num_iter), desc=\"Finding best LR\")\n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            inputs, spectral_features, targets = next(iter(train_loader))\n",
    "        except StopIteration:\n",
    "            train_loader = iter(train_loader)\n",
    "            inputs, spectral_features, targets = next(train_loader)\n",
    "        \n",
    "        inputs, spectral_features, targets = inputs.to(device), spectral_features.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        batch_num += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, spectral_features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Compute the smoothed loss\n",
    "        running_loss = 0.98 * running_loss + 0.02 * loss.item()\n",
    "        smoothed_loss = running_loss / (1 - 0.98**batch_num)\n",
    "        \n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            logging.info(f\"Loss is exploding, stopping early at lr={lr:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{smoothed_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "    plt.xlabel(\"Log Learning Rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Rate vs. Loss\")\n",
    "    plt.savefig('lr_finder_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Find the learning rate with the steepest negative gradient\n",
    "    smoothed_losses = np.array(losses[10:-5])\n",
    "    smoothed_lrs = np.array(log_lrs[10:-5])\n",
    "    gradients = (smoothed_losses[1:] - smoothed_losses[:-1]) / (smoothed_lrs[1:] - smoothed_lrs[:-1])\n",
    "    best_lr = 10 ** smoothed_lrs[np.argmin(gradients)]\n",
    "    \n",
    "    # Adjust the learning rate to be slightly lower than the one with steepest gradient\n",
    "    best_lr *= 0.1\n",
    "    \n",
    "    logging.info(f\"Learning rate finder completed. Suggested Learning Rate: {best_lr:.2e}\")\n",
    "    logging.info(\"Learning rate vs. loss plot saved as 'lr_finder_plot.png'\")\n",
    "    return best_lr\n",
    "\n",
    "def train_model(model, train_loader, val_data, optimizer, scheduler, criterion, device, epochs=100, accumulation_steps=4, log_interval=5):\n",
    "    scaler = GradScaler()\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        epoch_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for i, (batch_x, batch_x_spectral, batch_y) in enumerate(epoch_progress):\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n",
    "                outputs = model(batch_x, batch_x_spectral)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss).any():\n",
    "                logging.error(f\"NaN loss detected at epoch {epoch+1}, batch {i+1}\")\n",
    "                return None, 0\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            \n",
    "            epoch_progress.set_postfix({'loss': f'{running_loss/(i+1):.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % log_interval == 0 or epoch == epochs - 1:\n",
    "            accuracy = evaluate_model(model, val_data, device)\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_state = model.state_dict()\n",
    "                logging.info(f\"New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    return best_model_state, best_accuracy\n",
    "\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()\n",
    "    X, X_spectral, y = data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X.to(device), X_spectral.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return accuracy\n",
    "\n",
    "def distill_knowledge(teacher_model, student_model, train_loader, val_data, device, num_epochs=50, log_interval=5):\n",
    "    optimizer = optim.AdamW(student_model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    scheduler = get_scheduler(optimizer, num_warmup_steps=len(train_loader) * 5, num_training_steps=len(train_loader) * num_epochs)\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    overall_progress = tqdm(total=num_epochs, desc=\"Overall Distillation Progress\", position=0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        epoch_progress = tqdm(train_loader, desc=f\"Distillation Epoch {epoch+1}/{num_epochs}\", position=1, leave=False)\n",
    "        for batch_x, batch_x_spectral, batch_y in epoch_progress:\n",
    "            batch_x, batch_x_spectral, batch_y = batch_x.to(device), batch_x_spectral.to(device), batch_y.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(batch_x, batch_x_spectral)\n",
    "            \n",
    "            student_outputs = student_model(batch_x, batch_x_spectral)\n",
    "            \n",
    "            loss = criterion(F.log_softmax(student_outputs / 2, dim=1),\n",
    "                             F.softmax(teacher_outputs / 2, dim=1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            epoch_progress.set_postfix({'loss': f'{running_loss/(epoch_progress.n+1):.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
    "        \n",
    "        # Evaluate and log every log_interval epochs\n",
    "        if (epoch + 1) % log_interval == 0 or epoch == num_epochs - 1:\n",
    "            accuracy = evaluate_model(student_model, val_data, device)\n",
    "            logging.info(f\"Distillation Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        overall_progress.update(1)\n",
    "    \n",
    "    overall_progress.close()\n",
    "    return student_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'previous_model_path': './models/original/',\n",
    "        'new_model_path': './models/new/',\n",
    "        'best_params_name': 'best_params_ensemble.json',\n",
    "        'best_model_name': 'best_ensemble_model.pth',\n",
    "        'use_pretrained': True,\n",
    "        'pretrained_weights_path': './models/original/best_ensemble_model.pth',\n",
    "        'preprocessed_data_path': './preprocessed_data.pt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load data and parameters\n",
    "data_dict, best_params = load_data_and_params(config)\n",
    "\n",
    "# Extract data\n",
    "X_train, X_train_spectral, y_train = data_dict['X_train'], data_dict['X_train_spectral'], data_dict['y_train']\n",
    "X_test, X_test_spectral, y_test = data_dict['X_test'], data_dict['X_test_spectral'], data_dict['y_test']\n",
    "\n",
    "# Print shapes for verification\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"X_train_spectral shape: {X_train_spectral.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"X_test_spectral shape: {X_test_spectral.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Initialize model with best parameters\n",
    "ensemble_model = EnsembleModel(best_params).to(device)\n",
    "\n",
    "# Print the structure of the new model\n",
    "# print(\"\\nModel Structure:\")\n",
    "# print_model_structure(ensemble_model)\n",
    "\n",
    "# Load pre-trained weights if available\n",
    "if config['use_pretrained'] and os.path.exists(config['pretrained_weights_path']):\n",
    "    try:\n",
    "        state_dict = torch.load(config['pretrained_weights_path'])\n",
    "        ensemble_model.load_state_dict(state_dict)\n",
    "        logging.info(\"Successfully loaded pre-trained weights\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading pre-trained weights: {e}\")\n",
    "        logging.info(\"Attempting to load with strict=False\")\n",
    "        incompatible_keys = ensemble_model.load_state_dict(state_dict, strict=False)\n",
    "        logging.info(f\"Missing Keys: {incompatible_keys.missing_keys}\")\n",
    "        logging.info(f\"Unexpected Keys: {incompatible_keys.unexpected_keys}\")\n",
    "else:\n",
    "    logging.info(\"Starting with fresh weights\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting training process...\")\n",
    "overall_steps = 4  # LR finding, Ensemble training, Diverse Ensemble training, Knowledge Distillation\n",
    "overall_progress = tqdm(total=overall_steps, desc=\"Overall Training Progress\", position=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, X_train_spectral, y_train), \n",
    "    batch_size=32,  # You might want to adjust this based on your best_params\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Find best learning rate\n",
    "optimizer = optim.AdamW(ensemble_model.parameters(), lr=1e-8, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "logging.info(\"Finding best learning rate...\")\n",
    "best_lr = find_lr(ensemble_model, train_loader, optimizer, criterion, device, num_iter=100, start_lr=1e-8, end_lr=1)\n",
    "overall_progress.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and scheduler with best learning rate\n",
    "optimizer = optim.AdamW(ensemble_model.parameters(), lr=best_lr, weight_decay=1e-2)\n",
    "num_epochs = 100  # Adjust as needed\n",
    "num_warmup_steps = len(train_loader) * 5  # 5 epochs of warmup\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "# Train model\n",
    "logging.info(\"Training ensemble model...\")\n",
    "best_model_state, best_accuracy = train_model(\n",
    "    ensemble_model, train_loader, (X_test, X_test_spectral, y_test),\n",
    "    optimizer, scheduler, criterion, device, epochs=num_epochs\n",
    ")\n",
    "overall_progress.update(1)\n",
    "\n",
    "# Save best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, os.path.join(config['new_model_path'], config['best_model_name']))\n",
    "    logging.info(f\"Best ensemble model saved. Final accuracy: {best_accuracy:.4f}\")\n",
    "else:\n",
    "    logging.error(\"Training failed due to NaN loss.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diverse ensemble\n",
    "diverse_ensemble = DiverseEnsembleModel(best_params).to(device)\n",
    "diverse_optimizer = optim.AdamW(diverse_ensemble.parameters(), lr=best_lr, weight_decay=1e-2)\n",
    "diverse_scheduler = get_scheduler(diverse_optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "\n",
    "logging.info(\"Training diverse ensemble model...\")\n",
    "diverse_best_state, diverse_accuracy = train_model(\n",
    "    diverse_ensemble, train_loader, (X_test, X_test_spectral, y_test),\n",
    "    diverse_optimizer, diverse_scheduler, criterion, device, epochs=num_epochs\n",
    ")\n",
    "overall_progress.update(1)\n",
    "\n",
    "\n",
    "torch.save(diverse_best_state, os.path.join(config['new_model_path'], 'best_diverse_ensemble_model.pth'))\n",
    "logging.info(f\"Best diverse ensemble model saved. Final accuracy: {diverse_accuracy:.4f}\")\n",
    "\n",
    "# Distill knowledge\n",
    "single_model = ImprovedSleepdetector(**best_params).to(device)\n",
    "\n",
    "logging.info(\"Performing knowledge distillation...\")\n",
    "distilled_model = distill_knowledge(ensemble_model, single_model, train_loader, (X_test, X_test_spectral, y_test), device)\n",
    "overall_progress.update(1)\n",
    "\n",
    "\n",
    "torch.save(distilled_model.state_dict(), os.path.join(config['new_model_path'], 'distilled_model.pth'))\n",
    "overall_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "ensemble_model.load_state_dict(best_model_state)\n",
    "final_accuracy = evaluate_model(ensemble_model, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "diverse_ensemble.load_state_dict(diverse_best_state)\n",
    "diverse_final_accuracy = evaluate_model(diverse_ensemble, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "distilled_accuracy = evaluate_model(distilled_model, (X_test, X_test_spectral, y_test), device)\n",
    "\n",
    "\n",
    "logging.info(f\"Training completed. Best accuracy: {best_accuracy:.4f}\")\n",
    "logging.info(f\"Ensemble Model - Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "logging.info(f\"Diverse Ensemble Model - Final Test Accuracy: {diverse_final_accuracy:.4f}\")\n",
    "logging.info(f\"Distilled Model - Final Test Accuracy: {distilled_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
