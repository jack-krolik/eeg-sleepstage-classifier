{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import mne\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "\n",
    "# Load the EDF file\n",
    "edf_path = '../data/201 N1.edf'\n",
    "raw = mne.io.read_raw_edf(edf_path, preload=True)\n",
    "\n",
    "# Convert to microvolts\n",
    "raw._data *= 1e6  # Convert from volts to microvolts\n",
    "print(f\"After conversion to μV - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Bandpass filter from 0.5 to 50 Hz\n",
    "raw.filter(0.5, 50, fir_design='firwin')\n",
    "print(f\"After filtering - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Downsample to 100 Hz\n",
    "raw.resample(100)\n",
    "print(f\"After resampling - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Print out the channel names in the raw EDF data\n",
    "print(\"Channel names:\", raw.ch_names)\n",
    "\n",
    "# Create bipolar montage\n",
    "bipolar_montage = mne.set_bipolar_reference(\n",
    "    raw,\n",
    "    anode=['F3-M2', 'C3-M2', 'F4-M1', 'C4-M1'],\n",
    "    cathode=['C3-M2', 'O1-M2', 'C4-M1', 'O2-M1'],\n",
    "    ch_name=['F3-C3', 'C3-O1', 'F4-C4', 'C4-O2']\n",
    ")\n",
    "\n",
    "# Pick only the bipolar channels\n",
    "raw = bipolar_montage.pick_channels(['F3-C3', 'C3-O1', 'F4-C4', 'C4-O2'])\n",
    "print(f\"After bipolar montage - Min: {raw._data.min():.2f}, Max: {raw._data.max():.2f}\")\n",
    "\n",
    "# Segment into 30-second epochs\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=True)\n",
    "print(f\"After epoching - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "# Modified artifact detection function to return separate masks\n",
    "def detect_artifacts(data, threshold=500, flat_threshold=1e-6, flat_duration=5, sfreq=100):\n",
    "    # Detect amplitude saturation\n",
    "    amplitude_mask = np.abs(data) > threshold\n",
    "    \n",
    "    # Detect flat signals\n",
    "    diff = np.abs(np.diff(data))\n",
    "    flat_mask = np.zeros_like(data, dtype=bool)\n",
    "    flat_samples = int(flat_duration * sfreq)\n",
    "    for i in range(len(data) - flat_samples):\n",
    "        if np.all(diff[i:i+flat_samples-1] < flat_threshold):\n",
    "            flat_mask[i:i+flat_samples] = True\n",
    "    \n",
    "    return amplitude_mask, flat_mask\n",
    "\n",
    "def handle_artifacts(data, artifact_mask):\n",
    "    data[artifact_mask] = np.nan\n",
    "    return data\n",
    "\n",
    "def fill_nan(data):\n",
    "    mask = np.isnan(data)\n",
    "    x = np.arange(len(data))\n",
    "    data[mask] = np.interp(x[mask], x[~mask], data[~mask])\n",
    "    return data\n",
    "\n",
    "# def preprocess_sleep_stages(file_path, epochs_to_remove, keep_unscored=False):\n",
    "#     # Define the mapping for renumbering\n",
    "#     stage_mapping = {\n",
    "#         7: 4,  # Wake to 4\n",
    "#         5: 3,  # REM to 3\n",
    "#         1: 2,  # Stage 1 to 2 (N1)\n",
    "#         2: 1,  # Stage 2 to 1 (N2)\n",
    "#         3: 0,  # Stage 3 to 0 (N3)\n",
    "#         0: -1  # Assuming 0 means unscored, map it to -1 or handle as needed\n",
    "#     }\n",
    "\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "    \n",
    "#     # Skip the first two lines (timestamp information)\n",
    "#     data = [line.strip().split() for line in lines[2:]]\n",
    "    \n",
    "#     # Extract the first column (sleep stage scores)\n",
    "#     sleep_stages = [int(row[0]) for row in data]\n",
    "\n",
    "#     if keep_unscored:\n",
    "#         processed_stages = sleep_stages\n",
    "#     else:\n",
    "#         # Find the index of the first and last wake state (7)\n",
    "#         start_index = sleep_stages.index(7)\n",
    "#         end_index = len(sleep_stages) - sleep_stages[::-1].index(7)\n",
    "        \n",
    "#         # Extract the relevant sleep stages\n",
    "#         processed_stages = sleep_stages[start_index:end_index]\n",
    "    \n",
    "#     # Renumber the stages according to the mapping\n",
    "#     renumbered_stages = [stage_mapping.get(stage, stage) for stage in processed_stages]\n",
    "    \n",
    "#     # Convert to numpy array with shape (1, # of sleep scores)\n",
    "#     np_stages = np.array(renumbered_stages).reshape(1, -1)\n",
    "\n",
    "#     # Remove sleep stages corresponding to removed epochs\n",
    "#     np_stages = np.delete(np_stages, list(epochs_to_remove), axis=1)\n",
    "\n",
    "#     return np_stages\n",
    "\n",
    "\n",
    "def preprocess_sleep_stages(file_path, epochs_to_remove, keep_unscored=False):\n",
    "    # Define the mapping for renumbering\n",
    "    stage_mapping = {\n",
    "        7: 4,  # Wake to 4\n",
    "        5: 3,  # REM to 3\n",
    "        1: 2,  # Stage 1 to 2 (N1)\n",
    "        2: 1,  # Stage 2 to 1 (N2)\n",
    "        3: 0,  # Stage 3 to 0 (N3)\n",
    "        0: -1  # Assuming 0 means unscored, map it to -1 or handle as needed\n",
    "    }\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Skip the first two lines (timestamp information)\n",
    "    data = [line.strip().split() for line in lines[2:]]\n",
    "    \n",
    "    # Extract the first column (sleep stage scores)\n",
    "    sleep_stages = [int(row[0]) for row in data]\n",
    "\n",
    "    # Find the index of the first and last wake state (7)\n",
    "    start_index = sleep_stages.index(7)\n",
    "    end_index = len(sleep_stages) - sleep_stages[::-1].index(7)\n",
    "\n",
    "    # Ensure 50 awake states at the beginning\n",
    "    for i in range(max(0, start_index - 50), start_index):\n",
    "        if sleep_stages[i] == 0:  # If unscored\n",
    "            sleep_stages[i] = 7  # Change to awake\n",
    "    start_index = max(0, start_index - 50)\n",
    "\n",
    "    # Ensure 50 awake states at the end\n",
    "    for i in range(end_index, min(len(sleep_stages), end_index + 50)):\n",
    "        if sleep_stages[i] == 0:  # If unscored\n",
    "            sleep_stages[i] = 7  # Change to awake\n",
    "    end_index = min(len(sleep_stages), end_index + 50)\n",
    "\n",
    "    if keep_unscored:\n",
    "        processed_stages = sleep_stages\n",
    "    else:\n",
    "        # Extract the relevant sleep stages\n",
    "        processed_stages = sleep_stages[start_index:end_index]\n",
    "    \n",
    "    # Renumber the stages according to the mapping\n",
    "    renumbered_stages = [stage_mapping.get(stage, stage) for stage in processed_stages]\n",
    "    \n",
    "    # Convert to numpy array with shape (1, # of sleep scores)\n",
    "    np_stages = np.array(renumbered_stages).reshape(1, -1)\n",
    "\n",
    "    # Remove sleep stages corresponding to removed epochs\n",
    "    np_stages = np.delete(np_stages, list(epochs_to_remove), axis=1)\n",
    "\n",
    "    return np_stages\n",
    "\n",
    "# Initialize arrays to track artifacts\n",
    "amplitude_artifact_counts = np.zeros(epochs._data.shape[0])\n",
    "flat_artifact_counts = np.zeros(epochs._data.shape[0])\n",
    "epochs_to_remove = set()\n",
    "\n",
    "# Apply artifact detection and handling\n",
    "for i in range(epochs._data.shape[0]):  # Loop through epochs\n",
    "    for j in range(epochs._data.shape[1]):  # Loop through channels\n",
    "        amplitude_mask, flat_mask = detect_artifacts(epochs._data[i, j, :], sfreq=raw.info['sfreq'])\n",
    "        \n",
    "        amplitude_artifact_counts[i] += np.sum(amplitude_mask)\n",
    "        flat_artifact_counts[i] += np.sum(flat_mask)\n",
    "        \n",
    "        amplitude_labeled, _ = ndimage.label(amplitude_mask)\n",
    "        flat_labeled, _ = ndimage.label(flat_mask)\n",
    "        \n",
    "        amplitude_consecutive = ndimage.sum(amplitude_mask, amplitude_labeled, range(1, amplitude_labeled.max() + 1))\n",
    "        flat_consecutive = ndimage.sum(flat_mask, flat_labeled, range(1, flat_labeled.max() + 1))\n",
    "        \n",
    "        if np.any(amplitude_consecutive >= 5 * raw.info['sfreq']) or np.any(flat_consecutive >= 5 * raw.info['sfreq']):\n",
    "            epochs_to_remove.add(i)\n",
    "            break\n",
    "        \n",
    "        epochs._data[i, j, :] = handle_artifacts(epochs._data[i, j, :], amplitude_mask | flat_mask)\n",
    "\n",
    "# Separate the artifact counts for kept and removed epochs\n",
    "kept_amplitude_counts = np.delete(amplitude_artifact_counts, list(epochs_to_remove))\n",
    "kept_flat_counts = np.delete(flat_artifact_counts, list(epochs_to_remove))\n",
    "removed_amplitude_counts = amplitude_artifact_counts[list(epochs_to_remove)]\n",
    "removed_flat_counts = flat_artifact_counts[list(epochs_to_remove)]\n",
    "\n",
    "# Remove marked epochs\n",
    "epochs.drop(list(epochs_to_remove))\n",
    "\n",
    "print(f\"Total interpolated amplitude artifacts in kept epochs: {int(np.sum(kept_amplitude_counts))}\")\n",
    "print(f\"Total interpolated flat signal artifacts in kept epochs: {int(np.sum(kept_flat_counts))}\")\n",
    "print(f\"Total amplitude artifacts in removed epochs: {int(np.sum(removed_amplitude_counts))}\")\n",
    "print(f\"Total flat signal artifacts in removed epochs: {int(np.sum(removed_flat_counts))}\")\n",
    "print(f\"Number of epochs removed due to long artifacts: {len(epochs_to_remove)}\")\n",
    "\n",
    "\n",
    "# Fill NaN values\n",
    "for i in range(epochs._data.shape[0]):  # Loop through epochs\n",
    "    for j in range(epochs._data.shape[1]):  # Loop through channels\n",
    "        epochs._data[i, j, :] = fill_nan(epochs._data[i, j, :])\n",
    "\n",
    "print(f\"After artifact handling - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")\n",
    "\n",
    "# Scale each channel\n",
    "for i, ch in enumerate(raw.ch_names):\n",
    "    data = epochs.get_data(picks=ch).reshape(-1)\n",
    "    median = np.median(data)\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    scaled_data = (data - median) / iqr * iqr + median\n",
    "    epochs._data[:, epochs.ch_names.index(ch), :] = scaled_data.reshape(epochs._data.shape[0], -1)\n",
    "\n",
    "print(f\"After scaling - Min: {epochs._data.min():.2f}, Max: {epochs._data.max():.2f}\")\n",
    "\n",
    "# Usage\n",
    "file_path = '../data/Sleep Stages 201_N1.txt'  # Replace with the actual file path\n",
    "result = preprocess_sleep_stages(file_path, epochs_to_remove, keep_unscored=True)\n",
    "print(result)\n",
    "\n",
    "# Extract preprocessed data (without the need for transposition)\n",
    "preprocessed_data = epochs.get_data()\n",
    "\n",
    "# Create a dictionary for saving (no transposition required here)\n",
    "mat_dict = {\n",
    "    'sig1': preprocessed_data[:, 0, :],  # Save directly, no .T\n",
    "    'sig2': preprocessed_data[:, 1, :],  \n",
    "    'sig3': preprocessed_data[:, 2, :],  \n",
    "    'sig4': preprocessed_data[:, 3, :],  \n",
    "    'Fs': raw.info['sfreq'],\n",
    "    'ch_names': raw.ch_names,\n",
    "    'labels': result  # This now has the same number of epochs as the EEG data\n",
    "}\n",
    "\n",
    "# Save as .mat file\n",
    "try:\n",
    "    scipy.io.savemat('preprocessed_data_combined.mat', mat_dict)\n",
    "    print(\"Data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "# Plotting function\n",
    "def plot_channel(data, channel_name, fs):\n",
    "    time = np.arange(len(data)) / fs\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(time, data)\n",
    "    plt.title(f'{channel_name} from 201 N1.edf (Preprocessed)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude (μV)')\n",
    "    plt.show()\n",
    "\n",
    "# Plot each channel\n",
    "for i, ch_name in enumerate(raw.ch_names):\n",
    "    plot_channel(preprocessed_data[:, i, :].flatten(), ch_name, raw.info['sfreq'])\n",
    "\n",
    "# Histogram of amplitudes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(preprocessed_data.flatten(), bins=100)\n",
    "plt.title('Histogram of EEG Amplitudes After Preprocessing')\n",
    "plt.xlabel('Amplitude (μV)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Data quality check function\n",
    "def check_data_quality(data):\n",
    "    if np.isnan(data).any():\n",
    "        print(\"Warning: NaN values detected in preprocessed data.\")\n",
    "    if np.isinf(data).any():\n",
    "        print(\"Warning: Infinite values detected in preprocessed data.\")\n",
    "    print(f\"Final data range: {data.min():.2f} to {data.max():.2f} μV\")\n",
    "\n",
    "check_data_quality(preprocessed_data)\n",
    "\n",
    "# Plotting the frequency of artifacts\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# Plot for kept epochs - Amplitude artifacts\n",
    "ax[0].bar(np.arange(len(kept_amplitude_counts)), kept_amplitude_counts, color='blue', alpha=0.7)\n",
    "ax[0].set_title(f'Interpolated Amplitude Artifacts in Kept Epochs (Total: {int(np.sum(kept_amplitude_counts))})')\n",
    "ax[0].set_xlabel('Kept Epoch Index')\n",
    "ax[0].set_ylabel('Number of Artifacts')\n",
    "ax[0].set_xlim(0, max(1, len(kept_amplitude_counts)))\n",
    "ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax[0].set_yscale('symlog')\n",
    "\n",
    "# Plot for kept epochs - Flat signal artifacts\n",
    "ax[1].bar(np.arange(len(kept_flat_counts)), kept_flat_counts, color='red', alpha=0.7)\n",
    "ax[1].set_title(f'Interpolated Flat Signal Artifacts in Kept Epochs (Total: {int(np.sum(kept_flat_counts))})')\n",
    "ax[1].set_xlabel('Kept Epoch Index')\n",
    "ax[1].set_ylabel('Number of Artifacts')\n",
    "ax[1].set_xlim(0, max(1, len(kept_flat_counts)))\n",
    "ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax[1].set_yscale('symlog')\n",
    "\n",
    "# Plot for removed epochs - Both types of artifacts\n",
    "if len(removed_amplitude_counts) > 0:\n",
    "    ax[2].bar(np.arange(len(removed_amplitude_counts)), removed_amplitude_counts, color='blue', alpha=0.7, label='Amplitude')\n",
    "    ax[2].bar(np.arange(len(removed_flat_counts)), removed_flat_counts, color='red', alpha=0.7, label='Flat Signal')\n",
    "    ax[2].set_title(f'Artifacts in Removed Epochs (Amplitude: {int(np.sum(removed_amplitude_counts))}, Flat: {int(np.sum(removed_flat_counts))})')\n",
    "    ax[2].set_xlabel('Removed Epoch Index')\n",
    "    ax[2].set_ylabel('Number of Artifacts')\n",
    "    ax[2].set_xlim(0, len(removed_amplitude_counts))\n",
    "    ax[2].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax[2].set_yscale('symlog')\n",
    "    ax[2].legend()\n",
    "else:\n",
    "    ax[2].text(0.5, 0.5, 'No epochs were removed', ha='center', va='center', fontsize=12)\n",
    "    ax[2].set_title('Artifacts in Removed Epochs')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sleep stages\", mat_dict['labels'].shape[1])\n",
    "print(\"Number of epochs\", preprocessed_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Load the first .mat file (your preprocessed data)\n",
    "mat_data_1 = scipy.io.loadmat('./preprocessed_data_combined.mat')\n",
    "signals_file1 = [np.squeeze(mat_data_1['sig1']), np.squeeze(mat_data_1['sig2']), \n",
    "                 np.squeeze(mat_data_1['sig3']), np.squeeze(mat_data_1['sig4'])]  # No need for .T\n",
    "Fs_file1 = np.squeeze(mat_data_1['Fs'])\n",
    "ch_names = mat_data_1['ch_names'].flatten().tolist()  # Extract channel names\n",
    "\n",
    "# Load the second .mat file (research dataset)\n",
    "mat_data_2 = scipy.io.loadmat('../data/data.mat')\n",
    "signals_file2 = [np.squeeze(mat_data_2['sig1']), np.squeeze(mat_data_2['sig2']), \n",
    "                 np.squeeze(mat_data_2['sig3']), np.squeeze(mat_data_2['sig4'])]\n",
    "Fs_file2 = np.squeeze(mat_data_2['Fs'])\n",
    "\n",
    "# Flatten the signals (flatten to 1D for each channel's data)\n",
    "signals_file1_flat = [sig.flatten() for sig in signals_file1]  # Correct flattening\n",
    "signals_file2_flat = [sig.flatten() for sig in signals_file2]\n",
    "\n",
    "# Create time vectors for both datasets\n",
    "time_file1 = [np.arange(sig.shape[0]) / Fs_file1 for sig in signals_file1_flat]\n",
    "time_file2 = [np.arange(sig.shape[0]) / Fs_file2 for sig in signals_file2_flat]\n",
    "\n",
    "# Plot all signals for both datasets side by side\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 16))\n",
    "\n",
    "for i in range(4):\n",
    "    # Plot signals from File 1 (left column)\n",
    "    axes[i, 0].plot(time_file1[i], signals_file1_flat[i])\n",
    "    axes[i, 0].set_title(f'{ch_names[i]} from 201 N1.edf (Preprocessed)')\n",
    "    axes[i, 0].set_xlabel('Time (seconds)')\n",
    "    axes[i, 0].set_ylabel('Amplitude (μV)')\n",
    "    \n",
    "    # Plot signals from File 2 (right column)\n",
    "    axes[i, 1].plot(time_file2[i], signals_file2_flat[i])\n",
    "    axes[i, 1].set_title(f'{ch_names[i]} from Research Paper Dataset')\n",
    "    axes[i, 1].set_xlabel('Time (seconds)')\n",
    "    axes[i, 1].set_ylabel('Amplitude (μV)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram of amplitudes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "axes[0].hist(np.concatenate(signals_file1_flat), bins=100, alpha=0.7)\n",
    "axes[0].set_title('Histogram of EEG Amplitudes (Preprocessed Data)')\n",
    "axes[0].set_xlabel('Amplitude (μV)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(np.concatenate(signals_file2_flat), bins=100, alpha=0.7)\n",
    "axes[1].set_title('Histogram of EEG Amplitudes (Research Paper Dataset)')\n",
    "axes[1].set_xlabel('Amplitude (μV)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zoomed-in plot of a section of data\n",
    "zoom_duration = 30  # seconds\n",
    "zoom_samples_file1 = int(zoom_duration * Fs_file1)\n",
    "zoom_samples_file2 = int(zoom_duration * Fs_file2)\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 16))\n",
    "\n",
    "for i in range(4):\n",
    "    # Plot zoomed section from File 1 (left column)\n",
    "    axes[i, 0].plot(time_file1[i][:zoom_samples_file1], signals_file1_flat[i][:zoom_samples_file1])\n",
    "    axes[i, 0].set_title(f'{ch_names[i]} from 201 N1.edf (Preprocessed) - Zoomed')\n",
    "    axes[i, 0].set_xlabel('Time (seconds)')\n",
    "    axes[i, 0].set_ylabel('Amplitude (μV)')\n",
    "    \n",
    "    # Plot zoomed section from File 2 (right column)\n",
    "    axes[i, 1].plot(time_file2[i][:zoom_samples_file2], signals_file2_flat[i][:zoom_samples_file2])\n",
    "    axes[i, 1].set_title(f'{ch_names[i]} from Research Paper Dataset - Zoomed')\n",
    "    axes[i, 1].set_xlabel('Time (seconds)')\n",
    "    axes[i, 1].set_ylabel('Amplitude (μV)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
